pub_date	title	venue	excerpt	citation	url_slug	paper_url
2021-08-02	Multilingual Agreement for Multilingual Neural Machine Translation	Conference (ACL-IJCNLP 2021)	Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines.	Jian Yang, Yuwei Yin, Shuming Ma, Haoyang Huang, Dongdong Zhang, Zhoujun Li, and Furu Wei. 2021. Multilingual Agreement for Multilingual Neural Machine Translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 233–239, Online. Association for Computational Linguistics.	Multilingual Agreement for Multilingual Neural Machine Translation	https://aclanthology.org/2021.acl-short.31.pdf
2021-11-07	Improving Multilingual Neural Machine Translation with Auxiliary Source Languages	Conference (EMNLP 2021)	Multilingual neural machine translation models typically handle one source language at a time. However, prior work has shown that translating from multiple source languages improves translation quality. Different from existing approaches on multi-source translation that are limited to the test scenario where parallel source sentences from multiple languages are available at inference time, we propose to improve multilingual translation in a more common scenario by exploiting synthetic source sentences from auxiliary languages. We train our model on synthetic multi-source corpora and apply random masking to enable flexible inference with single-source or bi-source inputs. Extensive experiments on Chinese/English-Japanese and a large-scale multilingual translation benchmark show that our model outperforms the multilingual baseline significantly by up to +4.0 BLEU with the largest improvements on low-resource or distant language pairs.	Weijia Xu, Yuwei Yin, Shuming Ma, Dongdong Zhang, and Haoyang Huang. 2021. Improving Multilingual Neural Machine Translation with Auxiliary Source Languages. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3029–3041, Punta Cana, Dominican Republic. Association for Computational Linguistics.	Improving Multilingual Neural Machine Translation with Auxiliary Source Languages	https://aclanthology.org/2021.findings-emnlp.260.pdf