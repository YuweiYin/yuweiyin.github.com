(window.webpackJsonp=window.webpackJsonp||[]).push([[41],{252:function(t,s,a){"use strict";a.r(s);var n=a(28),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"algorithm-graph-theory-graph-basis"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#algorithm-graph-theory-graph-basis"}},[t._v("#")]),t._v(" Algorithm - Graph Theory - Graph Basis")]),t._v(" "),a("p",[t._v("Create Date: 2020.05.29")]),t._v(" "),a("p",[t._v("Last Update Date: 2020.05.30")]),t._v(" "),a("p",[t._v("By "),a("a",{attrs:{href:"https://yuweiyin.github.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("YuweiYin"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[t._v("#")]),t._v(" 简介")]),t._v(" "),a("p",[t._v("图的表示和搜索")]),t._v(" "),a("p",[t._v("图的搜索指的是系统化地跟随图中的边来访问图中的每个结点。图搜索算法可以用来发现图的结构。许多图算法在一开始都会先通过搜索来获得图的结构，其它的一些图算法则是"),a("RouterLink",{attrs:{to:"/article/info-technology/algorithm/graph-theory/search/"}},[t._v("对基本的搜索加以优化")]),t._v("。可以说，图的搜索技巧是整个图算法领域的核心。")],1),t._v(" "),a("h2",{attrs:{id:"图的表示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#图的表示"}},[t._v("#")]),t._v(" 图的表示")]),t._v(" "),a("p",[t._v("对于图 G = (V, E)，可以用两种标准表示方法表示：")]),t._v(" "),a("ul",[a("li",[t._v("将图作为"),a("strong",[t._v("邻接链表")]),t._v("的组合")]),t._v(" "),a("li",[t._v("将图作为"),a("strong",[t._v("邻接矩阵")]),t._v("来看待")])]),t._v(" "),a("p",[t._v("这两种方式都既可以表示无向图，也可以表示有向图 (digraph)。")]),t._v(" "),a("ul",[a("li",[t._v("邻接链表非常紧凑，因此常用于表示"),a("strong",[t._v("稀疏图")]),t._v("（边的条数 "),a("code",[t._v("|E|")]),t._v(" 远远小于 "),a("code",[t._v("|V|^2")]),t._v(" 的图）。")]),t._v(" "),a("li",[t._v("在"),a("strong",[t._v("稠密图")]),t._v("（边的条数 "),a("code",[t._v("|E|")]),t._v(" 接近 "),a("code",[t._v("|V|^2")]),t._v(" 的图）的情况下，往往更倾向于使用邻接矩阵表示法。")]),t._v(" "),a("li",[t._v("另外，如果需要快速判断任意两个结点是否有边相连，可能也需要使用邻接矩阵表示法。")])]),t._v(" "),a("h3",{attrs:{id:"邻接链表-adjacency-list"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#邻接链表-adjacency-list"}},[t._v("#")]),t._v(" 邻接链表 Adjacency List")]),t._v(" "),a("p",[t._v("对于图 G = (V, E) 来说，其"),a("strong",[t._v("邻接链表表示")]),t._v("由一个包含 "),a("code",[t._v("|V|")]),t._v(" 条链表的数组 Adj 所构成，每个顶点有一条链表。")]),t._v(" "),a("p",[t._v("即：对于每个顶点 $ u \\in V $，邻接链表元素 Adj[u] 代表所有"),a("strong",[t._v("与 u 邻接的顶点链表")]),t._v("的"),a("strong",[t._v("链表头指针")]),t._v("。该链表中保存着那些顶点(的指针)。")]),t._v(" "),a("p",[t._v("注意，在无向图里，“邻接”关系具有"),a("strong",[t._v("对称性")]),t._v("，但在有向图中则不不具有对称性(也不具有反对称性)：u "),a("strong",[t._v("与 v 邻接")]),t._v(" 是指从 u 到 v 的有向边。这也等同于："),a("strong",[t._v("v 邻接于")]),t._v(" u。")]),t._v(" "),a("p",[t._v("如果 G 是一个无向图，那么对于无向边 "),a("code",[t._v("(u, v)")]),t._v(" 而言，结点 v 会出现在链表 Adj[u] 中，且结点 u 会出现在链表 Adj[v] 中。因此无向图的 Adj 中所有邻接链表到长度之和等于 "),a("code",[t._v("2|E|")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-1.png",alt:"graph-basis-1"}})]),t._v(" "),a("p",[t._v("而如果 G 是一个有向图，那么对于有向边 "),a("code",[t._v("<u, v>")]),t._v(" 而言，结点 v 会出现在链表 Adj[u] 中。因此有向图的 Adj 中所有邻接链表到长度之和等于 "),a("code",[t._v("|E|")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-2.png",alt:"graph-basis-2"}})]),t._v(" "),a("p",[t._v("无论对于有向图还是无向图，邻接链表表示法的"),a("strong",[t._v("存储空间需求")]),t._v("均为 $ \\Theta(|V| + |E|) $，对于图算法而言，这就是一种"),a("strong",[t._v("线性数量级")]),t._v("了。")]),t._v(" "),a("p",[t._v("另外，可以对邻接链表稍加修改，即可以用来表示"),a("strong",[t._v("权重图")]),t._v("。权重图是图中的"),a("strong",[t._v("每条边")]),t._v("都带有一个相关权重的图（有时也会对结点赋予权重值）。该权重值通常由一个 w: E->R 的"),a("strong",[t._v("权重函数")]),t._v("给出，将边映射到一个实数值（权重值往往需要是具有全序关系的）。从这种意义上说，邻接链表表示法的"),a("strong",[t._v("可扩展性很高")]),t._v("，可以对其进行简单修改来支持许多其它的图变种。")]),t._v(" "),a("h3",{attrs:{id:"邻接矩阵-adjacency-matrix"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#邻接矩阵-adjacency-matrix"}},[t._v("#")]),t._v(" 邻接矩阵 Adjacency Matrix")]),t._v(" "),a("p",[t._v("邻接链表的一个"),a("strong",[t._v("潜在缺陷")]),t._v("是无法快速判断边 (u, v) 是否存在于图中，唯一的办法是在邻接链表 Adj[u] 中搜索结点 v，需要耗费 "),a("code",[t._v("O(|V|)")]),t._v(" 的时间。邻接矩阵(二维数组)克服了这个缺陷，只需 O(1) 时间去判断顶点 u 和 v 是否邻接，但是付出的代价是更大的存储空间 $ \\Theta(|V|^2) $。")]),t._v(" "),a("p",[t._v("对于"),a("strong",[t._v("邻接矩阵表示")]),t._v("来说，通常会将图 G 中的结点编号为 0, 1, ..., "),a("code",[t._v("|V|-1")]),t._v("，图 G 的邻接矩阵由一个 "),a("code",[t._v("|V| x |V|")]),t._v(" 的矩阵 A = (aij) 表达，该矩阵满足如下性质：")]),t._v(" "),a("ul",[a("li",[t._v("aij = 1  若边 (i, j) \\in E")]),t._v(" "),a("li",[t._v("aij = 0  若边 (i, j) \\notin E")])]),t._v(" "),a("p",[t._v("如果需要存放带"),a("strong",[t._v("边权")]),t._v("的权重图，可以将边 (u, v) 的权重值 w(u, v) 直接存储在其邻接矩阵的相应位置。对于不存在的边，用一个预先设定的特殊值（如 0、inf、NIL 等）表达即可。")]),t._v(" "),a("p",[t._v("当然，也可以让邻接矩阵的每个元素 aij 存储的是顶点结构体。但是这样存储空间会占用很多。本身邻接矩阵的元素个数就是 "),a("code",[t._v("|V|^2")]),t._v(" 了（不管图中有多少边，都会占用 $ \\Theta(|V|^2) $ 的空间）。")]),t._v(" "),a("p",[t._v("另外，易知"),a("strong",[t._v("无向图")]),t._v("的邻接矩阵是一个"),a("strong",[t._v("对称矩阵")]),t._v("，因此只需存放邻接矩阵的"),a("strong",[t._v("主对角线")]),t._v("及"),a("strong",[t._v("上三角矩阵")]),t._v("部分，从而将图存储空间的需求减少近乎一半。")]),t._v(" "),a("p",[t._v("此外，虽然邻接链表表示法和邻接矩阵表示法在渐近意义下至少是一样空间有效的，但"),a("strong",[t._v("邻接矩阵表示法更为简单")]),t._v("，因此在"),a("strong",[t._v("图规模比较小")]),t._v("时，往往更倾向于使用邻接矩阵表示法。而且，对于无权图来说，邻接矩阵"),a("strong",[t._v("每个记录项仅需要 1 bit 的空间")]),t._v("（边存在即为 True、不存在即为 False）。")]),t._v(" "),a("h3",{attrs:{id:"表示图的属性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#表示图的属性"}},[t._v("#")]),t._v(" 表示图的属性")]),t._v(" "),a("p",[t._v("对图进行操作的多数算法需要维持图中结点或边的某些属性。")]),t._v(" "),a("p",[t._v("在本系列文章里，这些属性可以使用通常的表述法来进行表示，如 v.d 表示结点 v 的属性 d。当使用一对结点来表示一条边时，也可以用同样的方式表述：(u, v).f 表示边 (u, v) 具有属性 f。")]),t._v(" "),a("p",[t._v("但是在算法的实际程序里，实现顶点和边的属性则依赖于诸多因素：所使用的程序设计语言、需要实现的图算法和程序中使用图的方式等。对每个顶点，往往可以构建 Vertex 结构体，但不常为边构造 Edge 结构体，而是将关于边的信息储存在邻接链表结点或邻接矩阵元素中。")]),t._v(" "),a("h3",{attrs:{id:"关联矩阵-incidence-matrix"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#关联矩阵-incidence-matrix"}},[t._v("#")]),t._v(" 关联矩阵 Incidence Matrix")]),t._v(" "),a("p",[t._v("有向无环图 G = (V, E) 的"),a("strong",[t._v("关联矩阵")]),t._v(" (Incidence Matrix) 是一个满足如下性质的 "),a("code",[t._v("|V| x |E|")]),t._v(" 的矩阵 B = (bij)：")]),t._v(" "),a("ul",[a("li",[t._v("bij = 1  如果边 j 以顶点 i 为起点")]),t._v(" "),a("li",[t._v("bij = -1 如果边 j 以顶点 i 为终点")]),t._v(" "),a("li",[t._v("bij = 0  其它情况")])]),t._v(" "),a("p",[t._v("“关联”是表示顶点与边之间的关系。对有向图来说，bij = 1 为顶点 i 贡献了 1 个出度，bij = -1 为顶点 i 贡献了 1 个入度。（这个定义完全可以反过来）")]),t._v(" "),a("h3",{attrs:{id:"其它"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其它"}},[t._v("#")]),t._v(" 其它")]),t._v(" "),a("p",[t._v("有向图 G = (V, E) 的"),a("strong",[t._v("转置")]),t._v("是图 G^T = (V, E^T)，这里 "),a("code",[t._v("E^T = {<v, e> \\in |V| x |V|: <u, v> \\in E}")]),t._v("。即：原 E 中的有向边全部反向。转置是矩阵操作里非常常用的，因此对于邻接链表和邻接矩阵 往往都需要设计一个高效的图转置算法。")]),t._v(" "),a("p",[t._v("如果数组 Adj[u] 的每个记录项不是链表，而是一个散列表，里面包含的是 $ (u, v) \\in E $ 的顶点 v。即：以二元元组 (u, v) 为关键字，散列映射到顶点 v。这样可以仅用 O(1) 的期望时间（假设每条边被查询的概率相同）确定某两个顶点是否邻接。")]),t._v(" "),a("h2",{attrs:{id:"图的搜索"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#图的搜索"}},[t._v("#")]),t._v(" 图的搜索")]),t._v(" "),a("h3",{attrs:{id:"广度优先搜索-bfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#广度优先搜索-bfs"}},[t._v("#")]),t._v(" 广度优先搜索 BFS")]),t._v(" "),a("p",[a("strong",[t._v("广度优先搜索")]),t._v(" (Breadth First Search, BFS) 是最简单的图搜索算法之一，也是许多重要的图算法的原型。Prim 的最小生成树算法 和 Dijkstra 的单源最短路径算法 都使用了类似 BFS 的思想。")]),t._v(" "),a("p",[t._v("给定图 G = (V, E) 和一个可以识别的"),a("strong",[t._v("源结点")]),t._v(" s，BFS 对图 G 中的边进行系统性地探索 来发现可以从源结点 s 到达的所有结点。该算法能够计算从源结点 s 到每个可到达的结点的距离（最少的边数 / 最短的路径长度），同时生成一棵“广度优先搜索树”。该树以源结点 s 为根结点，包含所有可以从 s 到达的结点。")]),t._v(" "),a("p",[t._v("对于每个从源结点 s 可以到达的结点 v，在 BFS 树里从结点 s 到结点 v 的简单路径所对应的就是图 G 中从 s 到结点 v 的“最短路径”，即包含最少边数的路径。该算法既可以用于有向图，也可以用于无向图。")]),t._v(" "),a("p",[t._v("“广度优先搜索”顾名思义，始终先沿着当前广度（与源结点的距离）探索，直到当前广度已经不能发现新结点了，则提升广度继续搜索。即：算法需要在发现所有距离源结点 s 为 k 的所有结点之后，才会去发现距离源结点 s 为 k+1 的其它结点。")]),t._v(" "),a("p",[t._v("在 BFS 过程中，所有结点的状态一般有三种："),a("strong",[t._v("未被发现")]),t._v("、"),a("strong",[t._v("已被发现")]),t._v("、"),a("strong",[t._v("已被探索")]),t._v("。")]),t._v(" "),a("p",[t._v("在执行 BFS 的过程，往往会借助一个先进先出 (FIFO) 的"),a("strong",[t._v("辅助队列 queue")]),t._v("结构。一开始，辅助队列 Q 中仅有源结点 s，随后将 s 的邻接结点（图表示法既可以是邻接链表，也可以是邻接矩阵）均加入到 Q 中。一旦某个结点 u 的所有邻接结点都被加入到 Q 后，u 就被移除出 Q。此时 u 即为 “已被探索” 状态，不能再被加入 Q 了。")]),t._v(" "),a("p",[t._v("BFS 过程逐步构建出 BFS 树。一开始，树仅有根结点，即源结点 s。在扫描到结点 u 时（u 一开始是 s），查看其邻接链表，将邻接链表中的结点标记为“已被发现”状态（实际上就是将此结点加入辅助队列中）。对于 BFS 树而言，对邻接链表的每一个结点 v，将结点 v 和边 (u, v) 同时加入到该树中，让 v 成为 u 的孩子结点，并称结点 u 是结点 v 的"),a("strong",[t._v("前驱")]),t._v("或"),a("strong",[t._v("父结点")]),t._v("。")]),t._v(" "),a("p",[t._v("下面的伪代码描述了 BFS 过程。伪代码中用结点的 color 属性表达三种状态：未被发现、已被发现、已被探索 分别对应 白色 white、灰色 gray、黑色 black。u.p 表示结点 u 的前驱结点，如果没有前驱结点(以及默认值)为 nil。u.d 记录了 BFS 算法所计算出的从源结点 s 到结点 u 的距离(默认值为 inf)。先进先出的辅助队列记为 Q。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("BFS(G, s)\n1  for each vertex u \\in G.V - {s}\n2      u.color = white\n3      u.d = inf\n4      u.p = nil\n5  s.color = gray\n6  s.d = 0\n7  s.p = nil\n8  Q = \\emptyset\n9  EnQueue(Q, s)\n10 while Q is not empty\n11     u = DeQueue(Q)\n12     for each v \\in G.Adj[u]\n13         if v.color == white\n14             v.color = gray\n15             v.d = u.d + 1\n16             v.p = u\n17             EnQueue(Q, v)\n18     u.color = black\n")])])]),a("p",[t._v("算法流程描述：")]),t._v(" "),a("ol",[a("li",[t._v("在 1～4 行，除了源结点 s 外，将其余所有结点 u 的状态标记为“未被发现”，即 color 为白色 white。另外，将 u.d 设置为无穷 inf，表示从源结点不可达结点 u。由于未探索到结点 u，将其前驱结点设置为空 nil。")]),t._v(" "),a("li",[t._v("在 5～7 行，设置源结点 s 的属性。由于已经发现了 s，所以 s.color 设置为灰色 gray。结点 s 到自身的距离为 0。结点 s 为 BFS 树的树根，所以前驱/父结点 u.p 为空 nil。")]),t._v(" "),a("li",[t._v("在 8～9 行，将 s 加入辅助队列 Q，成为其唯一成员。")]),t._v(" "),a("li",[t._v("在 10～18 行的 while 循环中，先进先出地逐个处理队列 Q 中的结点。\n"),a("ol",[a("li",[t._v("在 11 行，先取出 Q 队首结点 u")]),t._v(" "),a("li",[t._v("在 12～18 行，逐个处理 u 的所有邻接结点 v")]),t._v(" "),a("li",[t._v("在 13～17 行，如果 v.color 是白色，表示它未被发现，需要被加入到队列 Q 中。在入队之前，需要设置其属性：\n"),a("ul",[a("li",[t._v("v.color 颜色设置为灰色，表示它已被发现，但是尚未被探索完（所谓探索结束，是其邻接结点都已被处理）。")]),t._v(" "),a("li",[t._v("v.d 是 v 到源结点 s 的距离，这个距离等于 u.d 距离加上 1")]),t._v(" "),a("li",[t._v("v.p 设置前驱/父结点为 u")]),t._v(" "),a("li",[t._v("将 v 入队，之后的 while 循环中 会考察 v 的各个邻接结点")])])]),t._v(" "),a("li",[t._v("for 循环结束，u 的所有邻接结点都被考察了，所以 u 已经被探索结束了。u.color 设置为黑色，保证不会再被加入队列 Q。")])])])]),t._v(" "),a("p",[t._v("另外，其实灰色 gray 表达的 “已被发现”状态可以不必有，这只是用于辅助思考的（队列中的都是灰色结点，但是可以直接设置为黑色）。省掉灰色后，仅有两个状态，因此可以用 1 bit 来表达此状态信息。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-3.png",alt:"graph-basis-3"}})]),t._v(" "),a("p",[t._v("注意：广度优先搜索树的形态可能依赖于 对邻接结点的访问顺序，但是只要源结点 s 不变，那么任意某个结点 u 计算出的 u.d 距离是相同的。")]),t._v(" "),a("p",[t._v("BFS 过程可以确保每个结点的入队次数至多为 1 次，入队出队时间为 O(1)，因此对队列进行操作的总时间为 "),a("code",[t._v("O(|V|)")]),t._v("。算法只在某个结点出队时才对该结点的邻接链表进行扫描，所以每个邻接链表最多只被扫描一次。所以扫描邻接链表的总时间为 "),a("code",[t._v("O(|E|)")]),t._v("。另外，初始化操作的耗时是 "),a("code",[t._v("O(|V|)")]),t._v("，因此 BFS 的总运行时间为 "),a("code",[t._v("O(|V| + |E|)")]),t._v("，这是图 G 的邻接链表大小的一个"),a("strong",[t._v("线性函数")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"bfs-应用：无权图的最短路径"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bfs-应用：无权图的最短路径"}},[t._v("#")]),t._v(" BFS 应用：无权图的最短路径")]),t._v(" "),a("p",[t._v("无(边)权图，或者说每条边的权重值都相等。定义从源结点 s 到结点 v 的"),a("strong",[t._v("最短路径距离")]),t._v(" d(s, v) 为结点 s 到结点 v 之间所有路径里面最少的边数。如果从源结点的 s 到结点 v 之间没有路径，则 d(s, v) = inf 无穷。")]),t._v(" "),a("p",[t._v("称从结点 s 到结点 v 的长度为 d(s, v) 的路径为 s 到 v 的(无权)"),a("strong",[t._v("最短路径")]),t._v("。下面几个引理和推论证明了 BFS 可以正确计算出源结点 s 到最短路径距离。（无权图 单源最短路径）")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 22.1")]),t._v("：给定 G = (V, E)，G 为一个有向图或无向图，设 s \\in V 为任意结点，则对于任意边 (u, v) \\in E，有 d(s, v) <= d(s, u) + 1。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 22.2")]),t._v("：设 G = (V, E) 为一个有向图或无向图，假定 BFS 以给定结点 s \\in V 作为源结点 在图 G 上运行。那么在 BFS 终结时，对于每个结点 v \\in V，BFS 所计算出的 v.d 满足 v.d >= d(s, v)。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 22.3")]),t._v("：假定 BFS 在图 G = (V, E) 上运行的过程中，辅助队列 Q 包含的结点为 "),a("code",[t._v("<v1, v2, ..., vr>")]),t._v("，这里 v1 是队列 Q 的首结点，vr 是队列 Q 的尾结点。那么 vr.d <= v1.d + 1，并且对于 i = 1, 2, ..., r-1 而言，"),a("code",[t._v("vi.d <= v_{i+1}.d")]),t._v("。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("推论 22.4")]),t._v("：假定在执行 BFS 时，结点 vi 和结点 vj 都已经被加入到队列 Q 里，并且 vi 在 vj 之前入队。则在 vj 入队时，有 vi.d <= vj.d。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 22.5")]),t._v("（广度优先搜索的正确性）：设 G = (V, E)，G 为一个有向图或无向图，又假设 BFS 以 s 为源结点在图 G 上运行。那么在算法执行过程中，BFS 将发现从源结点 s "),a("strong",[t._v("可以到达的所有结点")]),t._v(" v \\in V，并在算法终止时，对于所有的 v \\in V，v.d = d(s, v) 最短路径长度。而且，对于任意可以从 s 到达的结点 v (v != s)，从源结点 s 到结点 v 的其中一条"),a("strong",[t._v("最短路径")]),t._v("为从结点 s 到结点 v.p 的最短路径再加上边 (v.p, v)，这提供了从 s 到 v 的最短路径的一种构造方法。")]),t._v(" "),a("h3",{attrs:{id:"广度优先树"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#广度优先树"}},[t._v("#")]),t._v(" 广度优先树")]),t._v(" "),a("p",[t._v("过程 BFS 在对图进行搜索的过程中将创建一棵广度优先树。对于图 G = (V, E) 和源结点 s，定义图 G 的"),a("strong",[t._v("前驱子图")]),t._v("为 Gp = (Vp, Ep)，其中顶点集 $ Vp = {v \\in V: v.p != nil} \\cup {s} $、边集 $ Ep = {(v.p, v): v \\in Vp - {s}} $。")]),t._v(" "),a("p",[t._v("即：除源结点 s 以外 所有前驱非空的顶点集合，以及这些顶点与其前驱的所有关联的边集合。")]),t._v(" "),a("p",[t._v("如果对于所有的 v \\in Vp，G 的子图 Gp 包含一条从源结点 s 到结点 v 的唯一简单路径，且这条路径也是从 s 到 v 的一条(无边权)最短路径，则前驱子图 Gp 是一棵"),a("strong",[t._v("广度优先树")]),t._v("。")]),t._v(" "),a("p",[t._v("在 BFS 算法后，通过下面的程序可以打印出一棵广度优先树。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('PRINT_PATH(G, s, v)\n1  if v == s\n2      print s\n3  elif v.p == nil\n4      print "no path from" s "to" v "exists"\n5  else\n6      PRINT_PATH(G, s, v.p)\n7      print v\n')])])]),a("p",[t._v("因为每次递归调用时的路径都比前一次调用中的路径少一个结点，所以 PRINT_PATH 过程的运行时间是 "),a("code",[t._v("O(|V|)")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"深度优先搜索-dfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度优先搜索-dfs"}},[t._v("#")]),t._v(" 深度优先搜索 DFS")]),t._v(" "),a("p",[a("strong",[t._v("深度优先搜索")]),t._v(" (Depth First Search, DFS) 所使用的策略正如其名：只要有可能，就在图中尽量“深入”。DFS 总是对最近才发现的结点 v 的出发边进行探索，知道该结点的"),a("strong",[t._v("所有出发边都被发现为止")]),t._v("（不重复发现已经发现过的结点，以及已经探索过的出发边）。一旦结点 v 的"),a("strong",[t._v("所有出发边都被发现")]),t._v("，则“"),a("strong",[t._v("回溯")]),t._v("”到 v 的前驱结点 v.p，来考察 v.p 的其余出发边（除了已经探索过的出发边）。")]),t._v(" "),a("p",[t._v("许多递归算法的递归树就是一棵深度优先搜索树：先往一个方向(选择)走到尽头，再回溯，考虑另一个选择。")]),t._v(" "),a("p",[t._v("广度优先搜索的前驱子图形成一棵广度优先搜索树，但 DFS 的前驱子图可能由多棵树组成，因为搜索可能从多个源结点重复进行。多棵"),a("strong",[t._v("深度优先搜索树")]),t._v("形成"),a("strong",[t._v("深度优先森林")]),t._v("。")]),t._v(" "),a("p",[t._v("与 BFS 类似，DFS 过程也用结点的 color 属性表达三种状态：未被发现、已被发现、已被探索 分别对应 白色 white、灰色 gray、黑色 black。这个方法可以保证每个结点 u 仅在一棵深度优先树中出现。即：以一个源结点 s 开始 DFS 过程得了 DFS 树，就会将树中的所有结点标记为黑色，而其它 DFS 过程不会考虑这些黑色结点。因此，所有深度优先树是不相交的 (disjoint)。")]),t._v(" "),a("p",[t._v("除了创建一个深度优先搜索森林外，DFS 算法还在每个结点上附加一个从 1 开始编号的正整数"),a("strong",[t._v("时间戳")]),t._v(" (timestamp)。每个结点 v 有两个时间戳：第一个时间戳 v.d （这里的 d 是 discover 的意思）记录结点 v 第一次被发现的时间（也就是涂上灰色 gray 的时刻）；第二个时间戳 v.f（这里的 f 是 finish 的意思）记录了完成对 v 的邻接链表扫描的时间（也就是涂上黑色 black 的时刻）。")]),t._v(" "),a("p",[t._v("由于总共 "),a("code",[t._v("|V|")]),t._v(" 个结点中 每个结点只能有一个发现事件和一个完成事件，所以这些时间戳都是处于 "),a("code",[t._v("1 ~ |V|")]),t._v(" 之间的整数。而且很显然，结点 u 在时刻 u.d 之前为白色，在时刻 u.d 和 u.f 之间为灰色，在时刻 u.f 之后为黑色。这些时间戳为图结构的分析提供了重要的线索，通常能够有助于推断 DFS 算法的行为。")]),t._v(" "),a("p",[t._v("下面的 DFS 伪代码中，每个结点 u 除了前述 u.d 和 u.f 这两个时间戳属性，还有 u.p 表示结点 u 的前驱结点，如果没有前驱结点(以及默认值)为 nil。变量 time 是一个全局变量，用于计算时间戳。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("DFS(G)\n1  for each vertex u \\in G.V\n2      u.color = white\n3      u.p = nil\n4  time = 0\n5  for each vertex u \\in G.V\n6      if u.color == white\n7          DFS_VISIT(G, u)\n")])])]),a("ol",[a("li",[t._v("在 1～3 行，初始化图 G 的每个顶点 u \\in G.V，让其颜色为白色 white、前驱结点/父结点为空 nil。")]),t._v(" "),a("li",[t._v("在 4 行，初始化时间戳 time")]),t._v(" "),a("li",[t._v("在 5～7 行，以每个白色的结点作为 DFS 算法的源结点（也即 DFS 树的根结点）调用 "),a("code",[t._v("DFS_VISIT")]),t._v(" 过程进行深度优先搜索。")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("DFS_VISIT(G, u)\n1  time = time + 1\n2  u.d = time\n3  u.color = gray\n4  for each v \\in G.Adj[u]\n5      if v.color == white\n6          v.p = u\n7          DFS_VISIT(G, v)\n8  u.color = black\n9  time = time + 1\n10 u.f = time\n")])])]),a("ol",[a("li",[t._v("在 1～3 行，时间戳增长，并赋予给结点 u 的发现时间 u.d。由于 u 已被发现，其颜色置为灰色 gray（已被发现状态）。")]),t._v(" "),a("li",[t._v("在 4～7 行，对于 u 的每个邻接结点 v，如果 v 是白色的（未被发现状态），则将 v 的前驱置为 u，并以 v 为起点递归地进行 DFS 搜索。")]),t._v(" "),a("li",[t._v("在 8～10 行，由于此时 u 的邻接结点都已被处理结束，故置 u 的颜色为黑色（处理完毕状态）。随后时间戳增长，并赋予给结点 u 的完成时间 u.f。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-4.png",alt:"graph-basis-4"}})]),t._v(" "),a("h3",{attrs:{id:"深度优先搜索的性质"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度优先搜索的性质"}},[t._v("#")]),t._v(" 深度优先搜索的性质")]),t._v(" "),a("p",[t._v("DFS 提供的是关于图结构很重要的信息。其基本性质是：DFS 生成的前驱子图 Gp 形成一个由多棵树所构成的森林，DFS 树的结构与 "),a("code",[t._v("DFS_VISIT")]),t._v(" 的递归调用结构完全对应。")]),t._v(" "),a("p",[t._v("另一个重要性质是：结点发现时间和完成时间具有"),a("strong",[t._v("括号化结构")]),t._v(" (parenthesis structure)。如果以左括号 “(u” 来表示结点 u 的发现，以右括号 “u)” 来表示结点 u 的完成，则发现和完成的历史记载形成一个“合法”（左右括号匹配）的表达式。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 22.7")]),t._v("（"),a("strong",[t._v("括号化定理")]),t._v("）：在对有向或无向图 G = (V, E) 进行的任意深度优先搜索中，对于任意两个结点 u 和 v 来说，下面三种情况有且仅哦与一种成立：")]),t._v(" "),a("ol",[a("li",[t._v("若区间 "),a("code",[t._v("[u.d, u.f]")]),t._v(" 和区间 "),a("code",[t._v("[v.d, v.f]")]),t._v(" 完全分离，则在深度优先森林中，结点 u 不是 v 的后代，结点 v 也不是 u 的后代。")]),t._v(" "),a("li",[t._v("若区间 "),a("code",[t._v("[u.d, u.f]")]),t._v(" 和完全包含在区间 "),a("code",[t._v("[v.d, v.f]")]),t._v(" 内，则在深度优先树中，结点 u 是 v 的后代。")]),t._v(" "),a("li",[t._v("若区间 "),a("code",[t._v("[u.d, u.f]")]),t._v(" 和完全包含了区间 "),a("code",[t._v("[v.d, v.f]")]),t._v("，则在深度优先树中，结点 v 是 u 的后代。")])]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("推论 22.8")]),t._v("（"),a("strong",[t._v("后代区间的嵌套")]),t._v("）：在有向或无向图 G 的深度优先森林中，结点 v 是结点 u 的真后代 当且仅当 u.d < v.d < v.f < u.f 成立。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 22.9")]),t._v("（"),a("strong",[t._v("白色路径定理")]),t._v("）：在有向或无向图 G 的深度优先森林中，结点 v 是结点 u 的后代 当且仅当 在发现结点 u 的时刻 u.d，存在一条从结点 u 到结点 v 的"),a("strong",[t._v("全部由白色结点所构成的路径")]),t._v("。")]),t._v(" "),a("p",[t._v("《CLRS》练习 22.3-13：一个深度优先搜索的应用是判断一个有向图是否为"),a("strong",[t._v("单连通图")]),t._v("：对于有向图 G = (V, E) 而言，如果 "),a("code",[t._v("u->v")]),t._v(" 意味着图 G 至多包含一条从 u 到 v 的简单路径，则图 G 是"),a("strong",[t._v("单连通图")]),t._v(" (single connected)。")]),t._v(" "),a("p",[t._v("《CLRS》练习 22.5-17：给定有向图 G = (V, E)，如果对于所有结点对 u, v \\in V，有路径 "),a("code",[t._v("u->v")]),t._v(" 或 "),a("code",[t._v("v->u")]),t._v("，则 G 是"),a("strong",[t._v("半连通")]),t._v("的。或者说，假如将此有向图的有向边转为无向边，那么如果此无向图是连通图，则原有向图 G 是半连通的。")]),t._v(" "),a("h3",{attrs:{id:"深度优先搜索-边的分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度优先搜索-边的分类"}},[t._v("#")]),t._v(" 深度优先搜索-边的分类")]),t._v(" "),a("p",[t._v("深度优先搜索的另一个有趣的性质是：可以通过搜索来对输入图 G = (V, E) 的边进行分类。每条边的类型可以提供关于图的重要信息。例如，有向图是无环图 当且仅当 深度优先搜索不产生“后向”边。（《CLRS》引理 22.11）")]),t._v(" "),a("p",[t._v("对于在图上运行深度优先搜索算法所生成的深度优先森林 Gp，可以定义如下 4 种边的类型：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("树边")]),t._v("：为深度优先森林 Gp 中的边。如果结点 v 是因算法对边 (u, v) 的探索而"),a("strong",[t._v("首先被发现")]),t._v("，则 (u, v) 是一条树边。")]),t._v(" "),a("li",[a("strong",[t._v("后向边")]),t._v("：后向边 (u, v) 是将结点 u 连接到其在深度优先树中的(一个)祖先结点 v 的边。由于有向图中可以有自循环，自循环也被认为是后向边。（因此没有后向边的有向图是无环图）")]),t._v(" "),a("li",[a("strong",[t._v("前向边")]),t._v("：将结点 u 连接到其 在深度优先树中的一个后代结点 v 的边 (u, v)。（某祖先结点跨越树边 直接连接到其某个后代结点）")]),t._v(" "),a("li",[a("strong",[t._v("横向边")]),t._v("：除前三类边外的所有边。这些边可以连接同一棵深度优先树中的结点，只要其中一个结点不是另外一个结点的祖先。也可以连接不同深度优先树中的两个结点。")])]),t._v(" "),a("p",[t._v("进一步理解：前三类边是同一棵深度优先树中，某个"),a("strong",[t._v("祖先")]),t._v("结点 a (ancestor) 与其某个"),a("strong",[t._v("后代")]),t._v("结点 o (offspring) 的有向边：")]),t._v(" "),a("ul",[a("li",[t._v("如果有向边是 "),a("code",[t._v("<a, o>")]),t._v(" "),a("ul",[a("li",[t._v("如果 a 是 o 的父结点（也即 o 是 a 的直接孩子结点），那么是"),a("strong",[t._v("树边")])]),t._v(" "),a("li",[t._v("否则是"),a("strong",[t._v("前向边")])])])]),t._v(" "),a("li",[t._v("否则是"),a("strong",[t._v("后向边")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-5.png",alt:"graph-basis-5"}})]),t._v(" "),a("p",[t._v("在 DFS 过程中，当"),a("strong",[t._v("第一次探索")]),t._v("边 (u, v) 时，根据"),a("strong",[t._v("结点 v 的颜色")]),t._v("可以判断该边的信息：")]),t._v(" "),a("ul",[a("li",[t._v("如果此时结点 v 是"),a("strong",[t._v("白色")]),t._v("，表明该边 (u, v) 是一条"),a("strong",[t._v("树边")]),t._v("。")]),t._v(" "),a("li",[t._v("如果此时结点 v 是"),a("strong",[t._v("灰色")]),t._v("，表明该边 (u, v) 是一条"),a("strong",[t._v("后向边")]),t._v("。")]),t._v(" "),a("li",[t._v("如果此时结点 v 是"),a("strong",[t._v("黑色")]),t._v("，表明该边 (u, v) 是一条"),a("strong",[t._v("前向边")]),t._v("或者"),a("strong",[t._v("横向边")]),t._v("。")])]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 22.10")]),t._v("：在对"),a("strong",[t._v("无向图")]),t._v(" G 进行"),a("strong",[t._v("深度优先搜索")]),t._v("时，每条边要么是"),a("strong",[t._v("树边")]),t._v("，要么是"),a("strong",[t._v("后向边")]),t._v("。不会出现前向边和横向边。")]),t._v(" "),a("h2",{attrs:{id:"dfs-拓扑排序"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dfs-拓扑排序"}},[t._v("#")]),t._v(" DFS - 拓扑排序")]),t._v(" "),a("p",[t._v("深度优先搜索 DFS 有一个重要应用是 对"),a("strong",[t._v("有向无环图")]),t._v(" (Directed Acyclic Graph, DAG) 进行"),a("strong",[t._v("拓扑排序")]),t._v(" (Topological Sorting)。")]),t._v(" "),a("p",[t._v("对于一个有向无环图 G = (V, E) 而言，其"),a("strong",[t._v("拓扑排序")]),t._v("是 G 中所有结点的一种"),a("strong",[t._v("线性次序")]),t._v("，该次序满足如下条件：如果图 G 包含有向边 "),a("code",[t._v("<u, v>")]),t._v("，则结点 u 在拓扑排序中处于结点 v 的前面。可以将图的拓扑排序看作是将图的所有结点在一条水平线上排开，图的所有有向边的从左指向右。拓扑排序和数组元素的排序（按关键字进行升序/降序排列）是不同的。")]),t._v(" "),a("p",[t._v("许多实际应用都需要使用有向无环图来指明时间的优先次序。比如计算依赖图，要先完成某些计算 才能开始进行另一些计算，因此先进行拓扑排序 可以很好地安排计算次序。")]),t._v(" "),a("p",[t._v("注意到，如果图 G 包含环路，则无法排出一个线性次序，因此拓扑排序的一个重要应用是有向图环路检测。可以在 "),a("code",[t._v("O(|V|)")]),t._v(" 的运行时间（与边集 E 无关）判断一个给定的"),a("strong",[t._v("无向图")]),t._v(" G = (V, E) 是否包含一个环路。（《CLRS》练习 22.4-3）")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-6.png",alt:"graph-basis-6"}})]),t._v(" "),a("p",[t._v("下面的伪代码展示了如何对一个有向无环图进行拓扑排序：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("TOPOLOGICAL_SORT(G)\n1  call DFS(G) to compute finishing times v.f for each vertex v\n2  as each vertex is finished, insert it into the front of a linked list\n3  return the linked list of vertices\n")])])]),a("p",[t._v("可以在 "),a("code",[t._v("\\Theta(|V| + |E|)")]),t._v(" 的时间内完成拓扑排序。因为 DFS 需要 "),a("code",[t._v("\\Theta(|V| + |E|)")]),t._v(" 的运行时间。将结点插入到链表最前端所需的时间为 O(1)，而一共只有 "),a("code",[t._v("|V|")]),t._v(" 个结点需要插入。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 22.11")]),t._v("：一个有向图 G = (V, E) 是"),a("strong",[t._v("无环的")]),t._v(" 当且仅当 对其进行的深度优先搜索"),a("strong",[t._v("不产生后向边")]),t._v("。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 22.12")]),t._v("：拓扑排序算法 "),a("code",[t._v("TOPOLOGICAL_SORT")]),t._v(" 生成的是有向无环图的拓扑排序。")]),t._v(" "),a("h2",{attrs:{id:"强连通分量"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#强连通分量"}},[t._v("#")]),t._v(" 强连通分量")]),t._v(" "),a("p",[t._v("深度优先搜索的有一个经典应用是：将"),a("strong",[t._v("有向图")]),t._v("分解为"),a("strong",[t._v("强连通分量")]),t._v(" (Strongly Connected Component, SCC)。有向图 G = (V, E) 的强连通分量是一个最大结点集合 $ C \\subseteq V $，对于该集合中的任意一对结点 u 和 v 都彼此可达（存在到达彼此的有向路径）。")]),t._v(" "),a("p",[t._v("许多针对有向图的算法都以此种分解操作开始。在将图分解为强连通分量后，这些算法将分别运行在每个强连通分量上，然后根据连通分量之间的连接结构 将各个结果组合起来，从而获得最终所需的结果。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-7.png",alt:"graph-basis-7"}})]),t._v(" "),a("p",[t._v("用于寻找强连通分量的算法需要用到图 G = (V, E) 的转置 G^T = (V, E^T)，其中 $ E^T = {(u, v): (v, u) \\in E} $。给定图 G 的邻接链表，创建 G^T 的时间为 "),a("code",[t._v("O(|V| + |E|)")]),t._v("。由于强连通分量 和 图转置的性质，图 G 和图 G^T 的强连通分量是完全相同的。")]),t._v(" "),a("p",[t._v("如下伪代码给出了计算有向图 G = (V, E) 强连通分量的 "),a("code",[t._v("\\Theta(|V| + |E|)")]),t._v(" 线性时间算法，其中使用了两次深度优先搜索。这两次 DFS 一次运行在图 G 上，一次运行在其转置图 G^T 上。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("STRONGLY_CONNECTED_COMPONENT(G)\n1  call DFS(G) to compute finishing times u.f for each vertex u\n2  compute G^T\n3  call DFS(G^T), but in the main loop of DFS, consider the vertices\n       in order of decreasing u.f (as computed in line 1)\n4  output the vertices of each tree in the depth-first forest formed in line 3\n       as a separate strongly connected component\n")])])]),a("p",[t._v("此算法背后的思想来自于分量图 G^{SCC} = (V^{SCC}, E^{SCC}) 的一个关键性质，这个关键性质的定义如下：")]),t._v(" "),a("p",[t._v("假定图 G 有强连通分量 C1, C2, ..., Ck。结点集 V^{SCC} 为 {v1, v2, ..., vk}，对于图 G 的每个强连通分量 Ci 而言，该集合包含代表该分量的结点 vi。如果对于某个 x \\in Ci 和 y \\in Cj，图 G 包含一条有向边 (x, y)，则边 (vi, vj) \\in E^{SCC}。从另一个角度来看，通过"),a("strong",[t._v("收缩")]),t._v("所有相邻结点都在同一个强连通分量中的边，剩下的图就是 G^{SCC}。")]),t._v(" "),a("p",[t._v("分量图的关键性质就是：分量图是一个"),a("strong",[t._v("有向无环图")]),t._v("。该结论可由如下引理所推出。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 22.13")]),t._v("：设 C 和 C' 为有向图 G = (V, E) 的两个不同的强连通分量，设结点 u, v \\in C，结点 u', v' \\in C'，假定图 G 包含一条从结点 u 到结点 u' 的路径 "),a("code",[t._v("u->u'")]),t._v("。那么图 G 不可能包含一条从结点 v' 到结点 v 的路径 "),a("code",[t._v("v'->v")]),t._v("。（从而形成环路）")]),t._v(" "),a("p",[t._v("下面将结点的发现时间 d 和完成时间 f 的概念推广到结点集合上：如果结点集合 $ U \\subseteq V $，则定义 d(U) = min{u.d} 和 f(U) = min{u.f}。即：d(U) 和 f(U) 分别是结点集合 U 中所有结点里 最早的发现时间 和 最晚的完成时间。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 22.14")]),t._v("：设 C 和 C' 为有向图 G = (V, E) 的两个不同的强连通分量。假如存在一条边 (u, v) \\in E，这里 u \\in C 且 v \\in C'，则有 "),a("code",[t._v("f(C) > f(C')")]),t._v("。")]),t._v(" "),a("p",[t._v("下面的推论表明，转置图 G^T 中连接不同强连通分量的每条边 都是从完成时间较早（第一次 DFS 所计算出的完成时间）的分量 指向完成时间较迟的分量。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("推论 22.15")]),t._v("：设 C 和 C' 为有向图 G = (V, E) 的两个不同的强连通分量。假如存在一条边 (u, v) \\in E^T，这里 u \\in C 且 v \\in C'，则有 "),a("code",[t._v("f(C) < f(C')")]),t._v("。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 22.16")]),t._v("：算法 "),a("code",[t._v("STRONGLY_CONNECTED_COMPONENT")]),t._v(" 能够正确计算出有向图 G 的强连通分量。")]),t._v(" "),a("p",[t._v("从另一个角度来看第二次 DFS 的运行过程，考虑转置图 G^T 的分量图 (G^T)^{SCC}。如果将第二次 DFS 所访问的每个强连通分量 映射到 (G^T)^{SCC} 的一个结点上，则此次 DFS 将以"),a("strong",[t._v("拓扑排序次序的逆序")]),t._v(" 来访问 (G^T)^{SCC} 中的结点。如果将 (G^T)^{SCC} 中的边翻转过来，则将获得 ((G^T)^{SCC})^T。因为 ((G^T)^{SCC})^T = G^{SCC}，所以第二次 DFS 是以"),a("strong",[t._v("拓扑排序的次序")]),t._v("来访问 G^{SCC} 中的结点的。")]),t._v(" "),a("h2",{attrs:{id:"其它-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其它-2"}},[t._v("#")]),t._v(" 其它")]),t._v(" "),a("h3",{attrs:{id:"衔接点、桥和双连通分量"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#衔接点、桥和双连通分量"}},[t._v("#")]),t._v(" 衔接点、桥和双连通分量")]),t._v(" "),a("p",[t._v("设 G = (V, E) 为一个"),a("strong",[t._v("连通无向图")]),t._v("。图 G 的"),a("strong",[t._v("衔接点")]),t._v("是指图 G 中的一个结点，如果删除该结点将导致图不连通。图 G 的"),a("strong",[t._v("桥")]),t._v("是指图 H 中的一条边，如果删除该边将导致图不连通。图 G 的"),a("strong",[t._v("双连通分量")]),t._v("是指一个最大的边集合，里面的任意两条边都处于同一条简单回路中。可以用 DFS 来判断图 G 的衔接点、桥和双连通分量。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/graph-basis/graph-basis-8.png",alt:"graph-basis-8"}})]),t._v(" "),a("h3",{attrs:{id:"欧拉回路"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#欧拉回路"}},[t._v("#")]),t._v(" 欧拉回路")]),t._v(" "),a("p",[t._v("强连通有向图 G = (V, E) 中的一个欧拉回路是指一条"),a("strong",[t._v("遍历图 G 中每条边恰好一次")]),t._v("的"),a("strong",[t._v("环路")]),t._v("。性质如下：")]),t._v(" "),a("ul",[a("li",[t._v("强连通有向图 G 有一条欧拉回路 当且仅当 对于图中的每个结点 v，都有：入度 in-degree(v) == 出度 out-degree(v)")]),t._v(" "),a("li",[t._v("连通无向图 G 有一条欧拉回路 当且仅当 对于图中的每个结点 v，其度数 degree(v) 均为偶数。\n"),a("ul",[a("li",[t._v("由于是连通图，所以不存在度为 0 的孤立点。")]),t._v(" "),a("li",[t._v("又由于度数是非负的，所以此欧拉回路上的每个结点度数是 >= 2 的偶数。")])])])]),t._v(" "),a("h2",{attrs:{id:"python-代码范例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#python-代码范例"}},[t._v("#")]),t._v(" Python 代码范例")]),t._v(" "),a("p",[t._v("Python 环境：Python 3.7")]),t._v(" "),a("h3",{attrs:{id:"邻接表-邻接矩阵"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#邻接表-邻接矩阵"}},[t._v("#")]),t._v(" 邻接表 & 邻接矩阵")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding:utf-8 -*-")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""=================================================\n@Project : algorithm/graph_theory/graph_basis\n@File    : graph-basis.py\n@Author  : YuweiYin\n@Date    : 2020-05-29\n=================================================="""')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n图的表示 - 邻接表 & 邻接矩阵\n\n参考资料：\nIntroduction to Algorithm (aka CLRS) Third Edition - Chapter 22\n"""')]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边结构体，表达边的信息，可随任务自定义 (增添其它值元素 val 对象)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_v  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的起始顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_v      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的终止顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的权重值 (默认值为 1，如果全部边的权重都相同，那图 G 就是无权图)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" is_directed  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# True 则表明此边是有向边，False 为无向边")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对无向边而言，起始顶点和终止顶点可以互换")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'->'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("\\\n               "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t weight:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t is_directed:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于邻接表的顶点结构体")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里是用散列表 (而不是用链表) 来表达某顶点的所有邻接顶点")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("VertexList")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的关键字 key (通常为顶点序号、唯一标志符)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("val "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的值元素 val (可自定义为任意对象，为结点附带的信息)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的邻居字典，key 为邻居的关键字，value 为 Edge 边结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是有向图，本结点为关联边的出发点 from_v，其邻居关联边的终止点 to_v")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is connected to:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增添本顶点的邻居 neighbor，以字典结构存储")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注意：如果不允许图有自环/自圈，那么在增添邻居/边 的时候要禁止增添 self.neighbor[self.key] 项。这里暂不限制")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_neighbor")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 neighbor 字典里已有此 key，则会覆盖。起到了更新边信息的作用")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以 Edge 边结构体来增添本顶点的邻居 neighbor")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查输入 edge 的合法性")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 edge 是有向边，那么本结点需要是 edge 的出发点 from_v")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 edge 是无向边，那么本结点需要是 edge 的出发点 from_v 或结束点 to_v 之一")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 from_v 和 to_v 交换")]),t._v("\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回本顶点的所有邻接顶点(的关键字/序号) 数组")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_connections")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回本顶点到邻居 neighbor 的 Edge 边结构体")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_weight")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" neighbor "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接表的图结构，通常适合稀疏图")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入顶点结构体列表、边结构体列表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AdjacencyList")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造函数，edges 必须是二维数组，内部维度是一系列长度为 2 或者 3 的数组，")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 分别代表着边的起始顶点 start、终止顶点 end 以及边权(可选)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edges         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的边列表")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertices   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的顶点列表 (可以从下标映射到顶点)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点映射到其下标 (既是邻接矩阵的行/列下标，也是 vertices 列表的下标)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点的关键字/唯一标志符映射到其下标")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本图的顶点字典, key 为顶点的序号，val 为顶点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertex\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 若 edges 合法，则进行边初始化处理")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是有向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是无向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" to_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断 key 号为 _key 的顶点是否位于顶点列表中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__contains__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类迭代器方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__iter__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("iter")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取图中 key 号为 _key 的顶点，如果没有此顶点则返回 None")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_vertex")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接表 - 图转置")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("graph_transposition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其实如果是无向边，无需处理，但这里还是转了")]),t._v("\n            temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于邻接矩阵的顶点结构体 (比 VertexList 简单)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里是用散列表 (而不是用链表) 来表达某顶点的所有邻接顶点")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("VertexMatrix")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的关键字 key (通常为顶点序号、唯一标志符)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("val "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的值元素 val (可自定义为任意对象，为结点附带的信息)")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Vertex key: '")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接矩阵的图结构，通常适合稠密图")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入顶点结构体列表、边结构体列表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AdjacencyMatrix")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edges         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的边列表")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertices   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的顶点列表 (可以从下标映射到顶点)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点映射到其下标 (既是邻接矩阵的行/列下标，也是 vertices 列表的下标)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点的关键字/唯一标志符映射到其下标")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建邻接矩阵(二维方阵)，adj[x][y] 为 True 表示存在边 (x, y)")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 矩阵中的每个元素，如果是无权图，可以仅为布尔值，True 表示边存在、False 表示边不存在")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是带权图，可以用实数值代表边的权值，如果边不存在，可以设置为特殊数值，比如 inf 无穷")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果边的信息比较多，矩阵中的每个元素也可以设置为 Edge 边结构体，在其中增添任务需要的信息")]),t._v("\n        v_num "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 顶点数目")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" v_num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里仅仅存储边是否存在的布尔值信息")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 若 edges 合法，则进行边初始化处理")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边起点的关键字 key")]),t._v("\n                to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边终点的关键字 key")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将顶点关键字 key 转为下标 index，然后设置 adj[from][to] 为 True")]),t._v("\n                    from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" v_num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" v_num\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是无向边，则 adj[to][from] 也设置为 True")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断 key 号为 _key 的顶点是否位于顶点列表中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__contains__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取图中 key 号为 _key 的顶点，如果没有此顶点则返回 None")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_vertex")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接矩阵 - 图转置")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("graph_transposition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其实如果是无向边，无需处理，但这里还是转了")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先获取 key")]),t._v("\n            from_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n            to_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 交换 key")]),t._v("\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_key\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_key\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把 key 转成 index")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index\n            from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 修改邻接矩阵")]),t._v("\n            temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于构造邻接表的顶点的 key/val 信息列表")]),t._v("\n    list_vertices_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("400")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("600")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("700")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("800")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于构造邻接矩阵的顶点的 key/val 信息列表")]),t._v("\n    matrix_vertices_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("111")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("222")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("333")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("444")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("555")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("666")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 无向边的 from/to/weight/is_directed 信息列表")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# is_directed 为 True 表示此边为有向边，否则为无向边")]),t._v("\n    bi_edges_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 有向边的 from/to/weight/is_directed 信息列表")]),t._v("\n    di_edges_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据前述列表信息构造结点列表")]),t._v("\n    list_vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    matrix_vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    bi_edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    di_edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" list_vertices_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        list_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" matrix_vertices_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        matrix_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" e "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" bi_edges_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        bi_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" e "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" di_edges_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        di_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    start "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建邻接表 (用邻接链表+无向图 执行 BFS)")]),t._v("\n    adj_l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("list_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bi_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建邻接矩阵 (用邻接矩阵+有向图 执行 DFS)")]),t._v("\n    adj_m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdjacencyMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("matrix_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" di_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 可以设置断点查看 adj_l 邻接表 和 adj_m 邻接矩阵")]),t._v("\n    end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Running Time: %.5f ms'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("h3",{attrs:{id:"广度优先搜索-bfs-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#广度优先搜索-bfs-2"}},[t._v("#")]),t._v(" 广度优先搜索 BFS")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding:utf-8 -*-")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""=================================================\n@Project : algorithm/graph_theory/graph_basis\n@File    : bfs.py\n@Author  : YuweiYin\n@Date    : 2020-05-30\n=================================================="""')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" queue\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n图的搜索 - 广度优先搜索 (Breadth First Search, BFS)\n\n参考资料：\nIntroduction to Algorithm (aka CLRS) Third Edition - Chapter 22\n"""')]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边结构体，表达边的信息，可随任务自定义 (增添其它值元素 val 对象)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_v  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的起始顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_v      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的终止顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的权重值 (默认值为 1，如果全部边的权重都相同，那图 G 就是无权图)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" is_directed  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# True 则表明此边是有向边，False 为无向边")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对无向边而言，起始顶点和终止顶点可以互换")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'->'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("\\\n               "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t weight:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t is_directed:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于邻接表的顶点结构体")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里是用散列表 (而不是用链表) 来表达某顶点的所有邻接顶点")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("VertexList")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" distance"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的关键字 key (通常为顶点序号、唯一标志符)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("val "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的值元素 val (可自定义为任意对象，为结点附带的信息)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的邻居字典，key 为邻居的关键字，value 为 Edge 边结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是有向图，本结点为关联边的出发点 from_v，其邻居关联边的终止点 to_v")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''下面是用于 BFS 的属性'''")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# False 为"白色"，表示未被发现；True 为"黑色"，表示已经探索结束')]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" distance  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点距离源结点的距离 (最短简单路径的边数)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点的前驱结点/广度优先搜索树的父结点")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is connected to:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增添本顶点的邻居 neighbor，以字典结构存储")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注意：如果不允许图有自环/自圈，那么在增添邻居/边 的时候要禁止增添 self.neighbor[self.key] 项。这里暂不限制")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_neighbor")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 neighbor 字典里已有此 key，则会覆盖。起到了更新边信息的作用")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以 Edge 边结构体来增添本顶点的邻居 neighbor")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查输入 edge 的合法性")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 edge 是有向边，那么本结点需要是 edge 的出发点 from_v")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 edge 是无向边，那么本结点需要是 edge 的出发点 from_v 或结束点 to_v 之一")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 from_v 和 to_v 交换")]),t._v("\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回本顶点的所有邻接顶点(的关键字/序号) 数组")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_connections")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回本顶点到邻居 neighbor 的 Edge 边结构体")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_weight")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" neighbor "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接表的图结构，通常适合稀疏图")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入顶点结构体列表、边结构体列表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AdjacencyList")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造函数，edges 必须是二维数组，内部维度是一系列长度为 2 或者 3 的数组，")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 分别代表着边的起始顶点 start、终止顶点 end 以及边权(可选)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edges         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的边列表")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertices   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的顶点列表 (可以从下标映射到顶点)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点映射到其下标 (既是邻接矩阵的行/列下标，也是 vertices 列表的下标)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点的关键字/唯一标志符映射到其下标")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本图的顶点字典, key 为顶点的序号，val 为顶点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertex\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 若 edges 合法，则进行边初始化处理")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是有向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是无向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" to_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断 key 号为 _key 的顶点是否位于顶点列表中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__contains__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类迭代器方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__iter__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("iter")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取图中 key 号为 _key 的顶点，如果没有此顶点则返回 None")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_vertex")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接表 - 图转置")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("graph_transposition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其实如果是无向边，无需处理，但这里还是转了")]),t._v("\n            temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 广度优先搜索 (Breadth First Search, BFS)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BFS")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化各个结点距离源结点的距离为 inf 无穷")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从源结点 s 到目标结点 v 的一条最短路径上的所有结点(的关键字)")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入：graph 图结构，start_key 源顶点的关键字，is_adj_link 为 True 表示图为邻接表结构，False 表示为邻接矩阵结构")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("do_bfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_adj_link"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" is_adj_link"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果图是邻接表结构")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_bfs_link"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果图是邻接矩阵结构")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_bfs_matrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对邻接表结构的图 graph，以 start_v 为源结点进行 BFS")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_bfs_link")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 start_key 源顶点的关键字 转为源结点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" start_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            start_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'输入的 start_key 不是任何顶点的关键字，BFS 失败'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 除了源结点 s 外，将其余所有结点 u 的状态标记为“未被发现”，即 color 为白色 white")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 另外，将 u.d 设置为无穷 inf，表示从源结点不可达结点 u。由于未探索到结点 u，将其前驱结点设置为空 nil")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 设置源结点 s 的属性。由于已经发现了 s，所以 s.color 设置为灰色 gray。结点 s 到自身的距离为 0")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 结点 s 为 BFS 树的树根，所以前驱/父结点 u.p 为空 nil")]),t._v("\n        start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n        start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 将 s 加入辅助队列 Q，成为其唯一成员")]),t._v("\n        aux_queue "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Python 内建 queue 队列对象")]),t._v("\n        aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 在 while 循环中，先进先出地逐个处理队列 Q 中的结点")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("empty"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.1. 先取出 Q 队首结点 u")]),t._v("\n            u "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.2. 逐个处理 u 的所有邻接结点 v")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先从 u 的关联边 edge 中得到邻接结点 v 的关键字")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 有向边，则邻接结点 v 的关键字是 edge 的终止结点")]),t._v("\n                    v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 无向边，则先设置邻接结点 v 的关键字是 edge 的终止结点")]),t._v("\n                    v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 但如果终止结点是 u，那么设置 邻接结点 v 的关键字是 edge 的起始结点")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果起始结点也是 u，则表示该边为无向图的自环，不必再处理")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 再从关键字 v_key 获取结点 v")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" v_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index\n                v_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.3. 如果 v.color 是白色，表示它未被发现，需要被加入到队列 Q 中。在入队之前，需要设置其属性：")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.color 颜色设置为灰色，表示它已被发现，但是尚未被探索完（所谓探索结束，是其邻接结点都已被处理）。")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.d 是 v 到源结点 s 的距离，这个距离等于 u.d 距离加上 1")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.p 设置前驱/父结点为 u")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - 将 v 入队，之后的 while 循环中 会考察 v 的各个邻接结点")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n                    v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n                    v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u\n                    aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. for 循环结束，u 的所有邻接结点都被考察了，所以 u 已经被探索结束了。u.color 设置为黑色，保证不会再被加入队列 Q。")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# u.color = True")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对邻接矩阵结构的图 graph，以 start_v 为源结点进行 BFS")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_bfs_matrix")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 BFS 之后，打印出所有结点(的关键字)及其距离")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("print_vertex_distance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 BFS 之后，打印出从源结点 s 到目标结点 v 的一条最短路径上的所有结点(的关键字)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此处图结构为邻接表 adj_l")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_path")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'get_path: 输入的参数类型不合法'")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 key 关键字 转为顶点点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" start_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            start_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'输入的 start_key 不是任何顶点的关键字'")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" end_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            end_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("end_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" end_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            end_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("end_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'输入的 end_key 不是任何顶点的关键字'")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_get_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_get_path")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" end_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'No path from '")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' to '")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' exists.'")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先获取其前驱结点/父结点的关键字，再获取本结点的关键字")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_get_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造图同《CLRS》Chapter 22.2 的 BFS 图")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于构造邻接表的顶点的 key/val 信息列表")]),t._v("\n    list_vertices_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("400")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("600")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("700")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("800")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 无向边的 from/to/weight/is_directed 信息列表")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# is_directed 为 True 表示此边为有向边，否则为无向边")]),t._v("\n    bi_edges_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据前述列表信息构造结点列表")]),t._v("\n    list_vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    bi_edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" list_vertices_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        list_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" e "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" bi_edges_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        bi_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建邻接表 (用邻接链表+无向图 执行 BFS)")]),t._v("\n    adj_l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("list_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bi_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行 BFS 过程")]),t._v("\n    start_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),t._v("\n    start "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    bfs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BFS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("do_bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_adj_link"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查看 BFS 结果")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n源顶点为 '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'BFS 结果 (各顶点关键字 及 与源顶点的距离)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_vertex_distance"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 打印(无权)最短路径")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n(无权) 最短路径'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v_info "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" list_vertices_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Running Time: %.5f ms'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("h3",{attrs:{id:"深度优先搜索-dfs-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度优先搜索-dfs-2"}},[t._v("#")]),t._v(" 深度优先搜索 DFS")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding:utf-8 -*-")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""=================================================\n@Project : algorithm/graph_theory/graph_basis\n@File    : dfs.py\n@Author  : YuweiYin\n@Date    : 2020-05-30\n=================================================="""')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n图的搜索 - 深度优先搜索 (Depth First Search, BFS)\n\n参考资料：\nIntroduction to Algorithm (aka CLRS) Third Edition - Chapter 22\n"""')]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边结构体，表达边的信息，可随任务自定义 (增添其它值元素 val 对象)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_v  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的起始顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_v      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的终止顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的权重值 (默认值为 1，如果全部边的权重都相同，那图 G 就是无权图)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" is_directed  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# True 则表明此边是有向边，False 为无向边")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对无向边而言，起始顶点和终止顶点可以互换")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'->'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("\\\n               "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t weight:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t is_directed:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于邻接表的顶点结构体")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里是用散列表 (而不是用链表) 来表达某顶点的所有邻接顶点")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("VertexList")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" discover"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" finish"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的关键字 key (通常为顶点序号、唯一标志符)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("val "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的值元素 val (可自定义为任意对象，为结点附带的信息)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的邻居字典，key 为邻居的关键字，value 为 Edge 边结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是有向图，本结点为关联边的出发点 from_v，其邻居关联边的终止点 to_v")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''下面是用于 DFS 的属性'''")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# False 为"白色"，表示未被发现；True 为"黑色"，表示已经完成探索')]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("discover "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" discover  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点被发现的时间戳 (在此时间前为颜色为白)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("finish "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" finish      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点完成探索的时间戳 (在此时间后为颜色为黑)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点的前驱结点/深度优先搜索树的父结点")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is connected to:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增添本顶点的邻居 neighbor，以字典结构存储")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 注意：如果不允许图有自环/自圈，那么在增添邻居/边 的时候要禁止增添 self.neighbor[self.key] 项。这里暂不限制")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_neighbor")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 neighbor 字典里已有此 key，则会覆盖。起到了更新边信息的作用")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以 Edge 边结构体来增添本顶点的邻居 neighbor")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("add_edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查输入 edge 的合法性")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 edge 是有向边，那么本结点需要是 edge 的出发点 from_v")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果 edge 是无向边，那么本结点需要是 edge 的出发点 from_v 或结束点 to_v 之一")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 from_v 和 to_v 交换")]),t._v("\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回本顶点的所有邻接顶点(的关键字/序号) 数组")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_connections")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回本顶点到邻居 neighbor 的 Edge 边结构体")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_weight")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" neighbor "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接表的图结构，通常适合稀疏图")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入顶点结构体列表、边结构体列表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AdjacencyList")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造函数，edges 必须是二维数组，内部维度是一系列长度为 2 或者 3 的数组，")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 分别代表着边的起始顶点 start、终止顶点 end 以及边权(可选)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edges         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的边列表")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertices   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的顶点列表 (可以从下标映射到顶点)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点映射到其下标 (既是邻接矩阵的行/列下标，也是 vertices 列表的下标)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点的关键字/唯一标志符映射到其下标")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本图的顶点字典, key 为顶点的序号，val 为顶点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertex\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 若 edges 合法，则进行边初始化处理")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是有向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果是无向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                    to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" to_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断 key 号为 _key 的顶点是否位于顶点列表中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__contains__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类迭代器方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__iter__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("iter")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取图中 key 号为 _key 的顶点，如果没有此顶点则返回 None")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_vertex")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接表 - 图转置")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("graph_transposition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其实如果是无向边，无需处理，但这里还是转了")]),t._v("\n            temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 深度优先搜索 (Depth First Search, BFS)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("DFS")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化各个结点被发现和结束探索的时间均为 inf 无穷")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timestamp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 时间戳")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topo_sort_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 拓扑排序的顺序。在 DFS 某结点 finish 时(其关键字)加入此列表")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入：graph 图结构，is_adj_link 为 True 表示图为邻接表结构，False 表示为邻接矩阵结构")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("do_dfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_adj_link"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topo_sort_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 拓扑排序的顺序")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" is_adj_link"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果图是邻接表结构")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_dfs_link"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果图是邻接矩阵结构")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_dfs_matrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topo_sort_list\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_dfs_link")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 初始化图 G 的每个顶点 u \\in G.V，让其颜色为白色 white、前驱结点/父结点为空 nil")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("discover "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("finish "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 初始化时间戳 time")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timestamp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 以每个白色的结点作为 DFS 算法的源结点（也即 DFS 树的根结点）调用 `DFS_VISIT` 过程进行深度优先搜索。")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" u "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_dfs_visit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以 u 为 DFS 树根结点，开始对邻接表结构的图 graph 进行 DFS")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_dfs_visit")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 时间戳增长，并赋予给结点 u 的发现时间 u.d")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由于 u 已被发现，其颜色置为灰色 gray（已被发现状态）")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timestamp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("discover "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timestamp\n        u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里省了灰色状态，直接赋为黑色")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 对于 u 的每个邻接结点 v，如果 v 是白色的（未被发现状态）")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 则将 v 的前驱置为 u，并以 v 为起点递归地进行 DFS 搜索")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("neighbor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先从 u 的关联边 edge 中得到邻接结点 v 的关键字")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 有向边，则邻接结点 v 的关键字是 edge 的终止结点")]),t._v("\n                v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 无向边，则先设置邻接结点 v 的关键字是 edge 的终止结点")]),t._v("\n                v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 但如果终止结点是 u，那么设置 邻接结点 v 的关键字是 edge 的起始结点")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果起始结点也是 u，则表示该边为无向图的自环，不必再处理")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 再从关键字 v_key 获取结点 v")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" v_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index\n            v_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u\n                self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_dfs_visit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 由于此时 u 的邻接结点都已被处理结束，故置 u 的颜色为黑色（处理完毕状态）")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 随后时间戳增长，并赋予给结点 u 的完成时间 u.f")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# u.color = True")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timestamp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("finish "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timestamp\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 拓扑排序：在 DFS 某结点 finish 时(其关键字)加入此列表")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topo_sort_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 插入到后方 O(1) 但这样是拓扑排序的逆序")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对邻接矩阵结构的图 graph 进行 DFS")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_dfs_matrix")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 DFS 之后，打印出所有结点(的关键字)及其发现时间 discover 与结束探索时间 finish")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("print_vertex_time")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("discover"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("finish"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造图同《CLRS》Chapter 22.3 的 DFS 图")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于构造邻接矩阵的顶点的 key/val 信息列表")]),t._v("\n    list_vertices_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("111")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("222")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("333")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("444")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("555")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("666")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 有向边的 from/to/weight/is_directed 信息列表")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# is_directed 为 True 表示此边为有向边，否则为无向边")]),t._v("\n    di_edges_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据前述列表信息构造结点列表")]),t._v("\n    list_vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    di_edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" list_vertices_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        list_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VertexList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" e "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" di_edges_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        di_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建邻接表 (用邻接链表+有向图 执行 DFS)")]),t._v("\n    adj_l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdjacencyList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("list_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" di_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行 DFS 过程")]),t._v("\n    start "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    dfs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DFS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    topo_sort_list "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("do_dfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_adj_link"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查看 DFS 结果")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\nDFS 结果 (各顶点关键字 及其 发现时间 与结束探索时间)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    dfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_vertex_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_l"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n拓扑排序的逆序'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("topo_sort_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Running Time: %.5f ms'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("h2",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),a("ul",[a("li",[t._v("Introduction to Algorithm (aka CLRS) Third Edition - Chapter 22")])])])}),[],!1,null,null,null);s.default=e.exports}}]);