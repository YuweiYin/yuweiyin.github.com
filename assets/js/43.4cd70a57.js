(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{264:function(t,s,a){"use strict";a.r(s);var n=a(28),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"algorithm-graph-theory-ford-fulkerson"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#algorithm-graph-theory-ford-fulkerson"}},[t._v("#")]),t._v(" Algorithm - Graph Theory - Ford-Fulkerson")]),t._v(" "),a("p",[t._v("Create Date: 2020.06.03")]),t._v(" "),a("p",[t._v("Last Update Date: 2020.06.03")]),t._v(" "),a("p",[t._v("By "),a("a",{attrs:{href:"https://yuweiyin.github.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("YuweiYin"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[t._v("#")]),t._v(" 简介")]),t._v(" "),a("p",[t._v("最大流 Max-Flow")]),t._v(" "),a("p",[t._v("Ford-Fulkerson 方法")]),t._v(" "),a("h3",{attrs:{id:"流网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#流网络"}},[t._v("#")]),t._v(" 流网络")]),t._v(" "),a("p",[t._v("流网络 G = (V, E) 是一个有向图，图中每条边 (u, v) \\in E 有一个"),a("strong",[t._v("非负")]),t._v("的"),a("strong",[t._v("容量值")]),t._v(" c(u, v) >= 0。而且，如果边集合 E 包含一条边 (u, v)，则图中不存在反方向的边 (v, u)。如果 (u, v) \\notin E，为方便起见，定义 c(u, v) = 0。并且在图中不允许有自循环/自圈 (u, u)。")]),t._v(" "),a("p",[t._v("另外，在流网络的所有结点中，有两个特殊的结点："),a("strong",[t._v("源结点")]),t._v(" s (source) 和"),a("strong",[t._v("汇点")]),t._v(" t (terminal)。源结点入度为 0、汇点出度为 0。为方便起见，假定 V 中每个结点 v 都位于某条从 s 到 v 的路径上，即有 "),a("code",[t._v("s ~> v ~> t")]),t._v(" 路径。")]),t._v(" "),a("p",[t._v("因此，流网络图是"),a("strong",[t._v("弱连通")]),t._v("的。并且由于除源结点 s 外的每个结点都至少有一条进入的边，有 "),a("code",[t._v("|E| >= |V| - 1")]),t._v("。")]),t._v(" "),a("ul",[a("li",[t._v("流网络的性质主要如下：\n"),a("ul",[a("li",[t._v("流网络 G = (V, E) 是一个弱连通的有向图")]),t._v(" "),a("li",[t._v("所有边的权重为非负值，且每条边的权重值有上界 c(u, v)")]),t._v(" "),a("li",[t._v("任意两个结点 u, v \\in V，不能同时存在边 (u, v) 和边 (v, u)")]),t._v(" "),a("li",[t._v("如果某边 (u, v) 不存在，定义其权重值 c(u, v) = 0")]),t._v(" "),a("li",[t._v("图中没有自循环 (u, u)")]),t._v(" "),a("li",[t._v("通常来说，源结点 s 的入度为 0，汇点 t 的出度为 0")]),t._v(" "),a("li",[t._v("每个结点 v 都处于从 s 到 t 的某条路径上。即：从起点 s 发出的流量可以流经 v 到达终点 t")])])])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/max-flow-1.png",alt:"max-flow-1"}})]),t._v(" "),a("p",[t._v("这里给出流的形式化定义。设 G = (V, E) 是一个"),a("strong",[t._v("流网络")]),t._v("，其"),a("strong",[t._v("容量函数")]),t._v("为 c。设 s 为网络的"),a("strong",[t._v("源结点")]),t._v("，t 为"),a("strong",[t._v("汇点")]),t._v("。G 中的"),a("strong",[t._v("流")]),t._v("是一个"),a("strong",[t._v("实值函数")]),t._v(" f: VxV -> R，满足如下两条性质：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("容量限制")]),t._v("：（“流量有限额”）对于所有的结点 u, v \\in V，要求 0 <= f(u, v) <= c(u, v)")]),t._v(" "),a("li",[a("strong",[t._v("流量守恒")]),t._v("：（“流入等于流出”）对于所有的结点 u \\in V - {s, t}，要求 $ \\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v) $\n"),a("ul",[a("li",[t._v("当 (u, v) \\notin E 时，从结点 u 到结点 v 之间没有流，因此 f(u, v) == 0")])])])]),t._v(" "),a("p",[t._v("称非负数值 f(u, v) 为从结点 u 到结点 v 的流。一个流 f 的值 "),a("code",[t._v("|f|")]),t._v(" 定义如下：")]),t._v(" "),a("p",[t._v("$$ |f| = \\sum_{v \\in V} f(s, v) = \\sum_{v \\in V} f(v, s) $$")]),t._v(" "),a("p",[t._v("即，流 f 的值时从源结点流出的总流量 减去 流入源结点的总流量。这里符号 "),a("code",[t._v("|·|")]),t._v(" 仅用作表达流的值，而不是数的绝对值或者集合的基数值。")]),t._v(" "),a("p",[t._v("通常来说，一个流网络不会有任何进入源结点的边，即源结点的入度为 0，故求和项 $ \\sum_{v \\in V} f(v, s) $ 的值将是 0。但对于有的网络而言（比如"),a("strong",[t._v("残存网络")]),t._v("），流入源结点的流量十分重要。")]),t._v(" "),a("p",[t._v("在"),a("strong",[t._v("最大流问题")]),t._v("中，给定一个"),a("strong",[t._v("流网络")]),t._v(" G、一个"),a("strong",[t._v("源结点")]),t._v(" s、一个"),a("strong",[t._v("汇点")]),t._v(" t，目标是找到值最大的一个"),a("strong",[t._v("流")]),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"ford-fulkerson-方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ford-fulkerson-方法"}},[t._v("#")]),t._v(" Ford-Fulkerson 方法")]),t._v(" "),a("p",[t._v("常用于解决最大流问题的 Ford-Fulkerson 方法，之所以被称为“方法”而不是“算法”，是因为它主要提供的是一种通用的解决思路，包含了几种运行时间不同的具体算法实现。")]),t._v(" "),a("p",[t._v("Ford-Fulkerson 方法依赖于三种重要思想："),a("strong",[t._v("残存网络")]),t._v("、"),a("strong",[t._v("增广路径")]),t._v(" 和 "),a("strong",[t._v("切割")]),t._v("。这三种思想与许多流算法和问题有关，它们是"),a("strong",[t._v("最大流最小切割定理")]),t._v("（《CLRS》定理 26.6）的精髓。该定理以"),a("strong",[t._v("流网络的切割")]),t._v("来表述最大流的值。")]),t._v(" "),a("p",[t._v("Ford-Fulkerson 方法"),a("strong",[t._v("循环增加流的值")]),t._v("：")]),t._v(" "),a("ul",[a("li",[t._v("在开始的时候，对于所有的结点 u, v \\in V，f(u, v) = 0，给出的初始流量为 0。")]),t._v(" "),a("li",[t._v("每一次迭代中，将图 G 的流值进行增加，增加的方法就是在一个关联的“残存网络” Gf 中寻找一条“增广路径”\n"),a("ul",[a("li",[t._v("一旦知道图 Gf 中的一条增广路径的边，就可以很容易地辨别出 G 中的一些具体的边，可以对这些边上的流量进行修改，从而增加流的值。")])])]),t._v(" "),a("li",[t._v("虽然 Ford-Fulkerson 方法的每次迭代都增加流（整体）的值，但是对于图 G 的一条特定边来说，其流量可能增加，也可能减少。\n"),a("ul",[a("li",[t._v("而且，对某些边的流进行缩减可能是必要的，以便让算法可以将更多的流从源结点发送到汇点。")])])]),t._v(" "),a("li",[t._v("重复对流进行这一过程，知道残存网络中不再存在增广路径为止。最大流最小切割定理将说明在算法终结时，该算法将获得一个最大流。")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("FORD_FULKERSON_METHOD(G, s, t)\n1  initialize flow f to 0\n2  while there exists an augmenting path p in the residual network Gf\n3      augment flow f along p\n4  return f\n")])])]),a("p",[t._v("为了实现和分析 Ford-Fulkerson 方法，需要引入如下几个新的概念。")]),t._v(" "),a("h3",{attrs:{id:"残存网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#残存网络"}},[t._v("#")]),t._v(" 残存网络")]),t._v(" "),a("p",[t._v("从直观上看，给定流网络 G 和流量 f，残存网络 Gf 由那些 "),a("strong",[t._v("仍有空间对流量进行调整的边")]),t._v(" 构成。流网络的一条边"),a("strong",[t._v("可以允许的额外流量")]),t._v("等于 该边的容量 减去 该边上的流量。如果该差值为正，则将该条边置于残存网络图 Gf 中，并将其"),a("strong",[t._v("残存容量")]),t._v("设置为 cf(u, v) = c(u, v) - f(u, v)。对于图 G 中的边来说，只有能够允许额外流量的边才能被加入到图 Gf 中。如果边 (u, v) 的流量 f(u, v) 等于其容量 c(u, v)，则其 cf(u, v) == 0，该条边将不属于图 Gf。（注意，根据"),a("strong",[t._v("容量限制")]),t._v("原则，残存容量 cf(u, v) 不可能为负值）")]),t._v(" "),a("p",[t._v("残存网络 Gf 中可能包含图 G 中不存在的边。算法对流量进行操作的目标是增加总流量，为此，算法可能对某些特定边上的流量进行缩减，为了表示对一个正流量 f(u, v) 的缩减，将反向边 (v, u) 加入到图 Gf 中，并将其残存容量设置为 cf(v, u) = f(u, v)。即 一条边所能允许的反向流量最多将其正向流量抵消。残存网络中的这些反向边允许算法将已经发送出来的流量发送回去。")]),t._v(" "),a("p",[t._v("而将流量从同一条边发送回去等同于"),a("strong",[t._v("缩减")]),t._v("该条边的流量，这种操作在许多算法中都是必须的。")]),t._v(" "),a("p",[t._v("更形式化地说，假定有一个流网络 G = (V, E)，其源结点为 s，汇点为 t。设 f 为图 G 中的一个流，考虑结点对 u, v \\in V，定义"),a("strong",[t._v("残存容量")]),t._v(" cf(u, v) 如下：")]),t._v(" "),a("ul",[a("li",[t._v("若 (u, v) \\in E，则 cf(u, v) = c(u, v) - f(u, v)")]),t._v(" "),a("li",[t._v("若 (v, u) \\in E，则 cf(u, v) = f(v, u)")]),t._v(" "),a("li",[t._v("若是其它情况，则 cf(u, v) = 0")])]),t._v(" "),a("p",[t._v("由于已经假定在图 G 中边 (u, v) 和边 (v, u) 不能同时出现，故上述三种情况有且仅有一种会发生。")]),t._v(" "),a("p",[t._v("举例来说，如果容量限制 c(u, v) = 16，并且当前流量 f(u, v) = 11，则对 f(u, v) 可以增加的量最多为 cf(u, v) = 5，再多就超过边 (u, v) 的容量限制了。同时，允许算法从结点 v 向结点 u 最多返回 11 单位的流量（即全部当前流量），因此残存网络中反向边 cf(v, u) = 11。")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("给定一个流网络 G = (V, E) 和一个流 f，则由 f 所诱导的图 G 的"),a("strong",[t._v("残存网络")]),t._v("为 Gf = (V, Ef)，其中边集 Ef = {(u, v) \\in V x V: cf(u, v) > 0}")]),t._v(" "),a("p",[t._v("即 残存网络的每条边（称为"),a("strong",[t._v("残存边")]),t._v("），必须允许大于 0 的流量通过。下面图 26-4 中的 (a) 图是前面图 26-1 (b) 的流网络 G 和流量 f 的重新绘制，图 26-4 (b) 描述的是对应的残存网络 Gf。Ef 中的边要么是 E 中原有的边，要么是其反向边，因此有 "),a("code",[t._v("|Ef| <= 2|E|")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-1.png",alt:"ford-fulkerson-1"}})]),t._v(" "),a("p",[t._v("残存网络 Gf 类似于一个容量为 cf 的流网络，但是不满足前面对流网络的定义，因为在 Gf 中可以有反平行边。除了反平行边这个区别外，流网络的其他性质 Gf 都有保留，因此可以在残存网络中定义一个流，它满足流的两条性质："),a("strong",[t._v("容量限制")]),t._v(" 和 "),a("strong",[t._v("流量守恒")]),t._v("。该流针对的是残存网络 Gf 中的残存容量 cf。")]),t._v(" "),a("p",[t._v("残存网络 Gf 中的一个流指出的是一条"),a("strong",[t._v("路线图")]),t._v("：如何在原来的流网络 G 中增加流。如果 f 是 G 的一个流，f' 是对应的残存网络 Gf 中的一个流，定义 f↑f' 为流 f' 对流 f 的"),a("strong",[t._v("递增")]),t._v(" (augmentation)，它是一个从 V x V 到 R 的函数，具体定义如下：")]),t._v(" "),a("ul",[a("li",[t._v("若 (u, v) \\in E，则 (f↑f')(u, v) = f(u, v) + f'(u, v) - f'(v, u)")]),t._v(" "),a("li",[t._v("其它情况，则 (f↑f')(u, v) = 0")])]),t._v(" "),a("p",[t._v("该定义背后的直观解释遵循残存网络的定义。因为在残存网络中"),a("strong",[t._v("将流量发送到反向边")]),t._v(" 等同于 在原来的网络中"),a("strong",[t._v("缩减流量")]),t._v("，所以将边 (u, v) 的流量增加 f'(u, v)，但减少 f'(v, u)。在残存网络中将流量推送回去 也被称为"),a("strong",[t._v("抵消操作")]),t._v(" (cancellation)。")]),t._v(" "),a("p",[t._v("举例来说，如果某公司将 5 箱货物从城市 u 发送到城市 v，同时将 2 箱同样的货物从城市 v 发送到城市 u，那么可以(从整体上来说)等价于 将 3 箱货物从城市 u 发送到城市 v。这类"),a("strong",[t._v("抵消操作")]),t._v("对于任何"),a("strong",[t._v("最大流算法")]),t._v("来说都是"),a("strong",[t._v("非常关键")]),t._v("的。")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 26.1")]),t._v("：设 G = (V, E) 为一个流网络，源结点为 s，汇点为 t，设 f 为 G 中的一个流。设 Gf 为由流 f 所诱导的 G 的残存网络，设 f' 为 Gf 中的一个流。那么函数 (f↑f') 是 G 的一个流，其值为 "),a("code",[t._v("|f↑f'| = |f| + |f'|")]),t._v("。")]),t._v(" "),a("p",[t._v("对引理 26.1 的证明主要有以下四点（具体证明参考《CLRS》Chapter 26.2）：")]),t._v(" "),a("ol",[a("li",[t._v("流量非负：(f↑f')(u, v) >= 0")]),t._v(" "),a("li",[t._v("容量限制：(f↑f')(u, v) <= c(u, v)")]),t._v(" "),a("li",[t._v("流量守恒：对于所有的结点 u \\in V - {s, t}，$ \\sum_{v \\in V} (f↑f')(u, v) = \\sum_{v \\in V} (f↑f')(v, u) $")]),t._v(" "),a("li",[t._v("计算 (f↑f') 的值，确保 "),a("code",[t._v("|f↑f'| = |f| + |f'|")])])]),t._v(" "),a("h3",{attrs:{id:"增广路径"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#增广路径"}},[t._v("#")]),t._v(" 增广路径")]),t._v(" "),a("p",[t._v("给定流网络 G = (V, E) 和流 f，"),a("strong",[t._v("增广路径")]),t._v(" p 是残存网络 Gf 中一条从源结点 s 到汇点 t 的"),a("strong",[t._v("简单路径")]),t._v("。根据残存网络的定义，对于一条增广路径上的边 (u, v)，可以增加其流量的幅度 最大为 cf(u, v)，再多会违反容量限制。")]),t._v(" "),a("p",[t._v("例如前面 图 26-4 (b) 中阴影覆盖的路径是一条增广路径。如果将图中的残存网络 Gf 看作一个流网络，那么可以对这条路径上的"),a("strong",[t._v("每条边的流量增加")]),t._v(" 4 个单位，而不会违反容量限制，因为该条路径上"),a("strong",[t._v("最小的残存容量")]),t._v("是 cf(v2, v3) = 4。")]),t._v(" "),a("p",[t._v("称在一条增广路径 p 上能够为每条边增加的流量的最大值 为路径 p 的"),a("strong",[t._v("残存容量")]),t._v("，该容量的表达式为：cf(p) = min{cf(u, v): (u, v) 属于路径 p}。下面的引理更加精确地阐述了此论断：")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 26.2")]),t._v("：设 G = (V, E) 为一个流网络，设 f 为图 G 中的一个流，设 p 为残存网络 Gf 中的一条增广路径。定义一个函数 fp: V x V -> R 如下：")]),t._v(" "),a("ul",[a("li",[t._v("若 (u, v) 在 p 上，则 fp(u, v) = cf(p)")]),t._v(" "),a("li",[t._v("若是其它情况，则 fp(u, v) = 0")])]),t._v(" "),a("p",[t._v("则 fp 是残存网络 Gf 中的一个流，其值为 "),a("code",[t._v("|fp| = cf(p) > 0")])]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("下面的推论证明，如果将流 f 增加 fp 的量，则将获得 G 的另一个流，该流的值更加接近最大值。图 26-4 (c) 描述的是对图 26-4 (a) 的流 f 增加图 26-4 (b) 所示的 fp 的量 所获得的结果，而图 26-4 (d) 描述的则是残存网络 Gf。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("推论 26.3")]),t._v("：设 G = (V, E) 为一个流网络，设 f 为 G 中的一个流，设 p 为残存网络 Gf 中的一条增广路径。设 fp 由引理 26.2 所定义，假定将 f 增加 fp 的量，则函数 "),a("code",[t._v("|f↑fp| = |f| + |fp| > |f|")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"流网络和切割"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#流网络和切割"}},[t._v("#")]),t._v(" 流网络和切割")]),t._v(" "),a("p",[t._v("Ford-Fulkerson 方法的核心就是"),a("strong",[t._v("沿着增广路径重复增加路径上的流量")]),t._v("，直到找到一个最大流为止。稍后证明的"),a("strong",[t._v("最大流最小切割定理")]),t._v("表明：一个流是最大流 当且仅当 其残存网络不包含任何增广路径。")]),t._v(" "),a("p",[t._v("流网络 G = (V, E) 中的一个切割 (S, T) 将结点集合 V 划分为 S 和 T=V-S 两个不相交集合，且使得源结点 s \\in S、汇点 t \\in T。这类似于最小生成树 MST 中的切割，不过这里是对有向图的切割，而非无向图。")]),t._v(" "),a("p",[t._v("若 f 是一个流，则定义横跨切割 (S, T) 的"),a("strong",[t._v("净流量")]),t._v(" f(S, T) 如下：")]),t._v(" "),a("p",[t._v("$$ f(S, T) = \\sum_{u \\in S} \\sum_{v \\in T} f(u, v) - \\sum_{u \\in S} \\sum_{v \\in T} f(v, u) $$")]),t._v(" "),a("p",[t._v("切割 (S, T) 的"),a("strong",[t._v("容量")]),t._v("是：")]),t._v(" "),a("p",[t._v("$$ c(S, T) = \\sum_{u \\in S} \\sum_{v \\in T} c(u, v) $$")]),t._v(" "),a("p",[t._v("一个网络的"),a("strong",[t._v("最小切割")]),t._v("是整个网络中"),a("strong",[t._v("容量最小的切割")]),t._v("。")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("流的定义和切割容量的定义之间不存在对称性，但这种不对称性是有意而为，并且很重要。对于容量来说，只计算从集合 S 发出、进入集合 T 的边的容量，而忽略反方向边上的容量。对于流，考虑的则是从 S 到 T 的总流量 减去 (反方向)从 T 到 S 的流量。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-2.png",alt:"ford-fulkerson-2"}})]),t._v(" "),a("p",[t._v("下面的引理将证明，对于给定流 f，横跨任何切割的净流量都相同，都等于 "),a("code",[t._v("|f|")]),t._v("，即流的值。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 26.4")]),t._v("：设 f 为流网络 G 的一个流，该流网络的源结点为 s，汇点为 t，设 (S, T) 为流网络 G 的"),a("strong",[t._v("任意切割")]),t._v("，则横跨切割 (S, T) 的净流量为 f(S, T) = "),a("code",[t._v("|f|")]),t._v("。")]),t._v(" "),a("p",[t._v("引理 26.4 的如下推论 说明如何使用切割容量来限定一个流的值。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("推论 26.5")]),t._v("：流网络 G 中任意流 f 的值不能超过 G 的任意切割的容量。")]),t._v(" "),a("p",[t._v("此推论给出的一个直接结论是："),a("strong",[t._v("一个流网络中最大流的值不能超过该网络最小切割的容量")]),t._v("。这就是下面要来陈述和证明的非常重要的最大流最小切割定理。该定理表明一个最大流的值 事实上等于一个最小切割的容量。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 26.6")]),t._v("（"),a("strong",[t._v("最大流最小切割定理")]),t._v("）：设 f 为流网络 G = (V, E) 中的一个流，该流网络的源结点为 s，汇点为 t，则下面的条件是等价的：")]),t._v(" "),a("ol",[a("li",[t._v("f 是 G 的一个最大流。")]),t._v(" "),a("li",[t._v("残存网络 Gf 不包含任何增广路径。\n"),a("ul",[a("li",[t._v("增广路径：残存网络 Gf 中一条从源结点 s 到汇点 t 的简单路径。")])])]),t._v(" "),a("li",[t._v("最大流的值 "),a("code",[t._v("|f|")]),t._v(" = c(S, T)，其中 (S, T) 是流网络 G 的某个切割。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-3.png",alt:"ford-fulkerson-3"}})]),t._v(" "),a("h2",{attrs:{id:"基本的-ford-fulkerson-算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#基本的-ford-fulkerson-算法"}},[t._v("#")]),t._v(" 基本的 Ford-Fulkerson 算法")]),t._v(" "),a("p",[t._v("在 Ford-Fulkerson 方法的每次迭代中，寻找某条增广路径 p，然后使用 p 来对流 f 进行修改（增加）。正如引理 26.2 和推论 26.3 所示，以 f↑fp 来替换 f，从而获得一个值为 "),a("code",[t._v("|f| + |fp|")]),t._v(" 的更大的流。")]),t._v(" "),a("p",[t._v("在如下算法实现中，通过为每条边 (u, v) \\in E 更新流属性 (u, v).f 来计算流网络 G = (V, E) 中的最大流。如果边 (u, v) \\notin E，则设置 (u, v).f = 0。另外，假设流网络各边的容量 c(u, v) 都已经给出，如果边 (u, v) \\notin E，则设置 c(u, v) = 0。根据如下式子来计算残存容量 cf(u, v)。代码中的表达式 cf(p) 只是一个临时变量，用于存放路径 p 的残存容量。")]),t._v(" "),a("ul",[a("li",[t._v("若 (u, v) \\in E，则 cf(u, v) = c(u, v) - f(u, v)")]),t._v(" "),a("li",[t._v("若 (v, u) \\in E，则 cf(u, v) = f(v, u)")]),t._v(" "),a("li",[t._v("若是其它情况，则 cf(u, v) = 0")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("FORD_FULKERSON(G, s, t)\n1  for each edge(u, v) \\in G.E\n2      (u, v).f = 0\n3  while there exists a path p from s to t in the residual network Gf\n4      cf(p) = min { cf(u, v): (u, v) is in path p }\n5      for each edge(u, v) in p\n6          if (u, v) \\in E\n7              (u, v).f = (u, v).f + cf(p)\n8          else\n9              (v, u).f = (v, u).f - cf(p)\n")])])]),a("p",[a("code",[t._v("FORD_FULKERSON(G, s, t)")]),t._v(" 算法是对 "),a("code",[t._v("FORD_FULKERSON_METHOD(G, s, t)")]),t._v(" 方法的简单扩展。算法流程描述如下：")]),t._v(" "),a("ol",[a("li",[t._v("在 1～2 行，将流 f 初始化为 0。")]),t._v(" "),a("li",[t._v("在 3～9 行的 while 循环中，重复在残存网络 Gf 中寻找一条增广路径 p，然后使用残存容量 cf(p) 来对路径 p 上的流 f 进行增加。路径 p 上的一条边要么是原来网络中的一条边，要么是原来网络中的边 的反向边。\n"),a("ul",[a("li",[t._v("在 4 行，找出路径 p 中的最小残存容量 cf(u, v)。")]),t._v(" "),a("li",[t._v("在 5～9 行的 for 循环中，对路径上 p 的每条边 (u, v) 的流量进行更新。")]),t._v(" "),a("li",[t._v("在 6～7 行，如果残存边 (u, v) 是原来流网络中的一条边，则增加其流量 (u, v).f")]),t._v(" "),a("li",[t._v("在 8～9 行，如果残存边 (u, v) 不是原来流网络中的一条边，则减少其反向边 (v, u) 流量 (v, u).f")])])]),t._v(" "),a("li",[t._v("最后，当 while 循环结束时，不再有增广路径。根据最大流最小切割定理，此时流 f 就是最大流。")])]),t._v(" "),a("h3",{attrs:{id:"ford-fulkerson-算法的分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ford-fulkerson-算法的分析"}},[t._v("#")]),t._v(" Ford-Fulkerson 算法的分析")]),t._v(" "),a("p",[t._v("Ford-Fulkerson 算法的运行时间取决于算法第 3 行是如何寻找增广路径 p 的。如果使用广度优先搜索 BFS 来寻找增广路径，算法的运行时间是多项式数量级。如果选择不好，"),a("code",[t._v("FORD_FULKERSON")]),t._v(" "),a("strong",[t._v("算法")]),t._v("可能不会终止：流的值会随着后续的递增 (augmentation) 而增加，但它却不一定收敛于最大的流值。")]),t._v(" "),a("p",[t._v("另外，只有当变得容量为"),a("strong",[t._v("无理数")]),t._v("时，"),a("code",[t._v("FORD_FULKERSON_METHOD")]),t._v(" "),a("strong",[t._v("方法")]),t._v("才可能无法终止。下面均假定所选择的任意增广路径 p 和所有的容量 c 都是整数值。在实际情况中，最大流问题中的容量常常都是整数。如果容量为有理数，则可以通过乘以某个系数（或者采用近似值）来将其转换为整数。")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("如果 f* 表示转换后网络中的一个最大流，则在 "),a("code",[t._v("FORD_FULKERSON")]),t._v(" 算法的一个直接实现中，执行第 3～9 行的 while 循环的次数最多为 "),a("code",[t._v("|f*|")]),t._v(" 次，因为流量值在每次迭代中至少增加一个单位。")]),t._v(" "),a("p",[t._v("如果用于实现流网络 G = (V, E) 的数据结构是合理的，并且寻找一条增广路径 p 的算法时间是线性的（比如 DFS 和 BFS），则整个 while 循环的执行将非常高效。假设有一个与有向图 G' = (V, E') 相对应的数据结构，这里 E' = {(u, v): (u, v) \\in E 或者 (v, u) \\in E}。网络 G 中的边也是网络 G' 中的边，因此在这一数据结构中，保持其容量和流就非常简单了。给定网络 G 的一个流 f，残存网络 Gf 中的边由网络 G' 中所有满足条件 cf(u, v) > 0 的边 (u, v) 所构成，其中 cf 遵守前述残存容量的性质。")]),t._v(" "),a("p",[t._v("因此，如果使用深度优先搜索 DFS 或广度优先搜索 BFS，在一个残存网络中找到一条路径的时间应是 "),a("code",[t._v("O(|V| + |E'|) = O(|E|)")]),t._v("。而 while 循环的每一遍执行所需的时间因此为 "),a("code",[t._v("O(|E|)")]),t._v("，这与算法第 1～2 行的初始化成本一样，从而整个 "),a("code",[t._v("FORD_FULKERSON")]),t._v(" 算法的运行时间为 "),a("code",[t._v("O(|E|·|f*|)")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-4.png",alt:"ford-fulkerson-4"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-5.png",alt:"ford-fulkerson-5"}})]),t._v(" "),a("p",[t._v("当容量都是整数值且最优的流量值 "),a("code",[t._v("|f*|")]),t._v(" 较小时，"),a("code",[t._v("FORD_FULKERSON")]),t._v(" 算法的运行时间相等不错。但当最优流量值 "),a("code",[t._v("|f*|")]),t._v(" 取值较大时，可能会效率很慢，如图 26-7 示例。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-6.png",alt:"ford-fulkerson-6"}})]),t._v(" "),a("h2",{attrs:{id:"edmonds-karp-算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#edmonds-karp-算法"}},[t._v("#")]),t._v(" Edmonds-Karp 算法")]),t._v(" "),a("p",[t._v("可以通过在 "),a("code",[t._v("FORD_FULKERSON")]),t._v(" 算法第 3 行寻找增广路径 p 的操作中 使用广度优先搜索 BFS 来改善算法的效率。即 在残存网络中选择的增广路径是一条从源结点 s 到汇点 t 的(无权重)最短路径，BFS 时每条边的权重均为单位距离。称如此实现的 Ford-Fulkerson 方法为 Edmonds-Karp 算法，其运行时间为 "),a("code",[t._v("O(|V|·|E|^2)")]),t._v("，这就与最优流量值 "),a("code",[t._v("|f*|")]),t._v(" 的取值大小无关了。")]),t._v(" "),a("p",[t._v("对 Edmonds-Karp 算法的分析取决于残存网络 Gf 中结点之间的距离。下面的引理使用符号 df(u, v) 来表示残存网络 Gf 中从结点 u 到结点 v 的(无权重)最短路径距离，其中每条边的权重为单位距离。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 26.7")]),t._v("：如果 Edmonds-Karp 算法运行在流网络 G = (V, E) 上，该网络的源结点为 s、汇点为 t，则对于所有的结点 v \\in V-{s, t}，残存网络 Gf 中"),a("strong",[t._v("最短路径距离")]),t._v(" df(u, v) 随着每次流量的递增 而"),a("strong",[t._v("单调递增")]),t._v("。")]),t._v(" "),a("p",[t._v("下面的定理给出了 Edmonds-Karp 算法的迭代次数的上界。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 26.8")]),t._v("：如果 Edmonds-Karp 算法运行在源结点为 s、汇点为 t 的流网络 G = (V, E) 上，则该算法所执行的流量递增操作的总次数为 "),a("code",[t._v("O(|V|·|E|)")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/ford-fulkerson-7.png",alt:"ford-fulkerson-7"}})]),t._v(" "),a("p",[t._v("由于在用广度优先搜索 BFS 寻找增广路径时，"),a("code",[t._v("FORD_FULKERSON(G, s, t)")]),t._v(" 中的每次迭代可以在 "),a("code",[t._v("O(|E|)")]),t._v(" 时间内实现，所以 Edmonds-Karp 算法的总运行时间为 "),a("code",[t._v("O(|V|·|E|^2)")]),t._v("。")]),t._v(" "),a("p",[t._v("而"),a("strong",[t._v("推送-重贴标签")]),t._v("算法能够取得更好的界，可以达到 "),a("code",[t._v("O(|V|^2·|E|)")]),t._v(" 甚至 "),a("code",[t._v("O(|V|^3)")]),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"最大二分匹配"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最大二分匹配"}},[t._v("#")]),t._v(" 最大二分匹配")]),t._v(" "),a("p",[t._v("一些组合问题可以很容易地标注为最大流问题，例如多源结点多汇点的最大流问题。其它一些组合问题在表面上看似与流网络没有什么关系，但实际上却能够归约到最大流问题，比如"),a("strong",[t._v("最大二分匹配")]),t._v("问题：在一个二分图(或称“二部图”)中找出一个最大匹配。")]),t._v(" "),a("p",[t._v("解决此问题 将用到由 Ford-Fulkerson 方法所提供的完整性性质 (integrality property)。使用 Ford-Fulkerson 方法能够在 "),a("code",[t._v("O(|V|·|E|)")]),t._v(" 时间内解决图 G = (V, E) 的最大二分匹配问题。")]),t._v(" "),a("h3",{attrs:{id:"最大二分匹配问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最大二分匹配问题"}},[t._v("#")]),t._v(" 最大二分匹配问题")]),t._v(" "),a("p",[t._v("二分图的最大匹配 (Bipartite Graph Maximum Matching)")]),t._v(" "),a("p",[t._v("给定一个"),a("strong",[t._v("无向图")]),t._v(" G = (V, E)，一个"),a("strong",[t._v("匹配")]),t._v(" (match) 是边的一个子集 $ M \\subseteq E $，使得对于所有结点 v \\in V，子集 M 中"),a("strong",[t._v("最多有一条边")]),t._v("与结点 v 关联。如果子集 M 中的某条边与结点 v 关联，则称结点 v "),a("strong",[t._v("由 M 所匹配")]),t._v("；否则，结点 v 就是"),a("strong",[t._v("没有匹配")]),t._v("的。"),a("strong",[t._v("最大匹配")]),t._v("是具有"),a("strong",[t._v("最大基数")]),t._v("的边集 M。这里的“最大”也是“极大”的意思。")]),t._v(" "),a("p",[t._v("最大匹配问题不仅限于 二分图，即两类事物的最大匹配，而是可以在 n 分图 (n >= 2) 上寻找最大匹配。而二分图的匹配是其中最基础的，也最具有代表性。")]),t._v(" "),a("p",[t._v("在一个二分图中，结点集合 V 有划分 {L, R}，并且边集合 E 中所有的边都横跨 L 和 R。即：任意边 (u, v) \\in E，要么 u \\in L 且 v \\in R，要么 u \\in R 且 v in L。进一步假定无向图 G 是"),a("strong",[t._v("连通")]),t._v("的，因此 V 中的每个结点至少关联了一条边。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/info-technology/algorithm/graph-theory/max-flow-matching/bipartite-graph-maximum-matching-1.png",alt:"bipartite-graph-maximum-matching-1"}})]),t._v(" "),a("p",[t._v("在二分图中寻找最大匹配问题有许多实际的应用。例如，把一个机器集合 L 和待执行的任务集合 R 相匹配。E 中有边 (u, v) 就说明机器 u \\in L 能够完成任务 v \\in R。最大匹配能够让尽可能多的机器同时运行。")]),t._v(" "),a("p",[t._v("最大二分匹配问题仅需匹配的基数越大越好，因此边的权重都设定为单位权重，即无权图。但如果考虑边的权重，则可以建模更多的场景，如果在带权图中 希望匹配的总权重值最高，则是最佳匹配的问题了。")]),t._v(" "),a("h3",{attrs:{id:"寻找最大二分匹配"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#寻找最大二分匹配"}},[t._v("#")]),t._v(" 寻找最大二分匹配")]),t._v(" "),a("p",[t._v("使用 Ford-Fulkerson 方法可以在 "),a("code",[t._v("O(|V|·|E|)")]),t._v(" 时间内找出(无权)无向二分图 G = (V, E) 的最大匹配。解决这一问题的关键技巧是增加额外的源结点 s 和汇点 t，构造出一个流网络，其中的流对应于匹配，如图 26-8 (c) 所示。")]),t._v(" "),a("p",[t._v("将二分图 G 所对应的流网络 G' = (V', E') 定义如下：设源结点 s 和汇点 t 为原本不属于结点集合 V 的新结点，并设 V' = V \\cup {s, t}。如果图 G 的结点集划分为 V = L \\cup R，则 E 中所有从 L 指向 R 的边都是流网络 G' 的边。此外 G' 中的边还包括 "),a("code",[t._v("|V|")]),t._v(" 条新的有向边。E' = {(s, u): u \\in L} \\cup E \\cup {(v, t): v \\in R}")]),t._v(" "),a("p",[t._v("最后，给 E' 中的每条边赋予"),a("strong",[t._v("单位容量")]),t._v("。由于结点集 V 中的每个结点至少有一条相连的边，"),a("code",[t._v("|E >= |V| / 2")]),t._v("。因此 "),a("code",[t._v("|E| <= |E'| = |E| + |V| <= 3 |E|")]),t._v("，所以 "),a("code",[t._v("|E'| = \\Theta(|E|)")]),t._v("。")]),t._v(" "),a("p",[t._v("下面的引理证明了图 G 中的一个匹配直接对应 G 所对应的流网络 G' 中的一个流。对于流网络 G = (V, E) 中的一个流 f 来说，如果对于所有的边 (u, v) \\in V x V，f(u, v) 都是整数值，则称流 f 是"),a("strong",[t._v("整数值")]),t._v("的。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("引理 26.9")]),t._v("：设 G = (V, E) 为一个二分图，其结点划分为 V = L \\cup R，设 G' = (V', E') 是图 G 所对应的流网络。如果 M 是 G 中的一个匹配，则流网络 G' 中存在一个整数值的流 f，使得 "),a("code",[t._v("|f| = |M|")]),t._v("。相反，如果 f 是 G' 中的一个整数值的流，则图 G 中存在一个匹配 M，使得 "),a("code",[t._v("|M| = |f|")]),t._v("。")]),t._v(" "),a("p",[t._v("基于引理 26.9，希望得出如下结论：二分图 G 中的一个最大匹配对应于流网络 G' 中的一个最大流，并且可以通过在流网络 G' 上运行一个最大流算法来计算出图 G 中的最大匹配。此结论的得来 存在的唯一障碍是：最大流算法可能返回流网络 G' 中一个非整数的流 f(u, v)，即便流的值 "),a("code",[t._v("|f|")]),t._v(" 本身必须是整数。不过，下面的定理将说明，如果使用 Ford-Fulkerson 方法，前述问题不会发生，因此前述结论能够成立。")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("定理 26.10")]),t._v("（"),a("strong",[t._v("完整性定理")]),t._v("）：如果容量函数 c 只能取整数值，则 Ford-Fulkerson 方法所生成的最大流 f 满足 "),a("code",[t._v("|f|")]),t._v(" 是整数值的性质。而且，对于所有的结点 u 和 v，f(u, v) 的值都是整数。")]),t._v(" "),a("p",[t._v("定理 26.10 可以通过对迭代次数进行归纳来证明。下面给出引理 26.9 的一个推论：")]),t._v(" "),a("p",[t._v("《CLRS》"),a("strong",[t._v("推论 26.11")]),t._v("：二分图 G 中的一个最大匹配 M 的基数 等于 其对应的流网络 G' 中某一最大流 f 的值。")]),t._v(" "),a("p",[t._v("因此，给定一个无向二分图 G，可以通过创建流网络 G'，在其上运行 Ford-Fulkerson 方法来找到一个最大匹配。这个最大匹配 M 可以直接从找到的整数最大流 f 中获得。由于二分图中任何匹配的基数最大值为 min(L, R) = "),a("code",[t._v("O(|V|)")]),t._v("，G' 中最大流的值为 "),a("code",[t._v("O(|V|)")]),t._v("。又由于 "),a("code",[t._v("|E'| = \\Theta(|E|)")]),t._v("，所以可以在 "),a("code",[t._v("O(|V|·|E'|) = O(|V|·|E|)")]),t._v(" 时间内找到一个二分图的最大匹配。")]),t._v(" "),a("h3",{attrs:{id:"hopcroft-karp-最大二分匹配算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hopcroft-karp-最大二分匹配算法"}},[t._v("#")]),t._v(" Hopcroft-Karp 最大二分匹配算法")]),t._v(" "),a("p",[t._v("目前最快的解决最大二分匹配问题的算法是由 Hopcroft 和 Karp[176] 所发明的，其运行时间为 "),a("code",[t._v("O(|E|·\\sqrt(|V|))")])]),t._v(" "),a("p",[t._v("[176] John E. Hopcroft and Richard M. Karp. An n^{5/2} algorithm for maximum matchings in biparite graphs. "),a("em",[t._v("SIAM Journal on Computing")]),t._v(", 2(4):225-231, 1973.")]),t._v(" "),a("p",[t._v("给定一个无向二分图 G = (V, E)，其中 V = L \\cup R 并且所有的边都恰有一个端点在集合 L 中，而另一个端点在集合 R 中。设 M 为图 G 的一个匹配。")]),t._v(" "),a("p",[t._v("对于图 G 中的一条简单路径 P，如果该路径的起点是 L 中一个未匹配的结点，终结点是集合 R 中的一个未匹配的结点，而路径上的边交替属于 M 和 E-M，则称路径 P 是一条相对于 M 的增广路径（此增广路径的定义与流网络中的增广路径相关，但并不相同）。")]),t._v(" "),a("p",[t._v("在这里，将一条路径看作是一系列的边，而不是一系列的结点。一条关于匹配 M 的最短增广路径是一条包含最少边数的增广路径。")]),t._v(" "),a("p",[t._v("给定两个集合 A 和 B，"),a("strong",[t._v("对称差")]),t._v(" A \\oplus B 定义为 (A-B) \\cup (B-A)，即仅在一个集合中出现的元素。")]),t._v(" "),a("p",[t._v("详见《CLRS》思考题 26-6")]),t._v(" "),a("h3",{attrs:{id:"完全匹配"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#完全匹配"}},[t._v("#")]),t._v(" 完全匹配")]),t._v(" "),a("p",[a("strong",[t._v("完全匹配")]),t._v("是指图中所有的结点都得到匹配的匹配。设 G = (V, E) 是结点划分为 V = L \\cup R 的无向二分图，其中 "),a("code",[t._v("|L| == |R|")]),t._v("。对于任意结点子集 $ X \\subseteq V $，定义 X 的"),a("strong",[t._v("邻居")]),t._v("为：N(X) = {y \\in V: 对某个 x \\in X，(x, y) \\in E}，即由与集合 X 中的元素 相邻的结点所构成的集合。")]),t._v(" "),a("p",[t._v("关于完全匹配，有 "),a("strong",[t._v("Hall 定理")]),t._v("：图 G 中存在一个完全匹配 当且仅当 对于每个边子集 $ A \\subseteq L $，有 "),a("code",[t._v("|A| <= |N(A)|")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"d-正则"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#d-正则"}},[t._v("#")]),t._v(" d 正则")]),t._v(" "),a("p",[t._v("对于一个结点划分为 V = L \\cup R 的无向二分图 G = (V, E) 而言，如果每个属于结点集合 V 的结点 v 的度数都是自然数 d，则称该二分图是 "),a("strong",[t._v("d 正则的")]),t._v("。对于每个 d 正则的二分图，都有 "),a("code",[t._v("|L| == |R|")]),t._v("。")]),t._v(" "),a("p",[t._v("可以证明：每个 d 正则二分图的匹配基数都是 "),a("code",[t._v("|L|")]),t._v("。")]),t._v(" "),a("p",[t._v("证明思路：该二分图对应的流网络的一个最小切割的容量为 "),a("code",[t._v("|L|")]),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"python-代码范例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#python-代码范例"}},[t._v("#")]),t._v(" Python 代码范例")]),t._v(" "),a("p",[t._v("Python 环境：Python 3.7")]),t._v(" "),a("h3",{attrs:{id:"ford-fulkerson-最大流算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ford-fulkerson-最大流算法"}},[t._v("#")]),t._v(" Ford-Fulkerson 最大流算法")]),t._v(" "),a("ul",[a("li",[t._v("Ford-Fulkerson 方法\n"),a("ul",[a("li",[t._v("Ford-Fulkerson 算法 O(V |f*|)")]),t._v(" "),a("li",[t._v("Edmonds-Karp 算法 O(V E^2)")])])]),t._v(" "),a("li",[t._v("Push-Relabel 推送-重贴标签方法\n"),a("ul",[a("li",[t._v("Push-Relabel 通用的推送-重贴标签算法 O(V^2 E)")]),t._v(" "),a("li",[t._v("Relabel-To-Front 前置重贴标签算法 O(V^3)")])])])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding:utf-8 -*-")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""=================================================\n@Project : algorithm/graph_theory/max_flow\n@File    : ford-fulkerson.py\n@Author  : YuweiYin\n@Date    : 2020-06-03\n=================================================="""')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" queue\n\n"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n最大流 Max-Flow\n\n- Ford-Fulkerson 方法\n    - Ford-Fulkerson 算法\n    - Edmonds-Karp 算法\n\n参考资料：\nIntroduction to Algorithm (aka CLRS) Third Edition - Chapter 26\n"""')]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边结构体，表达边的信息，可随任务自定义 (增添其它值元素 val 对象)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Edge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" capacity"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_v  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的起始顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_v      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的终止顶点(关键字/序号)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (用于最短路)边的权重值 (默认值为 1，如果全部边的权重都相同，那图 G 就是无权图)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" is_directed  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# True 则表明此边是有向边，False 为无向边")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对无向边而言，起始顶点和终止顶点可以互换")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''下面是用于 Max-Flow 的属性'''")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("capacity "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" capacity  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此边的最大容量")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此边最大流的流量，初始为 0，取值范围 0 <= flow <= capacity")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 运行过程中的边流量存储于矩阵中，这里的 self.flow 仅存储最终的本条边的流量")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'->'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" \\\n               "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t capacity:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("capacity"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t flow:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" \\\n               "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t weight:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t is_directed:'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于邻接矩阵的顶点结构体 (比 VertexList 简单)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里是用散列表 (而不是用链表) 来表达某顶点的所有邻接顶点")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("VertexMatrix")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" distance"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的关键字 key (通常为顶点序号、唯一标志符)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("val "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的值元素 val (可自定义为任意对象，为结点附带的信息)")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''下面是用于 BFS 的属性'''")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" color        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# False 为"白色"，表示未被发现；True 为"黑色"，表示已经探索结束')]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" distance  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点距离源结点的距离 (最短简单路径的边数)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" p                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点的前驱结点/广度优先搜索树的父结点")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Vertex key: '")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (带边权的)邻接矩阵的图结构，通常适合稠密图")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入顶点结构体列表、边结构体列表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AdjacencyMatrix")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# self.inf = 0x3f3f3f3f        # 初始各边的权重值均为 inf 无穷")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 经最大流算法后计算出的最大流值")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edges           "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的边列表")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由(起始,终止)顶点的关键字/唯一标志符映射到边数组下标")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertices     "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的顶点列表 (可以从下标映射到顶点)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点映射到其下标 (既是邻接矩阵的行/列下标，也是 vertices 列表的下标)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点的关键字/唯一标志符映射到顶点数组下标")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertex "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建邻接矩阵(二维方阵)，adj[x][y] 的值为边 (x, y) 的当前流量，而不是边权重")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里用于残存网络 Gf，可以有反平行边。如果 adj[x][y] 为 0 表示没有此边，在 Gf 上运行 BFS")]),t._v("\n        v_num "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 顶点数目")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" v_num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 若 edges 合法，则进行边初始化处理")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 断言最大流算法里都是有向边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed\n                from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边起点的关键字 key")]),t._v("\n                to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v          "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边终点的关键字 key")]),t._v("\n                capacity "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("capacity  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的容量")]),t._v("\n                self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将顶点关键字 key 转为下标 index，然后初始化 adj[from][to] 为边的最大容量")]),t._v("\n                    from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" v_num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" v_num\n                    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" capacity\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断 key 号为 _key 的顶点是否位于顶点列表中")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__contains__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取图中 key 号为 _key 的顶点，如果没有此顶点则返回 None")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_vertex")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" _key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接矩阵 - 图转置")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("graph_transposition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其实如果是无向边，无需处理，但这里还是转了")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先获取 key")]),t._v("\n            from_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n            to_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 交换 key")]),t._v("\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_key\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_key\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把 key 转成 index")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n            from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 修改邻接矩阵")]),t._v("\n            temp "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出邻接矩阵")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("print_matrix_info")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 广度优先搜索 (Breadth First Search, BFS)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BFS")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化各个结点距离源结点的距离为 inf 无穷")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从源结点 s 到目标结点 v 的一条最短路径上的所有结点(的关键字)")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果此标志为真，则会结束掉递归过程")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入：输入图结构默认为邻接矩阵 adj_m，而 start_key 为源顶点的关键字")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("do_bfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 start_key 源顶点的关键字 转为源结点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" start_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            start_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'输入的 start_key 不是任何顶点的关键字，BFS 失败'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 除了源结点 s 外，将其余所有结点 u 的状态标记为“未被发现”，即 color 为白色 white")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 另外，将 u.d 设置为无穷 inf，表示从源结点不可达结点 u。由于未探索到结点 u，将其前驱结点设置为空 nil")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 设置源结点 s 的属性。由于已经发现了 s，所以 s.color 设置为灰色 gray。结点 s 到自身的距离为 0")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 结点 s 为 BFS 树的树根，所以前驱/父结点 u.p 为空 nil")]),t._v("\n        start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n        start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 将 s 加入辅助队列 Q，成为其唯一成员")]),t._v("\n        aux_queue "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Python 内建 queue 队列对象")]),t._v("\n        aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 在 while 循环中，先进先出地逐个处理队列 Q 中的结点")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("empty"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.1. 先取出 Q 队首结点 u")]),t._v("\n            u "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n            u_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.2. 逐个处理 u 的所有邻接结点 v")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge_flow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("u_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接矩阵中边流量为 0 表示残存网络中没有此边")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge_flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("continue")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取边的终点 v 结构体")]),t._v("\n                v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.3. 如果 v.color 是白色，表示它未被发现，需要被加入到队列 Q 中。在入队之前，需要设置其属性：")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.color 颜色设置为灰色，表示它已被发现，但是尚未被探索完（所谓探索结束，是其邻接结点都已被处理）")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.d 是 v 到源结点 s 的距离，这个距离等于 u.d 距离加上 1")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.p 设置前驱/父结点为 u")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - 将 v 入队，之后的 while 循环中 会考察 v 的各个邻接结点")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n                    v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n                    v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u\n                    aux_queue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. for 循环结束，u 的所有邻接结点都被考察了，所以 u 已经被探索结束了。u.color 设置为黑色，保证不会再被加入队列 Q")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# u.color = True")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 BFS 之后，打印出所有结点(的关键字)及其距离")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("print_vertex_distance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 BFS 之后，获取从源结点 s 到目标结点 v 的一条最短路径上的所有结点(的关键字)")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此处图结构为邻接矩阵 adj_m")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_path")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return 'get_path: 输入的参数类型不合法'")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 key 关键字 转为顶点点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" start_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" start_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            start_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return '输入的 start_key 不是任何顶点的关键字'")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" end_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            end_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("end_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" end_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            end_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("end_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return '输入的 end_key 不是任何顶点的关键字'")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_get_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_get_path")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" end_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# self.path = 'No path from ' + str(start_v.key) + ' to ' + str(end_v.key) + ' exists.'")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先获取其前驱结点/父结点的关键字，再获取本结点的关键字")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_get_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Ford-Fulkerson 最大流算法 - O(VE^2)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FordFulkerson")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 所有结点的 distance 初始化为 inf")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算流网络(邻接矩阵) adj_m 的最大流")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("do_edmonds_karp")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 首先确认输入的合法性，并将输入的源结点和汇点关键字 转为结点结构体")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" source_v_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" terminal_v_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n        source_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("source_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        terminal_v "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("terminal_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("source_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("terminal_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 将各边的流量 flow 初始化为 0")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 在 while 循环中，重复在残存网络 Gf 中寻找一条增广路径 p")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 然后使用残存容量 cf(p) 来对路径 p 上的流 flow 进行增加")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 路径 p 上的一条边要么是原来网络中的一条边，要么是原来网络中的边 的反向边")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在本实现中，adj_m 中的矩阵保存的就是残存网络 Gf 中各个边的流量")]),t._v("\n        bfs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BFS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        is_exist_aug_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 循环标志 True 表示当前残存网络 Gf 中存在一条增广路径")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" is_exist_aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.1. 找出增广路径 aug_path")]),t._v("\n            bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("do_bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            aug_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bfs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果不存在增广路径，BFS 会返回空列表。如果返回仅含 1 个元素关键字的列表，是异常情况")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.2. 找出路径 p 中的最小残存容量 cf(u, v)")]),t._v("\n            min_cf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据 key 获取 index")]),t._v("\n                from_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n                from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更新路径上的最小残存容量 min_cf")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" min_cf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    min_cf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" min_cf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 断言增广路径上的流量值不为 inf")]),t._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.3. 在 for 循环中，对路径上 p 的每条边 (u, v) 的流量进行更新")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据 key 获取 index")]),t._v("\n                from_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n                from_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果残存边 (u, v) 是原来流网络中的一条边，则增加其流量 (u, v).f")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果残存边 (u, v) 不是原来流网络中的一条边，则减少其反向边 (v, u) 流量 (v, u).f")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在本实现中，增加边 (u, v) 的流量，等于缩减 (u, v) 的剩余容量、增加反平行边 (v, u) 的剩余容量")]),t._v("\n                adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-=")]),t._v(" min_cf\n                adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" min_cf\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 最后，当 while 循环结束时，不再有增广路径。根据最大流最小切割定理，此时流 flow 就是最大流")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在本实现中，adj_m 中的矩阵保存的就是残存网络 Gf 中各个边的流量，最终将实际的流量赋予各个结点的 flow 属性")]),t._v("\n        adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" from_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" to_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                from_node "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_node "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果此边是原图中的边，则赋予该边 flow 属性，表示最大流的流量")]),t._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    edge_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    edge "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 赋予此边流量属性 flow")]),t._v("\n                    adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增长图的最大流量值")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造图同《CLRS》图 26-6 的(含边容量的)有向图用于计算最大流")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于构造邻接矩阵的顶点的 key/val 信息列表")]),t._v("\n    matrix_vertices_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("400")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 有向边的 from/to/c/is_directed 信息列表")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# is_directed 为 True 表示此边为有向边，否则为无向边")]),t._v("\n    di_edges_info "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("14")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据前述列表信息构造结点列表")]),t._v("\n    inf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 需保证与程序中其它 inf 是相同的值")]),t._v("\n    matrix_vertices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    di_edges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" matrix_vertices_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        matrix_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VertexMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" distance"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("inf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" e "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" di_edges_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        di_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Edge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" capacity"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建邻接矩阵 (用邻接矩阵+有向图 执行最大流算法)")]),t._v("\n    adj_m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdjacencyMatrix"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("matrix_vertices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" di_edges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行 O(VE^2) Ford-Fulkerson 最大流算法")]),t._v("\n    source_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),t._v("\n    ford_fulkerson "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" FordFulkerson"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    start "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ford_fulkerson"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("do_edmonds_karp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出结果 & 运行时间")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# max_flow: 23")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 4, 2, 0, 0, 0]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [12, 0, 0, 0, 0, 0]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [11, 4, 0, 0, 3, 0]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 12, 9, 0, 7, 1]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 0, 11, 0, 0, 0]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 0, 0, 19, 4, 0]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\nmax_flow:'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    adj_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_matrix_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Running Time: %.5f ms'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("h2",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),a("ul",[a("li",[t._v("Introduction to Algorithm (aka CLRS) Third Edition - Chapter 26")])])])}),[],!1,null,null,null);s.default=e.exports}}]);