(window.webpackJsonp=window.webpackJsonp||[]).push([[63],{338:function(e,r,t){"use strict";t.r(r);var a=t(28),n=Object(a.a)({},(function(){var e=this,r=e.$createElement,t=e._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"embedding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#embedding"}},[e._v("#")]),e._v(" Embedding")]),e._v(" "),t("p",[e._v("Create Date: 2019.10.16")]),e._v(" "),t("p",[e._v("Last Update Date: 2020.04.22")]),e._v(" "),t("p",[e._v("By "),t("a",{attrs:{href:"https://yuweiyin.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("YuweiYin"),t("OutboundLink")],1)]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-整体讲解"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-整体讲解"}},[e._v("#")]),e._v(" 1.整体讲解")]),e._v(" "),t("h3",{attrs:{id:"视频"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#视频"}},[e._v("#")]),e._v(" 视频")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://www.youtube.com/watch?v=X7PH3NuYW0Q",target:"_blank",rel:"noopener noreferrer"}},[e._v("李宏毅 - ML2017 Youtube"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://www.bilibili.com/video/av10590361/?p=25",target:"_blank",rel:"noopener noreferrer"}},[e._v("李宏毅 - ML2017 BiliBili"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"博客"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#博客"}},[e._v("#")]),e._v(" 博客")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"http://licstar.net/archives/328",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deep Learning in NLP - LICSTAR"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4122",target:"_blank",rel:"noopener noreferrer"}},[e._v("词向量与Embedding究竟是怎么回事？ - 苏剑林"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"论文"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#论文"}},[e._v("#")]),e._v(" 论文")]),e._v(" "),t("ul",[t("li",[e._v("1986 - G. Hinton - "),t("a",{attrs:{href:"https://www.cs.toronto.edu/~hinton/absps/ieee-lre.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Learning distributed representations of concepts"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2000 - Wei Xu - "),t("a",{attrs:{href:"http://www.speech.cs.cmu.edu/Communicator/papers/01291.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Can Artificial Neural Networks Learn Language Models?"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2003 - Y. Bengio - "),t("a",{attrs:{href:"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Neural Probabilistic Language Model"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2007 - A. Mnih, G. Hinton - "),t("a",{attrs:{href:"https://www.cs.toronto.edu/~amnih/papers/threenew.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Three New Graphical Models for Statistical Language Modelling"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2008 - A. Mnih, G. Hinton - "),t("a",{attrs:{href:"https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Scalable Hierarchical Distributed Language Model"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2008 - C&W - "),t("a",{attrs:{href:"http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Natural Language Processing (Almost) from Scratch"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2010 - "),t("a",{attrs:{href:"http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Word representations: a simple and general method for semi-supervised learning"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2010 - T. Mikolov - "),t("a",{attrs:{href:"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Recurrent neural network based language model"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2012 - T. Mikolov - "),t("a",{attrs:{href:"https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Statistical Language Models based on Neural Networks"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2012 - Huang - "),t("a",{attrs:{href:"https://nlp.stanford.edu/pubs/HuangACL12.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Improving Word Representations via Global Context and Multiple Word Prototypes"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2013 - T. Mikolov - "),t("a",{attrs:{href:"https://www.aclweb.org/anthology/N13-1090.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Linguistic Regularities in Continuous Space Word Representations"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2015 - "),t("a",{attrs:{href:"https://arxiv.org/pdf/1507.05523.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("How to Generate a Good Word Embedding?"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2016 - "),t("em",[e._v("中文")]),e._v(" "),t("a",{attrs:{href:"https://arxiv.org/abs/1611.05962",target:"_blank",rel:"noopener noreferrer"}},[e._v("Word and Document Embeddings based on Neural Network Approaches"),t("OutboundLink")],1)])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-word2vec-word-to-vector"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-word2vec-word-to-vector"}},[e._v("#")]),e._v(" 2.Word2Vec (Word to Vector)")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://code.google.com/archive/p/word2vec/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Word2Vec WebPage/Code"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://arxiv.org/abs/1301.3781",target:"_blank",rel:"noopener noreferrer"}},[e._v("Word2Vec Paper"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"博客-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#博客-2"}},[e._v("#")]),e._v(" 博客")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("不可思议的Word2Vec")]),e._v(" "),t("ol",[t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4299",target:"_blank",rel:"noopener noreferrer"}},[e._v("数学原理"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4304",target:"_blank",rel:"noopener noreferrer"}},[e._v("训练好的模型"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4316",target:"_blank",rel:"noopener noreferrer"}},[e._v("提取关键词"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4368",target:"_blank",rel:"noopener noreferrer"}},[e._v("不一样的“相似”"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4402",target:"_blank",rel:"noopener noreferrer"}},[e._v("Tensorflow版的Word2Vec"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4515",target:"_blank",rel:"noopener noreferrer"}},[e._v("Keras版的Word2Vec"),t("OutboundLink")],1)])])]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/6191",target:"_blank",rel:"noopener noreferrer"}},[e._v("最小熵原理（四）：“物以类聚”之从图书馆到词向量"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("详解 Word2vec 之 Skip-Gram 模型（结构篇）"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"论文-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#论文-2"}},[e._v("#")]),e._v(" 论文")]),e._v(" "),t("ul",[t("li",[e._v("2003 - "),t("a",{attrs:{href:"http://www.jmlr.org/papers/v3/bengio03a",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Neural Probabilistic Language Model"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2013 - Word2Vec "),t("a",{attrs:{href:"https://arxiv.org/abs/1301.3781",target:"_blank",rel:"noopener noreferrer"}},[e._v("Efficient Estimation of Word Representations in Vector Space"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2013 - "),t("a",{attrs:{href:"https://arxiv.org/abs/1310.4546",target:"_blank",rel:"noopener noreferrer"}},[e._v("Distributed Representations of Words and Phrases and their Compositionality"),t("OutboundLink")],1)])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_3-glove-global-vectors"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-glove-global-vectors"}},[e._v("#")]),e._v(" 3.GloVe (Global Vectors)")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://nlp.stanford.edu/projects/glove/",target:"_blank",rel:"noopener noreferrer"}},[e._v("GloVe WebPage"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://nlp.stanford.edu/pubs/glove.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("GloVe Paper"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://github.com/stanfordnlp/GloVe",target:"_blank",rel:"noopener noreferrer"}},[e._v("GloVe Code"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"博客-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#博客-3"}},[e._v("#")]),e._v(" 博客")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("更别致的词向量模型")]),e._v(" "),t("ol",[t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4667",target:"_blank",rel:"noopener noreferrer"}},[e._v("simpler glove"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4669",target:"_blank",rel:"noopener noreferrer"}},[e._v("对语言进行建模"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4671",target:"_blank",rel:"noopener noreferrer"}},[e._v("描述相关的模型"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4675",target:"_blank",rel:"noopener noreferrer"}},[e._v("模型的求解"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4677",target:"_blank",rel:"noopener noreferrer"}},[e._v("有趣的结果"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://kexue.fm/archives/4681",target:"_blank",rel:"noopener noreferrer"}},[e._v("代码、分享与结语"),t("OutboundLink")],1)])])])]),e._v(" "),t("h3",{attrs:{id:"使用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#使用"}},[e._v("#")]),e._v(" 使用")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://www.jianshu.com/p/4148c0c72d95",target:"_blank",rel:"noopener noreferrer"}},[e._v("GloVe在Linux下的安装与使用"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://www.jianshu.com/p/5b60e5b27cf1",target:"_blank",rel:"noopener noreferrer"}},[e._v("如何训练并使用GloVe词向量模型"),t("OutboundLink")],1)])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_4-doc2vec-document-to-vector"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-doc2vec-document-to-vector"}},[e._v("#")]),e._v(" 4.Doc2Vec (Document to Vector)")]),e._v(" "),t("h3",{attrs:{id:"论文-3"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#论文-3"}},[e._v("#")]),e._v(" 论文")]),e._v(" "),t("ul",[t("li",[e._v("Doc2Vec (Beyond Bag of Words)\n"),t("ul",[t("li",[e._v("ICML2014 - "),t("a",{attrs:{href:"https://arxiv.org/abs/1405.4053",target:"_blank",rel:"noopener noreferrer"}},[e._v("Distributed Representations of Sentences and Documents"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2015 - "),t("a",{attrs:{href:"https://arxiv.org/abs/1506.01057.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Hierarchical Neural Autoencoder for Paragraphs and Documents"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2015 - "),t("a",{attrs:{href:"https://arxiv.org/abs/1506.06726",target:"_blank",rel:"noopener noreferrer"}},[e._v("Skip-Thought Vectors"),t("OutboundLink")],1)])])]),e._v(" "),t("li",[e._v("Exploiting other kind of labels\n"),t("ul",[t("li",[e._v("2013 - "),t("a",{attrs:{href:"https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Learning Deep Structured Semantic Models for Web Search using Clickthrough Data"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2014 - "),t("a",{attrs:{href:"http://www.iro.umontreal.ca/~lisa/pointeurs/ir0895-he-2.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("2015 - "),t("a",{attrs:{href:"https://arxiv.org/abs/1503.00075",target:"_blank",rel:"noopener noreferrer"}},[e._v("Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"),t("OutboundLink")],1)])])])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_5-neighbor-embedding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-neighbor-embedding"}},[e._v("#")]),e._v(" 5.Neighbor Embedding")]),e._v(" "),t("h3",{attrs:{id:"manifold-learning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#manifold-learning"}},[e._v("#")]),e._v(" Manifold Learning")]),e._v(" "),t("ul",[t("li",[e._v("嵌入在高维欧式空间中的低维超曲面(流形 Manifold)，如果用高维欧式的坐标来表达流形上的点，然后用高维空间的欧式距离来度量这些点上的相似度，其实是不一定有道理的，尤其是当做比对的点距离比较大的时候。")]),e._v(" "),t("li",[e._v("比如：嵌入在三维空间的一个被弯曲折叠的二维曲面，因折叠而贴近的两点，并不一定在曲面上的距离很近。")]),e._v(" "),t("li",[e._v("形式化地说：要比较点A与点B、点C的谁更相近，在高维空间计算欧式距离发现|AB|>|AC|，却不代表在流形上A一定与B更近、与C更远。")]),e._v(" "),t("li",[e._v("如果把这个高维空间投影到一个低维空间，使得原来的超曲面得以“摊平”，这样在低维空间上的距离度量就更能准确地表达超曲面上的点之间距离了。而这种投影的理念，"),t("strong",[e._v("在 Embedding 中就能体现！")])])]),e._v(" "),t("h3",{attrs:{id:"local-linear-embedding-lle"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#local-linear-embedding-lle"}},[e._v("#")]),e._v(" Local Linear Embedding (LLE)")]),e._v(" "),t("ul",[t("li",[e._v("通过约束使得原高维空间中相似/相近的点，在降维变换后的低维空间中仍然相似/相近。")]),e._v(" "),t("li",[e._v("JMLR2013 - "),t("a",{attrs:{href:"http://www.jmlr.org/papers/volume4/saul03a/saul03a.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"laplacian-eigenmaps"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#laplacian-eigenmaps"}},[e._v("#")]),e._v(" Laplacian Eigenmaps")]),e._v(" "),t("ul",[t("li",[e._v("可以用于度量流形上面样本点的光滑度。")]),e._v(" "),t("li",[t("a",{attrs:{href:"http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NIPS_01.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("ICML2008 - "),t("a",{attrs:{href:"http://www.thespermwhale.com/jaseweston/papers/deep_embed.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deep Learning via Semi-Supervised Embedding"),t("OutboundLink")],1)]),e._v(" "),t("li",[e._v("ICLR2017 - "),t("a",{attrs:{href:"https://arxiv.org/abs/1611.01449.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Semi-supervised deep learning by metric embedding"),t("OutboundLink")],1)])]),e._v(" "),t("h3",{attrs:{id:"t-sne-t-distributed-stochastic-neighbor-embedding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#t-sne-t-distributed-stochastic-neighbor-embedding"}},[e._v("#")]),e._v(" t-SNE (T-distributed Stochastic Neighbor Embedding)")]),e._v(" "),t("ul",[t("li",[e._v("简要介绍\n"),t("ul",[t("li",[e._v("前面的做法，但没有约束使得那些在原高维空间中不相似/不相近的点，在降维变换后的低维空间中仍然不相似/不相近。")]),e._v(" "),t("li",[e._v("这就可能会导致不同类别的样本点也相互靠近、耦合堆叠，不利于分类、聚类。而 t-SNE 能够处理这个问题。")]),e._v(" "),t("li",[e._v("Trick：一般不会直接在原高维空间做 t-SNE，因为在高维空间计算相似度 S(x_i, x_j) 的计算量很大，所以通常会用比较快的方法(比如PCA)先降到一个较低的维度，然后再用 t-SNE 从这个较低的维度开始继续降维。")]),e._v(" "),t("li",[e._v("由于 t-SNE 只能对一次性对所有输入的样本点进行降维转换，因此如果后来又出现一个新的样本点，是没法在原来的基础上将这个新样本点降维到同一个低维度的。")]),e._v(" "),t("li",[e._v("如果要将这个新样本点降维，那就只能用之前的所有数据加上新数据一起拿出来 "),t("strong",[e._v("从头重新")]),e._v("运行一遍 t-SNE 的。")]),e._v(" "),t("li",[e._v("因此 t-SNE 不适合于训练集测试集分开的一般情况，它一般用来做数据可视化分析。")])])]),e._v(" "),t("li",[e._v("相似性度量 Similarity Measure\n"),t("ul",[t("li",[e._v("记|x|表示 x 的 2-范数/Euclid-Norm，原高维空间计算相似度的函数为 $ S(x_i, x_j) = exp(-| x_i - x_j|) $ 即高斯核/径向基函数，在低维空间 SNE 采用的还是高斯核 $ S'(z_i, z_j) = exp(-|z_i - z_j|) $，而 t-SNE 则利用了 t 分布，其计算相似度的函数为 $ S'(z_i, z_j) = 1 / (1 + | z_i - z_j|) $")]),e._v(" "),t("li",[e._v("t-SNE 这样做的好处在于：（在图像上会看得更直观）\n"),t("ol",[t("li",[e._v("设原高维空间中两点 $ x_i $ 和 $ x_j $ 的S函数值 $ S(x_i, x_j) = k $，那么为了保持S'函数值 $ S'(z_i, z_j) $ 也等于 $ k $，需要低维空间中的两点 $ z_i $ 和 $ z_j $ 的距离被拉大一些(相比于 $ x_i $ 和 $ x_j $ 的距离)。")]),e._v(" "),t("li",[e._v("根据 t-SNE 的低维空间相似性函数的特性：在原高维空间中距离较近的两点(S函数值较大)，为了在低维空间中的S'函数值仍然保持同等水平，只需被拉开很少的量；而在原高维空间中距离较远的两点(S函数值较小)的两点，为了在低维空间中的S'函数值仍然保持同等水平，需要被拉开很大的量，这会使得原本就相距较远的两点在低维空间中相距更远了。")])])])])])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"其它"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其它"}},[e._v("#")]),e._v(" 其它")]),e._v(" "),t("ul",[t("li",[e._v("对 Embedding 的一些更多的理解：\n"),t("ul",[t("li",[e._v("假设词典里有 10^6 个词/Token，先用 one-hot 编码(或者也可以根据词频采用 Huffman 编码、或者直接实数ID编码、或者二进制数编码等等)，如果使用 one-hot 编码，则会得到一个 10^6 x 10^6 的方阵，十分巨大。而 Embedding 类似于做了一个矩阵分解 Matrix Decomposition / 低秩逼近 Low-Rank Approximation，[10^6 x 10^6] = [10^6 x 100] x [100 x 10^6]，假设压缩为 100 维的 Embedding。")]),e._v(" "),t("li",[e._v("而分解开来的两个矩阵分别对应了压缩和解压的功能。以线性变换的角度来看，压缩矩阵实际上是对原方阵的某些行/列的挑选、然后再叠加的操作，那么如果能挑选出“最重要”的部分，低秩逼近就会很成功，因此 Embedding 的表示效果也更佳。")]),e._v(" "),t("li",[e._v("这种降维/压缩、挑选重要部分的做法，可以借鉴 PCA 主成分分析的思想。而矩阵分解的思想，可以参考 SVD 奇异值分解、LU 三角分解等。")]),e._v(" "),t("li",[e._v("进一步谈谈 Embedding 的“压缩存储”能力，实际上压缩能力并不如想象中那么惊人。首先，为了避免损失太多信息，一般不会把维度压缩得太低。其次，压缩后每一维存储的是实数，一般需要双精度存储，而原本 one-hot 的每一维只需要一个 bit 位即可。最后，one-hot 编码的向量组成的矩阵是满秩、每一行/列仅有一个元素为 1、其余均为 0，这样的矩阵执行矩阵乘法和矩阵求逆是十分容易的，相较于稠密的实数矩阵而言。")]),e._v(" "),t("li",[e._v("所以，从原文中抽取重要信息、融合表示上下文信息，才是词向量的核心意义所在，而非“压缩存储”功能，虽然深度神经网络很需要这个压缩工作，否则会出现“维数灾难”，但这也只是神经网络的众多毛病之一而已。")])])]),e._v(" "),t("li",[e._v("Tomas Mikolov (Word2Vec 作者) 解释为何只使用浅层的神经网络(只有一层 hidden layer)，而不采用深层的网络：\n"),t("ul",[t("li",[e._v("1.在 Mikolov 之前已经有人做了很多 Word2Vec 的尝试，多数都是使用深度神经网络，但是效果一直没能做得很好，而且深层网络会带来大量的计算开销，以至于无法使用大量数据来训练。Mikolov 使用浅层的网络减少了网络前馈和反馈的计算开销，增大训练数据的量，使得最终效果很好。")]),e._v(" "),t("li",[e._v("2.Mikolov 的 "),t("a",{attrs:{href:"https://code.google.com/archive/p/word2vec/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Word2Vec 实现"),t("OutboundLink")],1),e._v(" 中，有很多优化的 trick & skill，是一个很成功的工程实现，最终做出了实际可行、效果上佳的 Word2Vec 实现。")])])])]),e._v(" "),t("hr")])}),[],!1,null,null,null);r.default=n.exports}}]);