(window.webpackJsonp=window.webpackJsonp||[]).push([[188],{437:function(e,i,n){"use strict";n.r(i);var a=n(28),t=Object(a.a)({},(function(){var e=this,i=e.$createElement,n=e._self._c||i;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"interpretability-paper"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#interpretability-paper"}},[e._v("#")]),e._v(" Interpretability Paper")]),e._v(" "),n("p",[e._v("By "),n("a",{attrs:{href:"https://github.com/YuweiYin",target:"_blank",rel:"noopener noreferrer"}},[e._v("YuweiYin"),n("OutboundLink")],1)]),e._v(" "),n("hr"),e._v(" "),n("h2",{attrs:{id:"paper-list"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#paper-list"}},[e._v("#")]),e._v(" Paper List")]),e._v(" "),n("h3",{attrs:{id:"_2010-2015"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2010-2015"}},[e._v("#")]),e._v(" 2010 - 2015")]),e._v(" "),n("ul",[n("li",[e._v("(2013) (ICCV) HOGgles: Visualizing Object Detection Features")]),e._v(" "),n("li",[e._v("(2014) (ECCV) Visualizing and Understanding Convolutional Networks")]),e._v(" "),n("li",[e._v("(2014) (ICLR) Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps")]),e._v(" "),n("li",[e._v("(2015) (AAS) Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model")]),e._v(" "),n("li",[e._v("(2015) (CVPR) Understanding Deep Image Representations by Inverting Them")]),e._v(" "),n("li",[e._v("(2015) (ICCV) Understanding deep features with computer-generated imagery")]),e._v(" "),n("li",[e._v("(2015) (ICLR) Striving for Simplicity: The All Convolutional Net")]),e._v(" "),n("li",[e._v("(2015) (ICML) Understanding Neural Networks Through Deep Visualization")])]),e._v(" "),n("h3",{attrs:{id:"_2016"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2016"}},[e._v("#")]),e._v(" 2016")]),e._v(" "),n("ul",[n("li",[e._v("(2016) (ACL) Explaining Predictions of Non-Linear Classifiers in NLP")]),e._v(" "),n("li",[e._v("(2016) (arXiv) Attentive Explanations: Justifying Decisions and Pointing to the Evidence")]),e._v(" "),n("li",[e._v("(2016) (arXiv) Grad-CAM: Why did you say that")]),e._v(" "),n("li",[e._v("(2016) (arXiv) Investigating the influence of noise and distractors on the interpretation of neural networks")]),e._v(" "),n("li",[e._v("(2016) (arXiv) Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks")]),e._v(" "),n("li",[e._v("(2016) (arXiv) The Mythos of Model Interpretability")]),e._v(" "),n("li",[e._v("(2016) (arXiv) Understanding Neural Networks through Representation Erasure")]),e._v(" "),n("li",[e._v("(2016) (CVPR) Analyzing Classifiers: Fisher Vectors and Deep Neural Networks")]),e._v(" "),n("li",[e._v("(2016) (CVPR) Inverting Visual Representations with Convolutional Networks")]),e._v(" "),n("li",[e._v("(2016) (CVPR) Visualizing and Understanding Deep Texture Representations")]),e._v(" "),n("li",[e._v("(2016) (ECCV) Design of kernels in convolutional neural networks for image classification")]),e._v(" "),n("li",[e._v("(2016) (ECCV) Generating Visual Explanations")]),e._v(" "),n("li",[e._v("(2016) (EMNLP) Rationalizing Neural Predictions")]),e._v(" "),n("li",[e._v("(2016) (ICML) Understanding and improving convolutional neural networks via concatenated rectified linear units")]),e._v(" "),n("li",[e._v("(2016) (ICML) Visualizing and Comparing AlexNet and VGG using Deconvolutional Layers")]),e._v(" "),n("li",[e._v("(2016) (IJCV) Visualizing deep convolutional neural networks using natural pre-images")]),e._v(" "),n("li",[e._v("(2016) (IJCV) Visualizing Object Detection Features")]),e._v(" "),n("li",[e._v("(2016) (KDD) LIME-Why should i trust you: Explaining the predictions of any classifier")]),e._v(" "),n("li",[e._v("(2016) (NAACL) Visualizing and Understanding Neural Models in NLP")]),e._v(" "),n("li",[e._v("(2016) (NIPS) Synthesizing the preferred inputs for neurons in neural networks via deep generator networks")]),e._v(" "),n("li",[e._v("(2016) (NIPS) Understanding the effective receptive field in deep convolutional neural networks")]),e._v(" "),n("li",[e._v("(2016) (TVCG) Towards Better Analysis of Deep Convolutional Neural Networks")]),e._v(" "),n("li",[e._v("(2016) (TVCG) Visualizing the Hidden Activity of Artificial Neural Networks")])]),e._v(" "),n("h3",{attrs:{id:"_2017"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2017"}},[e._v("#")]),e._v(" 2017")]),e._v(" "),n("ul",[n("li",[e._v("(2017) (AAAI) Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning")]),e._v(" "),n("li",[e._v("(2017) (ACL) Visualizing and Understanding Neural Machine Translation")]),e._v(" "),n("li",[e._v("(2017) (arXiv) Contextual Explanation Networks")]),e._v(" "),n("li",[e._v("(2017) (arXiv) Distilling a neural network into a soft decision tree")]),e._v(" "),n("li",[e._v("(2017) (arXiv) Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models")]),e._v(" "),n("li",[e._v("(2017) (arXiv) Interpretable & Explorable Approximations of Black Box Models")]),e._v(" "),n("li",[e._v("(2017) (arXiv) SmoothGrad: removing noise by adding noise")]),e._v(" "),n("li",[e._v("(2017) (arXiv) Towards interpretable deep neural networks by leveraging adversarial examples")]),e._v(" "),n("li",[e._v("(2017) (arXiv) Transparency: Motivations and Challenges")]),e._v(" "),n("li",[e._v("(2017) (CEURW) What does explainable AI really mean: A new conceptualization of perspectives")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Improving Interpretability of Deep Neural Networks with Semantic Information")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Interpretable 3d human action analysis with temporal convolutional networks")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Knowing when to look: Adaptive attention via a visual sentinel for image captioning")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Looking under the hood: Deep neural network visualization to interpret whole-slide image analysis outcomes for colorectal polyps")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering")]),e._v(" "),n("li",[e._v("(2017) (CVPR) MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Mining Object Parts from CNNs via Active Question-Answering")]),e._v(" "),n("li",[e._v("(2017) (CVPR) Network dissection: Quantifying interpretability of deep visual representations")]),e._v(" "),n("li",[e._v("(2017) (EMNLP) A causal framework for explaining the predictions of black-box sequence-to-sequence models")]),e._v(" "),n("li",[e._v("(2017) (ICCV) Grad-cam: Visual explanations from deep networks via gradient-based localization")]),e._v(" "),n("li",[e._v("(2017) (ICCV) Interpretable Explanations of Black Boxes by Meaningful Perturbation")]),e._v(" "),n("li",[e._v("(2017) (ICCV) Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention")]),e._v(" "),n("li",[e._v("(2017) (ICCV) Learning to Disambiguate by Asking Discriminative Questions")]),e._v(" "),n("li",[e._v("(2017) (ICCV) Understanding and comparing deep neural networks for age and gender classification")]),e._v(" "),n("li",[e._v("(2017) (ICLR) Exploring LOTS in Deep Neural Networks")]),e._v(" "),n("li",[e._v("(2017) (ICLR) Visualizing deep neural network decisions: Prediction difference analysis")]),e._v(" "),n("li",[e._v("(2017) (ICML) Axiomatic Attribution for Deep Networks")]),e._v(" "),n("li",[e._v("(2017) (ICML) Learning Important Features Through Propagating Activation Differences")]),e._v(" "),n("li",[e._v("(2017) (ICML) Understanding Black-box Predictions via Influence Functions")]),e._v(" "),n("li",[e._v("(2017) (IJCAI) Right for the right reasons: Training differentiable models by constraining their explanations")]),e._v(" "),n("li",[e._v("(2017) (IJCAI) Understanding and improving convolutional neural networks via concatenated rectified linear units")]),e._v(" "),n("li",[e._v("(2017) () Interpretability of deep learning models: a survey of results")]),e._v(" "),n("li",[e._v("(2017) (NIPS) A Unified Approach to Interpreting Model Predictions")]),e._v(" "),n("li",[e._v("(2017) (NIPS) Real Time Image Saliency for Black Box Classifiers")]),e._v(" "),n("li",[e._v("(2017) (NIPS) SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability")]),e._v(" "),n("li",[e._v("(2017) (SOSP) Deepxplore: Automated whitebox testing of deep learning systems")]),e._v(" "),n("li",[e._v("(2017) (TVCG) ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models")])]),e._v(" "),n("h3",{attrs:{id:"_2018"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2018"}},[e._v("#")]),e._v(" 2018")]),e._v(" "),n("ul",[n("li",[e._v("(2018) (AAAI) Anchors: High-Precision Model-Agnostic Explanations")]),e._v(" "),n("li",[e._v("(2018) (AAAI) Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions")]),e._v(" "),n("li",[e._v("(2018) (AAAI) Examining CNN Representations With Respect To Dataset Bias")]),e._v(" "),n("li",[e._v("(2018) (AAAI) Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients")]),e._v(" "),n("li",[e._v("(2018) (AAAI) Interpreting CNN Knowledge Via An Explanatory Graph")]),e._v(" "),n("li",[e._v("(2018) (ACL) Did the Model Understand the Question")]),e._v(" "),n("li",[e._v("(2018) (ACM-CompSurv) A Survey of Methods for Explaining Black Box Models")]),e._v(" "),n("li",[e._v("(2018) (arXiv) Computationally Efficient Measures of Internal Neuron Importance")]),e._v(" "),n("li",[e._v("(2018) (arXiv) Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation")]),e._v(" "),n("li",[e._v("(2018) (arXiv) How convolutional neural network see the world-A survey of convolutional neural network visualization methods")]),e._v(" "),n("li",[e._v("(2018) (arXiv) Manipulating and Measuring Model Interpretability")]),e._v(" "),n("li",[e._v("(2018) (arXiv) Revisiting the importance of individual units in cnns via ablation")]),e._v(" "),n("li",[e._v("(2018) (arXiv) Unsupervised Learning of Neural Networks to Explain Neural Networks")]),e._v(" "),n("li",[e._v("(2018) (BMVC) Rise: Randomized input sampling for explanation of black-box models")]),e._v(" "),n("li",[e._v("(2018) (CommunACM) The Mythos of Model Interpretability")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Interpret Neural Networks by Identifying Critical Data Routing Paths")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Interpretable Convolutional Neural Networks")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Learning to Act Properly: Predicting and Explaining Affordances from Images")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Multimodal Explanations: Justifying Decisions and Pointing to the Evidence")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Teaching Categories to Human Learners with Visual Explanations")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Tell Me Where to Look: Guided Attention Inference Network")]),e._v(" "),n("li",[e._v("(2018) (CVPR) Transparency by design: Closing the gap between performance and interpretability in visual reasoning")]),e._v(" "),n("li",[e._v("(2018) (CVPR) What do Deep Networks Like to See")]),e._v(" "),n("li",[e._v("(2018) (CVPR) What have we learned from deep representations for action recognition")]),e._v(" "),n("li",[e._v("(2018) (DSP) Methods for Interpreting and Understanding Deep Neural Networks")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Deep clustering for unsupervised learning of visual features")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Diverse feature visualizations reveal invariances in early layers of deep neural networks")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Explainable neural computation via stack neural module networks")]),e._v(" "),n("li",[e._v("(2018) (ECCV) ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Grounding Visual Explanations")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Interpretable basis decomposition for visual explanation")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Textual Explanations for Self-Driving Vehicles")]),e._v(" "),n("li",[e._v("(2018) (ECCV) Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions")]),e._v(" "),n("li",[e._v("(2018) (FITEE) Visual Interpretability for Deep Learning: a Survey")]),e._v(" "),n("li",[e._v("(2018) (ICLR) Detecting statistical interactions from neural network weights")]),e._v(" "),n("li",[e._v("(2018) (ICLR) Interpretable counting for visual question answering")]),e._v(" "),n("li",[e._v("(2018) (ICLR) Learning how to explain neural networks: PatternNet and PatternAttribution")]),e._v(" "),n("li",[e._v("(2018) (ICLR) On the importance of single directions for generalization")]),e._v(" "),n("li",[e._v("(2018) (ICLR) Towards better understanding of gradient-based attribution methods for deep neural networks")]),e._v(" "),n("li",[e._v("(2018) (ICML) Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples")]),e._v(" "),n("li",[e._v("(2018) (ICML) Interpretability beyond feature attribution: Quantitative testing with concept activation vectors")]),e._v(" "),n("li",[e._v("(2018) (ICML) Learning to explain: An information-theoretic perspective on model interpretation")]),e._v(" "),n("li",[e._v("(2018) (IEEEacesss) Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)")]),e._v(" "),n("li",[e._v("(2018) (IJCV) Top-down neural attention by excitation backprop")]),e._v(" "),n("li",[e._v("(2018) (JAIR) Learning Explanatory Rules from Noisy Data")]),e._v(" "),n("li",[e._v("(2018) (KDD) Towards Explanation of DNN-based Prediction with Guided Feature Inversion")]),e._v(" "),n("li",[e._v("(2018) (MIPRO) Explainable Artificial Intelligence: A Survey")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) Attacks meet interpretability: Attribute-steered detection of adversarial samples")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) DeepPINK: reproducible feature selection in deep neural networks")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) Explanations based on the missing: Towards contrastive explanations with pertinent negatives")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) Interpretable Convolutional Filters with SincNet")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) Representer point selection for explaining deep neural networks")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) Sanity Checks for Saliency Maps")]),e._v(" "),n("li",[e._v("(2018) (NeurIPS) Towards robust interpretability with self-explaining neural networks")]),e._v(" "),n("li",[e._v("(2018) (TPAMI) Interpreting deep visual representations via network dissection")]),e._v(" "),n("li",[e._v("(2018) (WACV) Examining CNN Representations With Respect To Dataset Bias")])]),e._v(" "),n("h3",{attrs:{id:"_2019"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2019"}},[e._v("#")]),e._v(" 2019")]),e._v(" "),n("ul",[n("li",[e._v("(2019) (AAAI) Can You Explain That: Lucid Explanations Help Human-AI Collaborative Image Retrieval")]),e._v(" "),n("li",[e._v("(2019) (AAAI) Classifier-agnostic saliency map extraction")]),e._v(" "),n("li",[e._v("(2019) (AAAI) Interpretation of Neural Networks is Fragile")]),e._v(" "),n("li",[e._v("(2019) (AAAI) Network Transplanting")]),e._v(" "),n("li",[e._v("(2019) (AAAI) Unsupervised Learning of Neural Networks to Explain Neural Networks")]),e._v(" "),n("li",[e._v("(2019) (ACL) Attention is not Explanation")]),e._v(" "),n("li",[e._v("(2019) (ACMFAT) Explaining Explanations in AI")]),e._v(" "),n("li",[e._v("(2019) (AI) Explanation in Artificial Intelligence: Insights from the Social Sciences")]),e._v(" "),n("li",[e._v("(2019) (arXiv) Attention Interpretability Across NLP Tasks")]),e._v(" "),n("li",[e._v("(2019) (arXiv) Interpretable CNNs for Object Classification")]),e._v(" "),n("li",[e._v("(2019) (CSUR) A Survey of Methods for Explaining Black Box Models")]),e._v(" "),n("li",[e._v("(2019) (CVPR) Attention branch network: Learning of attention mechanism for visual explanation")]),e._v(" "),n("li",[e._v("(2019) (CVPR) FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference")]),e._v(" "),n("li",[e._v("(2019) (CVPR) From Recognition to Cognition: Visual Commonsense Reasoning")]),e._v(" "),n("li",[e._v("(2019) (CVPR) Interpretable and fine-grained visual explanations for convolutional neural networks")]),e._v(" "),n("li",[e._v("(2019) (CVPR) Interpreting CNNs via Decision Trees")]),e._v(" "),n("li",[e._v("(2019) (CVPR) Learning to Explain with Complemental Examples")]),e._v(" "),n("li",[e._v("(2019) (CVPR) Multimodal Explanations by Predicting Counterfactuality in Videos")]),e._v(" "),n("li",[e._v("(2019) (CVPR) Revealing Scenes by Inverting Structure from Motion Reconstructions")]),e._v(" "),n("li",[e._v("(2019) (CVPRW) Visualizing the Resilience of Deep Convolutional Network Interpretations")]),e._v(" "),n("li",[e._v("(2019) (EMNLP) Attention is not not Explanation")]),e._v(" "),n("li",[e._v("(2019) (ExplainableAI) The (Un)reliability of saliency methods")]),e._v(" "),n("li",[e._v("(2019) (ICAIS) Interpreting Black Box Predictions using Fisher Kernels")]),e._v(" "),n("li",[e._v("(2019) (ICCV) Explaining Neural Networks Semantically and Quantitatively")]),e._v(" "),n("li",[e._v("(2019) (ICCV) Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded")]),e._v(" "),n("li",[e._v("(2019) (ICCV) Towards Interpretable Face Recognition")]),e._v(" "),n("li",[e._v("(2019) (ICCV) U-CAM: Visual Explanation using Uncertainty based Class Activation Maps")]),e._v(" "),n("li",[e._v("(2019) (ICCV) Understanding Deep Networks via Extremal Perturbations and Smooth Masks")]),e._v(" "),n("li",[e._v("(2019) (ICLR) Hierarchical interpretations for neural network predictions")]),e._v(" "),n("li",[e._v("(2019) (ICLR) How Important Is a Neuron")]),e._v(" "),n("li",[e._v("(2019) (ICLR) Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks")]),e._v(" "),n("li",[e._v("(2019) (ICML) Towards A Deep and Unified Understanding of Deep Neural Models in NLP")]),e._v(" "),n("li",[e._v("(2019) (JVCIR) Interpretable Convolutional Neural Networks via Feedforward Design")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) A benchmark for interpretability methods in deep neural networks")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) Can you trust your model's uncertainty: Evaluating predictive uncertainty under dataset shift")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) CXPlain: Causal explanations for model interpretation under uncertainty")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) Full-gradient representation for neural network visualization")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) On the (In) fidelity and Sensitivity of Explanations")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) This looks like that: deep learning for interpretable image recognition")]),e._v(" "),n("li",[e._v("(2019) (NeurIPS) Towards Automatic Concept-based Explanations")]),e._v(" "),n("li",[e._v("(2019) (NMI) Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead")])]),e._v(" "),n("h3",{attrs:{id:"_2020"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2020"}},[e._v("#")]),e._v(" 2020")]),e._v(" "),n("ul",[n("li",[e._v("(2020) (arXiv) A Survey on Neural Network Interpretability")]),e._v(" "),n("li",[e._v("(2020) (CVPR) Explaining Knowledge Distillation by Quantifying the Knowledge")]),e._v(" "),n("li",[e._v("(2020) (CVPR) High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks")]),e._v(" "),n("li",[e._v("(2020) (CVPR) Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks")]),e._v(" "),n("li",[e._v("(2020) (ICLR) Interpretable Complex-Valued Neural Networks for Privacy Protection")]),e._v(" "),n("li",[e._v("(2020) (ICLR) Knowledge consistency between neural networks and beyond")])]),e._v(" "),n("h3",{attrs:{id:"_2021"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2021"}},[e._v("#")]),e._v(" 2021")]),e._v(" "),n("ul",[n("li",[e._v("(2021) (AAAI) Self-Attention Attribution: Interpreting Information Interactions Inside Transformer")])]),e._v(" "),n("hr")])}),[],!1,null,null,null);i.default=t.exports}}]);