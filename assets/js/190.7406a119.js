(window.webpackJsonp=window.webpackJsonp||[]).push([[190],{435:function(e,r,t){"use strict";t.r(r);var a=t(28),i=Object(a.a)({},(function(){var e=this,r=e.$createElement,t=e._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"xai-survey-paper"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#xai-survey-paper"}},[e._v("#")]),e._v(" XAI Survey Paper")]),e._v(" "),t("p",[e._v("By "),t("a",{attrs:{href:"https://github.com/YuweiYin",target:"_blank",rel:"noopener noreferrer"}},[e._v("YuweiYin"),t("OutboundLink")],1)]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"paper-list"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#paper-list"}},[e._v("#")]),e._v(" Paper List")]),e._v(" "),t("ul",[t("li",[e._v("(2017) (arXiv 1702.05747) A survey on deep learning in medical image analysis")]),e._v(" "),t("li",[e._v("(2017) (IEEE SmartWorld) Interpretability of deep learning models: A survey of results")]),e._v(" "),t("li",[e._v("(2017) (NIPS) A Unified Approach to Interpreting Model Predictions")]),e._v(" "),t("li",[e._v("(2018) (ICLR) Towards better understanding of gradient-based attribution methods for Deep Neural Networks")]),e._v(" "),t("li",[e._v("(2019) (ACM-CompSurv) A Survey of Methods for Explaining Black Box Models")]),e._v(" "),t("li",[e._v("(2019) (arXiv 1901.04592) Interpretable machine learning: definitions, methods, and applications")]),e._v(" "),t("li",[e._v("(2020) (arXiv) A Survey on Neural Network Interpretability")]),e._v(" "),t("li",[e._v("(2020) (arXiv 2010.05607) The elephant in the interpretability room: Why use attention as explanation when we have saliency methods")]),e._v(" "),t("li",[e._v("(2021) (arXiv 2103.11251) Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges")])]),e._v(" "),t("hr")])}),[],!1,null,null,null);r.default=i.exports}}]);