(window.webpackJsonp=window.webpackJsonp||[]).push([[328],{575:function(a,r,e){"use strict";e.r(r);var n=e(28),t=Object(n.a)({},(function(){var a=this,r=a.$createElement,e=a._self._c||r;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"plm-transformer-xl"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#plm-transformer-xl"}},[a._v("#")]),a._v(" PLM - Transformer-XL")]),a._v(" "),e("p",[a._v("Natural Language Processing - "),e("a",{attrs:{href:"https://github.com/YuweiYin",target:"_blank",rel:"noopener noreferrer"}},[a._v("YuweiYin"),e("OutboundLink")],1)]),a._v(" "),e("hr"),a._v(" "),e("p",[e("em",[e("strong",[a._v("Transformer-XL")])])]),a._v(" "),e("p",[e("strong",[a._v("paper")]),a._v(": Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context")]),a._v(" "),e("ul",[e("li",[a._v("Author: Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov")]),a._v(" "),e("li",[a._v("Paper Link: "),e("a",{attrs:{href:"https://arxiv.org/abs/1901.02860",target:"_blank",rel:"noopener noreferrer"}},[a._v("arXiv"),e("OutboundLink")],1)]),a._v(" "),e("li",[a._v("Paper Download: "),e("a",{attrs:{href:"https://arxiv.org/pdf/1901.02860",target:"_blank",rel:"noopener noreferrer"}},[a._v("arXiv"),e("OutboundLink")],1)])]),a._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("@article{dai2019transformer,\n  title={Transformer-xl: Attentive language models beyond a fixed-length context},\n  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},\n  journal={arXiv preprint arXiv:1901.02860},\n  year={2019}\n}\n")])])])])}),[],!1,null,null,null);r.default=t.exports}}]);