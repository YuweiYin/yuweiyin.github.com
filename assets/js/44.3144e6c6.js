(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{262:function(t,s,n){"use strict";n.r(s);var a=n(28),e=Object(a.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"algorithm-graph-theory-ford-fulkerson"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#algorithm-graph-theory-ford-fulkerson"}},[t._v("#")]),t._v(" Algorithm - Graph Theory - Ford-Fulkerson")]),t._v(" "),n("p",[t._v("Create Date: 2020.06.03")]),t._v(" "),n("p",[t._v("Last Update Date: 2020.06.03")]),t._v(" "),n("p",[t._v("By "),n("a",{attrs:{href:"https://yuweiyin.github.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("YuweiYin"),n("OutboundLink")],1)]),t._v(" "),n("h2",{attrs:{id:"简介"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[t._v("#")]),t._v(" 简介")]),t._v(" "),n("p",[t._v("最大流 Max-Flow")]),t._v(" "),n("p",[t._v("Ford-Fulkerson 方法")]),t._v(" "),n("h3",{attrs:{id:"流网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#流网络"}},[t._v("#")]),t._v(" 流网络")]),t._v(" "),n("p",[t._v("流网络 G = (V, E) 是一个有向图，图中每条边 (u, v) \\in E 有一个"),n("strong",[t._v("非负")]),t._v("的"),n("strong",[t._v("容量值")]),t._v(" c(u, v) >= 0。而且，如果边集合 E 包含一条边 (u, v)，则图中不存在反方向的边 (v, u)。如果 (u, v) \\notin E，为方便起见，定义 c(u, v) = 0。并且在图中不允许有自循环/自圈 (u, u)。")]),t._v(" "),n("p",[t._v("另外，在流网络的所有结点中，有两个特殊的结点："),n("strong",[t._v("源结点")]),t._v(" s (source) 和"),n("strong",[t._v("汇点")]),t._v(" t (terminal)。源结点入度为 0、汇点出度为 0。为方便起见，假定 V 中每个结点 v 都位于某条从 s 到 v 的路径上，即有 "),n("code",[t._v("s ~> v ~> t")]),t._v(" 路径。")]),t._v(" "),n("p",[t._v("因此，流网络图是"),n("strong",[t._v("弱连通")]),t._v("的。并且由于除源结点 s 外的每个结点都至少有一条进入的边，有 "),n("code",[t._v("|E| >= |V| - 1")]),t._v("。")]),t._v(" "),n("ul",[n("li",[t._v("流网络的性质主要如下：\n"),n("ul",[n("li",[t._v("流网络 G = (V, E) 是一个弱连通的有向图")]),t._v(" "),n("li",[t._v("所有边的权重为非负值，且每条边的权重值有上界 c(u, v)")]),t._v(" "),n("li",[t._v("任意两个结点 u, v \\in V，不能同时存在边 (u, v) 和边 (v, u)")]),t._v(" "),n("li",[t._v("如果某边 (u, v) 不存在，定义其权重值 c(u, v) = 0")]),t._v(" "),n("li",[t._v("图中没有自循环 (u, u)")]),t._v(" "),n("li",[t._v("通常来说，源结点 s 的入度为 0，汇点 t 的出度为 0")]),t._v(" "),n("li",[t._v("每个结点 v 都处于从 s 到 t 的某条路径上。即：从起点 s 发出的流量可以流经 v 到达终点 t")])])])]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/max-flow-1.png",alt:"max-flow-1"}})]),t._v(" "),n("p",[t._v("这里给出流的形式化定义。设 G = (V, E) 是一个"),n("strong",[t._v("流网络")]),t._v("，其"),n("strong",[t._v("容量函数")]),t._v("为 c。设 s 为网络的"),n("strong",[t._v("源结点")]),t._v("，t 为"),n("strong",[t._v("汇点")]),t._v("。G 中的"),n("strong",[t._v("流")]),t._v("是一个"),n("strong",[t._v("实值函数")]),t._v(" f: VxV -> R，满足如下两条性质：")]),t._v(" "),n("ol",[n("li",[n("strong",[t._v("容量限制")]),t._v("：（“流量有限额”）对于所有的结点 u, v \\in V，要求 0 <= f(u, v) <= c(u, v)")]),t._v(" "),n("li",[n("strong",[t._v("流量守恒")]),t._v("：（“流入等于流出”）对于所有的结点 u \\in V - {s, t}，要求 $ \\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v) $\n"),n("ul",[n("li",[t._v("当 (u, v) \\notin E 时，从结点 u 到结点 v 之间没有流，因此 f(u, v) == 0")])])])]),t._v(" "),n("p",[t._v("称非负数值 f(u, v) 为从结点 u 到结点 v 的流。一个流 f 的值 "),n("code",[t._v("|f|")]),t._v(" 定义如下：")]),t._v(" "),n("p",[t._v("$$ |f| = \\sum_{v \\in V} f(s, v) = \\sum_{v \\in V} f(v, s) $$")]),t._v(" "),n("p",[t._v("即，流 f 的值时从源结点流出的总流量 减去 流入源结点的总流量。这里符号 "),n("code",[t._v("|·|")]),t._v(" 仅用作表达流的值，而不是数的绝对值或者集合的基数值。")]),t._v(" "),n("p",[t._v("通常来说，一个流网络不会有任何进入源结点的边，即源结点的入度为 0，故求和项 $ \\sum_{v \\in V} f(v, s) $ 的值将是 0。但对于有的网络而言（比如"),n("strong",[t._v("残存网络")]),t._v("），流入源结点的流量十分重要。")]),t._v(" "),n("p",[t._v("在"),n("strong",[t._v("最大流问题")]),t._v("中，给定一个"),n("strong",[t._v("流网络")]),t._v(" G、一个"),n("strong",[t._v("源结点")]),t._v(" s、一个"),n("strong",[t._v("汇点")]),t._v(" t，目标是找到值最大的一个"),n("strong",[t._v("流")]),t._v("。")]),t._v(" "),n("h2",{attrs:{id:"ford-fulkerson-方法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ford-fulkerson-方法"}},[t._v("#")]),t._v(" Ford-Fulkerson 方法")]),t._v(" "),n("p",[t._v("常用于解决最大流问题的 Ford-Fulkerson 方法，之所以被称为“方法”而不是“算法”，是因为它主要提供的是一种通用的解决思路，包含了几种运行时间不同的具体算法实现。")]),t._v(" "),n("p",[t._v("Ford-Fulkerson 方法依赖于三种重要思想："),n("strong",[t._v("残存网络")]),t._v("、"),n("strong",[t._v("增广路径")]),t._v(" 和 "),n("strong",[t._v("切割")]),t._v("。这三种思想与许多流算法和问题有关，它们是"),n("strong",[t._v("最大流最小切割定理")]),t._v("（《CLRS》定理 26.6）的精髓。该定理以"),n("strong",[t._v("流网络的切割")]),t._v("来表述最大流的值。")]),t._v(" "),n("p",[t._v("Ford-Fulkerson 方法"),n("strong",[t._v("循环增加流的值")]),t._v("：")]),t._v(" "),n("ul",[n("li",[t._v("在开始的时候，对于所有的结点 u, v \\in V，f(u, v) = 0，给出的初始流量为 0。")]),t._v(" "),n("li",[t._v("每一次迭代中，将图 G 的流值进行增加，增加的方法就是在一个关联的“残存网络” Gf 中寻找一条“增广路径”\n"),n("ul",[n("li",[t._v("一旦知道图 Gf 中的一条增广路径的边，就可以很容易地辨别出 G 中的一些具体的边，可以对这些边上的流量进行修改，从而增加流的值。")])])]),t._v(" "),n("li",[t._v("虽然 Ford-Fulkerson 方法的每次迭代都增加流（整体）的值，但是对于图 G 的一条特定边来说，其流量可能增加，也可能减少。\n"),n("ul",[n("li",[t._v("而且，对某些边的流进行缩减可能是必要的，以便让算法可以将更多的流从源结点发送到汇点。")])])]),t._v(" "),n("li",[t._v("重复对流进行这一过程，知道残存网络中不再存在增广路径为止。最大流最小切割定理将说明在算法终结时，该算法将获得一个最大流。")])]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("FORD_FULKERSON_METHOD(G, s, t)\n1  initialize flow f to 0\n2  while there exists an augmenting path p in the residual network Gf\n3      augment flow f along p\n4  return f\n")])])]),n("p",[t._v("为了实现和分析 Ford-Fulkerson 方法，需要引入如下几个新的概念。")]),t._v(" "),n("h3",{attrs:{id:"残存网络"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#残存网络"}},[t._v("#")]),t._v(" 残存网络")]),t._v(" "),n("p",[t._v("从直观上看，给定流网络 G 和流量 f，残存网络 Gf 由那些 "),n("strong",[t._v("仍有空间对流量进行调整的边")]),t._v(" 构成。流网络的一条边"),n("strong",[t._v("可以允许的额外流量")]),t._v("等于 该边的容量 减去 该边上的流量。如果该差值为正，则将该条边置于残存网络图 Gf 中，并将其"),n("strong",[t._v("残存容量")]),t._v("设置为 cf(u, v) = c(u, v) - f(u, v)。对于图 G 中的边来说，只有能够允许额外流量的边才能被加入到图 Gf 中。如果边 (u, v) 的流量 f(u, v) 等于其容量 c(u, v)，则其 cf(u, v) == 0，该条边将不属于图 Gf。（注意，根据"),n("strong",[t._v("容量限制")]),t._v("原则，残存容量 cf(u, v) 不可能为负值）")]),t._v(" "),n("p",[t._v("残存网络 Gf 中可能包含图 G 中不存在的边。算法对流量进行操作的目标是增加总流量，为此，算法可能对某些特定边上的流量进行缩减，为了表示对一个正流量 f(u, v) 的缩减，将反向边 (v, u) 加入到图 Gf 中，并将其残存容量设置为 cf(v, u) = f(u, v)。即 一条边所能允许的反向流量最多将其正向流量抵消。残存网络中的这些反向边允许算法将已经发送出来的流量发送回去。")]),t._v(" "),n("p",[t._v("而将流量从同一条边发送回去等同于"),n("strong",[t._v("缩减")]),t._v("该条边的流量，这种操作在许多算法中都是必须的。")]),t._v(" "),n("p",[t._v("更形式化地说，假定有一个流网络 G = (V, E)，其源结点为 s，汇点为 t。设 f 为图 G 中的一个流，考虑结点对 u, v \\in V，定义"),n("strong",[t._v("残存容量")]),t._v(" cf(u, v) 如下：")]),t._v(" "),n("ul",[n("li",[t._v("若 (u, v) \\in E，则 cf(u, v) = c(u, v) - f(u, v)")]),t._v(" "),n("li",[t._v("若 (v, u) \\in E，则 cf(u, v) = f(v, u)")]),t._v(" "),n("li",[t._v("若是其它情况，则 cf(u, v) = 0")])]),t._v(" "),n("p",[t._v("由于已经假定在图 G 中边 (u, v) 和边 (v, u) 不能同时出现，故上述三种情况有且仅有一种会发生。")]),t._v(" "),n("p",[t._v("举例来说，如果容量限制 c(u, v) = 16，并且当前流量 f(u, v) = 11，则对 f(u, v) 可以增加的量最多为 cf(u, v) = 5，再多就超过边 (u, v) 的容量限制了。同时，允许算法从结点 v 向结点 u 最多返回 11 单位的流量（即全部当前流量），因此残存网络中反向边 cf(v, u) = 11。")]),t._v(" "),n("hr"),t._v(" "),n("p",[t._v("给定一个流网络 G = (V, E) 和一个流 f，则由 f 所诱导的图 G 的"),n("strong",[t._v("残存网络")]),t._v("为 Gf = (V, Ef)，其中边集 Ef = {(u, v) \\in V x V: cf(u, v) > 0}")]),t._v(" "),n("p",[t._v("即 残存网络的每条边（称为"),n("strong",[t._v("残存边")]),t._v("），必须允许大于 0 的流量通过。下面图 26-4 中的 (a) 图是前面图 26-1 (b) 的流网络 G 和流量 f 的重新绘制，图 26-4 (b) 描述的是对应的残存网络 Gf。Ef 中的边要么是 E 中原有的边，要么是其反向边，因此有 "),n("code",[t._v("|Ef| <= 2|E|")])]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-1.png",alt:"ford-fulkerson-1"}})]),t._v(" "),n("p",[t._v("残存网络 Gf 类似于一个容量为 cf 的流网络，但是不满足前面对流网络的定义，因为在 Gf 中可以有反平行边。除了反平行边这个区别外，流网络的其他性质 Gf 都有保留，因此可以在残存网络中定义一个流，它满足流的两条性质："),n("strong",[t._v("容量限制")]),t._v(" 和 "),n("strong",[t._v("流量守恒")]),t._v("。该流针对的是残存网络 Gf 中的残存容量 cf。")]),t._v(" "),n("p",[t._v("残存网络 Gf 中的一个流指出的是一条"),n("strong",[t._v("路线图")]),t._v("：如何在原来的流网络 G 中增加流。如果 f 是 G 的一个流，f' 是对应的残存网络 Gf 中的一个流，定义 f↑f' 为流 f' 对流 f 的"),n("strong",[t._v("递增")]),t._v(" (augmentation)，它是一个从 V x V 到 R 的函数，具体定义如下：")]),t._v(" "),n("ul",[n("li",[t._v("若 (u, v) \\in E，则 (f↑f')(u, v) = f(u, v) + f'(u, v) - f'(v, u)")]),t._v(" "),n("li",[t._v("其它情况，则 (f↑f')(u, v) = 0")])]),t._v(" "),n("p",[t._v("该定义背后的直观解释遵循残存网络的定义。因为在残存网络中"),n("strong",[t._v("将流量发送到反向边")]),t._v(" 等同于 在原来的网络中"),n("strong",[t._v("缩减流量")]),t._v("，所以将边 (u, v) 的流量增加 f'(u, v)，但减少 f'(v, u)。在残存网络中将流量推送回去 也被称为"),n("strong",[t._v("抵消操作")]),t._v(" (cancellation)。")]),t._v(" "),n("p",[t._v("举例来说，如果某公司将 5 箱货物从城市 u 发送到城市 v，同时将 2 箱同样的货物从城市 v 发送到城市 u，那么可以(从整体上来说)等价于 将 3 箱货物从城市 u 发送到城市 v。这类"),n("strong",[t._v("抵消操作")]),t._v("对于任何"),n("strong",[t._v("最大流算法")]),t._v("来说都是"),n("strong",[t._v("非常关键")]),t._v("的。")]),t._v(" "),n("hr"),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("引理 26.1")]),t._v("：设 G = (V, E) 为一个流网络，源结点为 s，汇点为 t，设 f 为 G 中的一个流。设 Gf 为由流 f 所诱导的 G 的残存网络，设 f' 为 Gf 中的一个流。那么函数 (f↑f') 是 G 的一个流，其值为 "),n("code",[t._v("|f↑f'| = |f| + |f'|")]),t._v("。")]),t._v(" "),n("p",[t._v("对引理 26.1 的证明主要有以下四点（具体证明参考《CLRS》Chapter 26.2）：")]),t._v(" "),n("ol",[n("li",[t._v("流量非负：(f↑f')(u, v) >= 0")]),t._v(" "),n("li",[t._v("容量限制：(f↑f')(u, v) <= c(u, v)")]),t._v(" "),n("li",[t._v("流量守恒：对于所有的结点 u \\in V - {s, t}，$ \\sum_{v \\in V} (f↑f')(u, v) = \\sum_{v \\in V} (f↑f')(v, u) $")]),t._v(" "),n("li",[t._v("计算 (f↑f') 的值，确保 "),n("code",[t._v("|f↑f'| = |f| + |f'|")])])]),t._v(" "),n("h3",{attrs:{id:"增广路径"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#增广路径"}},[t._v("#")]),t._v(" 增广路径")]),t._v(" "),n("p",[t._v("给定流网络 G = (V, E) 和流 f，"),n("strong",[t._v("增广路径")]),t._v(" p 是残存网络 Gf 中一条从源结点 s 到汇点 t 的"),n("strong",[t._v("简单路径")]),t._v("。根据残存网络的定义，对于一条增广路径上的边 (u, v)，可以增加其流量的幅度 最大为 cf(u, v)，再多会违反容量限制。")]),t._v(" "),n("p",[t._v("例如前面 图 26-4 (b) 中阴影覆盖的路径是一条增广路径。如果将图中的残存网络 Gf 看作一个流网络，那么可以对这条路径上的"),n("strong",[t._v("每条边的流量增加")]),t._v(" 4 个单位，而不会违反容量限制，因为该条路径上"),n("strong",[t._v("最小的残存容量")]),t._v("是 cf(v2, v3) = 4。")]),t._v(" "),n("p",[t._v("称在一条增广路径 p 上能够为每条边增加的流量的最大值 为路径 p 的"),n("strong",[t._v("残存容量")]),t._v("，该容量的表达式为：cf(p) = min{cf(u, v): (u, v) 属于路径 p}。下面的引理更加精确地阐述了此论断：")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("引理 26.2")]),t._v("：设 G = (V, E) 为一个流网络，设 f 为图 G 中的一个流，设 p 为残存网络 Gf 中的一条增广路径。定义一个函数 fp: V x V -> R 如下：")]),t._v(" "),n("ul",[n("li",[t._v("若 (u, v) 在 p 上，则 fp(u, v) = cf(p)")]),t._v(" "),n("li",[t._v("若是其它情况，则 fp(u, v) = 0")])]),t._v(" "),n("p",[t._v("则 fp 是残存网络 Gf 中的一个流，其值为 "),n("code",[t._v("|fp| = cf(p) > 0")])]),t._v(" "),n("hr"),t._v(" "),n("p",[t._v("下面的推论证明，如果将流 f 增加 fp 的量，则将获得 G 的另一个流，该流的值更加接近最大值。图 26-4 (c) 描述的是对图 26-4 (a) 的流 f 增加图 26-4 (b) 所示的 fp 的量 所获得的结果，而图 26-4 (d) 描述的则是残存网络 Gf。")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("推论 26.3")]),t._v("：设 G = (V, E) 为一个流网络，设 f 为 G 中的一个流，设 p 为残存网络 Gf 中的一条增广路径。设 fp 由引理 26.2 所定义，假定将 f 增加 fp 的量，则函数 "),n("code",[t._v("|f↑fp| = |f| + |fp| > |f|")]),t._v("。")]),t._v(" "),n("h3",{attrs:{id:"流网络和切割"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#流网络和切割"}},[t._v("#")]),t._v(" 流网络和切割")]),t._v(" "),n("p",[t._v("Ford-Fulkerson 方法的核心就是"),n("strong",[t._v("沿着增广路径重复增加路径上的流量")]),t._v("，直到找到一个最大流为止。稍后证明的"),n("strong",[t._v("最大流最小切割定理")]),t._v("表明：一个流是最大流 当且仅当 其残存网络不包含任何增广路径。")]),t._v(" "),n("p",[t._v("流网络 G = (V, E) 中的一个切割 (S, T) 将结点集合 V 划分为 S 和 T=V-S 两个不相交集合，且使得源结点 s \\in S、汇点 t \\in T。这类似于最小生成树 MST 中的切割，不过这里是对有向图的切割，而非无向图。")]),t._v(" "),n("p",[t._v("若 f 是一个流，则定义横跨切割 (S, T) 的"),n("strong",[t._v("净流量")]),t._v(" f(S, T) 如下：")]),t._v(" "),n("p",[t._v("$$ f(S, T) = \\sum_{u \\in S} \\sum_{v \\in T} f(u, v) - \\sum_{u \\in S} \\sum_{v \\in T} f(v, u) $$")]),t._v(" "),n("p",[t._v("切割 (S, T) 的"),n("strong",[t._v("容量")]),t._v("是：")]),t._v(" "),n("p",[t._v("$$ c(S, T) = \\sum_{u \\in S} \\sum_{v \\in T} c(u, v) $$")]),t._v(" "),n("p",[t._v("一个网络的"),n("strong",[t._v("最小切割")]),t._v("是整个网络中"),n("strong",[t._v("容量最小的切割")]),t._v("。")]),t._v(" "),n("hr"),t._v(" "),n("p",[t._v("流的定义和切割容量的定义之间不存在对称性，但这种不对称性是有意而为，并且很重要。对于容量来说，只计算从集合 S 发出、进入集合 T 的边的容量，而忽略反方向边上的容量。对于流，考虑的则是从 S 到 T 的总流量 减去 (反方向)从 T 到 S 的流量。")]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-2.png",alt:"ford-fulkerson-2"}})]),t._v(" "),n("p",[t._v("下面的引理将证明，对于给定流 f，横跨任何切割的净流量都相同，都等于 "),n("code",[t._v("|f|")]),t._v("，即流的值。")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("引理 26.4")]),t._v("：设 f 为流网络 G 的一个流，该流网络的源结点为 s，汇点为 t，设 (S, T) 为流网络 G 的"),n("strong",[t._v("任意切割")]),t._v("，则横跨切割 (S, T) 的净流量为 f(S, T) = "),n("code",[t._v("|f|")]),t._v("。")]),t._v(" "),n("p",[t._v("引理 26.4 的如下推论 说明如何使用切割容量来限定一个流的值。")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("推论 26.5")]),t._v("：流网络 G 中任意流 f 的值不能超过 G 的任意切割的容量。")]),t._v(" "),n("p",[t._v("此推论给出的一个直接结论是："),n("strong",[t._v("一个流网络中最大流的值不能超过该网络最小切割的容量")]),t._v("。这就是下面要来陈述和证明的非常重要的最大流最小切割定理。该定理表明一个最大流的值 事实上等于一个最小切割的容量。")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("定理 26.6")]),t._v("（"),n("strong",[t._v("最大流最小切割定理")]),t._v("）：设 f 为流网络 G = (V, E) 中的一个流，该流网络的源结点为 s，汇点为 t，则下面的条件是等价的：")]),t._v(" "),n("ol",[n("li",[t._v("f 是 G 的一个最大流。")]),t._v(" "),n("li",[t._v("残存网络 Gf 不包含任何增广路径。\n"),n("ul",[n("li",[t._v("增广路径：残存网络 Gf 中一条从源结点 s 到汇点 t 的简单路径。")])])]),t._v(" "),n("li",[t._v("最大流的值 "),n("code",[t._v("|f|")]),t._v(" = c(S, T)，其中 (S, T) 是流网络 G 的某个切割。")])]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-3.png",alt:"ford-fulkerson-3"}})]),t._v(" "),n("h2",{attrs:{id:"基本的-ford-fulkerson-算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#基本的-ford-fulkerson-算法"}},[t._v("#")]),t._v(" 基本的 Ford-Fulkerson 算法")]),t._v(" "),n("p",[t._v("在 Ford-Fulkerson 方法的每次迭代中，寻找某条增广路径 p，然后使用 p 来对流 f 进行修改（增加）。正如引理 26.2 和推论 26.3 所示，以 f↑fp 来替换 f，从而获得一个值为 "),n("code",[t._v("|f| + |fp|")]),t._v(" 的更大的流。")]),t._v(" "),n("p",[t._v("在如下算法实现中，通过为每条边 (u, v) \\in E 更新流属性 (u, v).f 来计算流网络 G = (V, E) 中的最大流。如果边 (u, v) \\notin E，则设置 (u, v).f = 0。另外，假设流网络各边的容量 c(u, v) 都已经给出，如果边 (u, v) \\notin E，则设置 c(u, v) = 0。根据如下式子来计算残存容量 cf(u, v)。代码中的表达式 cf(p) 只是一个临时变量，用于存放路径 p 的残存容量。")]),t._v(" "),n("ul",[n("li",[t._v("若 (u, v) \\in E，则 cf(u, v) = c(u, v) - f(u, v)")]),t._v(" "),n("li",[t._v("若 (v, u) \\in E，则 cf(u, v) = f(v, u)")]),t._v(" "),n("li",[t._v("若是其它情况，则 cf(u, v) = 0")])]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("FORD_FULKERSON(G, s, t)\n1  for each edge(u, v) \\in G.E\n2      (u, v).f = 0\n3  while there exists a path p from s to t in the residual network Gf\n4      cf(p) = min { cf(u, v): (u, v) is in path p }\n5      for each edge(u, v) in p\n6          if (u, v) \\in E\n7              (u, v).f = (u, v).f + cf(p)\n8          else\n9              (v, u).f = (v, u).f - cf(p)\n")])])]),n("p",[n("code",[t._v("FORD_FULKERSON(G, s, t)")]),t._v(" 算法是对 "),n("code",[t._v("FORD_FULKERSON_METHOD(G, s, t)")]),t._v(" 方法的简单扩展。算法流程描述如下：")]),t._v(" "),n("ol",[n("li",[t._v("在 1～2 行，将流 f 初始化为 0。")]),t._v(" "),n("li",[t._v("在 3～9 行的 while 循环中，重复在残存网络 Gf 中寻找一条增广路径 p，然后使用残存容量 cf(p) 来对路径 p 上的流 f 进行增加。路径 p 上的一条边要么是原来网络中的一条边，要么是原来网络中的边 的反向边。\n"),n("ul",[n("li",[t._v("在 4 行，找出路径 p 中的最小残存容量 cf(u, v)。")]),t._v(" "),n("li",[t._v("在 5～9 行的 for 循环中，对路径上 p 的每条边 (u, v) 的流量进行更新。")]),t._v(" "),n("li",[t._v("在 6～7 行，如果残存边 (u, v) 是原来流网络中的一条边，则增加其流量 (u, v).f")]),t._v(" "),n("li",[t._v("在 8～9 行，如果残存边 (u, v) 不是原来流网络中的一条边，则减少其反向边 (v, u) 流量 (v, u).f")])])]),t._v(" "),n("li",[t._v("最后，当 while 循环结束时，不再有增广路径。根据最大流最小切割定理，此时流 f 就是最大流。")])]),t._v(" "),n("h3",{attrs:{id:"ford-fulkerson-算法的分析"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ford-fulkerson-算法的分析"}},[t._v("#")]),t._v(" Ford-Fulkerson 算法的分析")]),t._v(" "),n("p",[t._v("Ford-Fulkerson 算法的运行时间取决于算法第 3 行是如何寻找增广路径 p 的。如果使用广度优先搜索 BFS 来寻找增广路径，算法的运行时间是多项式数量级。如果选择不好，"),n("code",[t._v("FORD_FULKERSON")]),t._v(" "),n("strong",[t._v("算法")]),t._v("可能不会终止：流的值会随着后续的递增 (augmentation) 而增加，但它却不一定收敛于最大的流值。")]),t._v(" "),n("p",[t._v("另外，只有当变得容量为"),n("strong",[t._v("无理数")]),t._v("时，"),n("code",[t._v("FORD_FULKERSON_METHOD")]),t._v(" "),n("strong",[t._v("方法")]),t._v("才可能无法终止。下面均假定所选择的任意增广路径 p 和所有的容量 c 都是整数值。在实际情况中，最大流问题中的容量常常都是整数。如果容量为有理数，则可以通过乘以某个系数（或者采用近似值）来将其转换为整数。")]),t._v(" "),n("hr"),t._v(" "),n("p",[t._v("如果 f* 表示转换后网络中的一个最大流，则在 "),n("code",[t._v("FORD_FULKERSON")]),t._v(" 算法的一个直接实现中，执行第 3～9 行的 while 循环的次数最多为 "),n("code",[t._v("|f*|")]),t._v(" 次，因为流量值在每次迭代中至少增加一个单位。")]),t._v(" "),n("p",[t._v("如果用于实现流网络 G = (V, E) 的数据结构是合理的，并且寻找一条增广路径 p 的算法时间是线性的（比如 DFS 和 BFS），则整个 while 循环的执行将非常高效。假设有一个与有向图 G' = (V, E') 相对应的数据结构，这里 E' = {(u, v): (u, v) \\in E 或者 (v, u) \\in E}。网络 G 中的边也是网络 G' 中的边，因此在这一数据结构中，保持其容量和流就非常简单了。给定网络 G 的一个流 f，残存网络 Gf 中的边由网络 G' 中所有满足条件 cf(u, v) > 0 的边 (u, v) 所构成，其中 cf 遵守前述残存容量的性质。")]),t._v(" "),n("p",[t._v("因此，如果使用深度优先搜索 DFS 或广度优先搜索 BFS，在一个残存网络中找到一条路径的时间应是 "),n("code",[t._v("O(|V| + |E'|) = O(|E|)")]),t._v("。而 while 循环的每一遍执行所需的时间因此为 "),n("code",[t._v("O(|E|)")]),t._v("，这与算法第 1～2 行的初始化成本一样，从而整个 "),n("code",[t._v("FORD_FULKERSON")]),t._v(" 算法的运行时间为 "),n("code",[t._v("O(|E|·|f*|)")]),t._v("。")]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-4.png",alt:"ford-fulkerson-4"}})]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-5.png",alt:"ford-fulkerson-5"}})]),t._v(" "),n("p",[t._v("当容量都是整数值且最优的流量值 "),n("code",[t._v("|f*|")]),t._v(" 较小时，"),n("code",[t._v("FORD_FULKERSON")]),t._v(" 算法的运行时间相等不错。但当最优流量值 "),n("code",[t._v("|f*|")]),t._v(" 取值较大时，可能会效率很慢，如图 26-7 示例。")]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-6.png",alt:"ford-fulkerson-6"}})]),t._v(" "),n("h2",{attrs:{id:"edmonds-karp-算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#edmonds-karp-算法"}},[t._v("#")]),t._v(" Edmonds-Karp 算法")]),t._v(" "),n("p",[t._v("可以通过在 "),n("code",[t._v("FORD_FULKERSON")]),t._v(" 算法第 3 行寻找增广路径 p 的操作中 使用广度优先搜索 BFS 来改善算法的效率。即 在残存网络中选择的增广路径是一条从源结点 s 到汇点 t 的(无权重)最短路径，BFS 时每条边的权重均为单位距离。称如此实现的 Ford-Fulkerson 方法为 Edmonds-Karp 算法，其运行时间为 "),n("code",[t._v("O(|V|·|E|^2)")]),t._v("，这就与最优流量值 "),n("code",[t._v("|f*|")]),t._v(" 的取值大小无关了。")]),t._v(" "),n("p",[t._v("对 Edmonds-Karp 算法的分析取决于残存网络 Gf 中结点之间的距离。下面的引理使用符号 df(u, v) 来表示残存网络 Gf 中从结点 u 到结点 v 的(无权重)最短路径距离，其中每条边的权重为单位距离。")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("引理 26.7")]),t._v("：如果 Edmonds-Karp 算法运行在流网络 G = (V, E) 上，该网络的源结点为 s、汇点为 t，则对于所有的结点 v \\in V-{s, t}，残存网络 Gf 中"),n("strong",[t._v("最短路径距离")]),t._v(" df(u, v) 随着每次流量的递增 而"),n("strong",[t._v("单调递增")]),t._v("。")]),t._v(" "),n("p",[t._v("下面的定理给出了 Edmonds-Karp 算法的迭代次数的上界。")]),t._v(" "),n("p",[t._v("《CLRS》"),n("strong",[t._v("定理 26.8")]),t._v("：如果 Edmonds-Karp 算法运行在源结点为 s、汇点为 t 的流网络 G = (V, E) 上，则该算法所执行的流量递增操作的总次数为 "),n("code",[t._v("O(|V|·|E|)")]),t._v("。")]),t._v(" "),n("p",[n("img",{attrs:{src:"/img/info_technology/algorithm/graph_theory/max_flow/ford-fulkerson-7.png",alt:"ford-fulkerson-7"}})]),t._v(" "),n("p",[t._v("由于在用广度优先搜索 BFS 寻找增广路径时，"),n("code",[t._v("FORD_FULKERSON(G, s, t)")]),t._v(" 中的每次迭代可以在 "),n("code",[t._v("O(|E|)")]),t._v(" 时间内实现，所以 Edmonds-Karp 算法的总运行时间为 "),n("code",[t._v("O(|V|·|E|^2)")]),t._v("。")]),t._v(" "),n("p",[t._v("而"),n("strong",[t._v("推送-重贴标签")]),t._v("算法能够取得更好的界，可以达到 "),n("code",[t._v("O(|V|^2·|E|)")]),t._v(" 甚至 "),n("code",[t._v("O(|V|^3)")]),t._v("。")]),t._v(" "),n("h2",{attrs:{id:"python-代码范例"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#python-代码范例"}},[t._v("#")]),t._v(" Python 代码范例")]),t._v(" "),n("p",[t._v("Python 环境：Python 3.7")]),t._v(" "),n("h3",{attrs:{id:"ford-fulkerson-最大流算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ford-fulkerson-最大流算法"}},[t._v("#")]),t._v(" Ford-Fulkerson 最大流算法")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#!/usr/bin/env python")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding:utf-8 -*-")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""=================================================\n@Project : algorithm/graph_theory/max_flow\n@File    : ford-fulkerson.py\n@Author  : YuweiYin\n@Date    : 2020-06-03\n=================================================="""')]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" queue\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n最大流 Max-Flow\n\n- Ford-Fulkerson 方法\n    - Ford-Fulkerson 算法\n    - Edmonds-Karp 算法\n\n参考资料：\nIntroduction to Algorithm (aka CLRS) Third Edition - Chapter 26\n"""')]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边结构体，表达边的信息，可随任务自定义 (增添其它值元素 val 对象)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Edge")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" from_v"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weight"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" capacity"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_v  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的起始顶点(关键字/序号)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_v      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的终止顶点(关键字/序号)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" weight  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (用于最短路)边的权重值 (默认值为 1，如果全部边的权重都相同，那图 G 就是无权图)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" is_directed  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# True 则表明此边是有向边，False 为无向边")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对无向边而言，起始顶点和终止顶点可以互换")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''下面是用于 Max-Flow 的属性'''")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("capacity "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" capacity  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此边的最大容量")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("             "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此边最大流的流量，初始为 0，取值范围 0 <= flow <= capacity")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 运行过程中的边流量存储于矩阵中，这里的 self.flow 仅存储最终的本条边的流量")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'->'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" \\\n               "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t capacity:'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("capacity"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t flow:'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" \\\n               "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t weight:'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t is_directed:'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于邻接矩阵的顶点结构体 (比 VertexList 简单)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里是用散列表 (而不是用链表) 来表达某顶点的所有邻接顶点")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("VertexMatrix")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造方法")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" color"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" distance"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" p"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的关键字 key (通常为顶点序号、唯一标志符)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("val "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" val            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本顶点的值元素 val (可自定义为任意对象，为结点附带的信息)")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''下面是用于 BFS 的属性'''")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" color        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# False 为"白色"，表示未被发现；True 为"黑色"，表示已经探索结束')]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" distance  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点距离源结点的距离 (最短简单路径的边数)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" p                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此结点的前驱结点/广度优先搜索树的父结点")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类序列化输出方法")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__str__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Vertex key: '")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (带边权的)邻接矩阵的图结构，通常适合稠密图")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入顶点结构体列表、边结构体列表")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AdjacencyMatrix")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# self.inf = 0x3f3f3f3f        # 初始各边的权重值均为 inf 无穷")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 经最大流算法后计算出的最大流值")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edges           "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的边列表")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由(起始,终止)顶点的关键字/唯一标志符映射到边数组下标")]),t._v("\n\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vertices     "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 存储输入的顶点列表 (可以从下标映射到顶点)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点映射到其下标 (既是邻接矩阵的行/列下标，也是 vertices 列表的下标)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由顶点的关键字/唯一标志符映射到顶点数组下标")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertex "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v2index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("vertex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建邻接矩阵(二维方阵)，adj[x][y] 的值为边 (x, y) 的当前流量，而不是边权重")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里用于残存网络 Gf，可以有反平行边。如果 adj[x][y] 为 0 表示没有此边，在 Gf 上运行 BFS")]),t._v("\n        v_num "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 顶点数目")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" v_num "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v_num"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 若 edges 合法，则进行边初始化处理")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 断言最大流算法里都是有向边")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_directed\n                from_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边起点的关键字 key")]),t._v("\n                to_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v          "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边终点的关键字 key")]),t._v("\n                capacity "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("capacity  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 边的容量")]),t._v("\n                self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" index\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" from_v "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_v "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将顶点关键字 key 转为下标 index，然后初始化 adj[from][to] 为边的最大容量")]),t._v("\n                    from_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    to_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" from_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" v_num "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" to_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" v_num\n                    self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" capacity\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断 key 号为 _key 的顶点是否位于顶点列表中")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__contains__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取图中 key 号为 _key 的顶点，如果没有此顶点则返回 None")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_vertex")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" _key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接矩阵 - 图转置")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("graph_transposition")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 其实如果是无向边，无需处理，但这里还是转了")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先获取 key")]),t._v("\n            from_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v\n            to_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 交换 key")]),t._v("\n            edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" to_key\n            edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" from_key\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把 key 转成 index")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n            from_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            to_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 修改邻接矩阵")]),t._v("\n            temp "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" temp\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出邻接矩阵")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print_matrix_info")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" row "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 广度优先搜索 (Breadth First Search, BFS)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BFS")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化各个结点距离源结点的距离为 inf 无穷")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("         "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从源结点 s 到目标结点 v 的一条最短路径上的所有结点(的关键字)")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果此标志为真，则会结束掉递归过程")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入：输入图结构默认为邻接矩阵 adj_m，而 start_key 为源顶点的关键字")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("do_bfs")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 start_key 源顶点的关键字 转为源结点结构体")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" start_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            start_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" start_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            start_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'输入的 start_key 不是任何顶点的关键字，BFS 失败'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 除了源结点 s 外，将其余所有结点 u 的状态标记为“未被发现”，即 color 为白色 white")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 另外，将 u.d 设置为无穷 inf，表示从源结点不可达结点 u。由于未探索到结点 u，将其前驱结点设置为空 nil")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n            v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 设置源结点 s 的属性。由于已经发现了 s，所以 s.color 设置为灰色 gray。结点 s 到自身的距离为 0")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 结点 s 为 BFS 树的树根，所以前驱/父结点 u.p 为空 nil")]),t._v("\n        start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n        start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 将 s 加入辅助队列 Q，成为其唯一成员")]),t._v("\n        aux_queue "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Python 内建 queue 队列对象")]),t._v("\n        aux_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 在 while 循环中，先进先出地逐个处理队列 Q 中的结点")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" aux_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("empty"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.1. 先取出 Q 队首结点 u")]),t._v("\n            u "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aux_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" u"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n            u_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("u"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.2. 逐个处理 u 的所有邻接结点 v")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edge_flow "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("u_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 邻接矩阵中边流量为 0 表示残存网络中没有此边")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" edge_flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("continue")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取边的终点 v 结构体")]),t._v("\n                v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.3. 如果 v.color 是白色，表示它未被发现，需要被加入到队列 Q 中。在入队之前，需要设置其属性：")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.color 颜色设置为灰色，表示它已被发现，但是尚未被探索完（所谓探索结束，是其邻接结点都已被处理）")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.d 是 v 到源结点 s 的距离，这个距离等于 u.d 距离加上 1")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - v.p 设置前驱/父结点为 u")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     - 将 v 入队，之后的 while 循环中 会考察 v 的各个邻接结点")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n                    v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n                    v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" u\n                    aux_queue"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("put"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. for 循环结束，u 的所有邻接结点都被考察了，所以 u 已经被探索结束了。u.color 设置为黑色，保证不会再被加入队列 Q")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# u.color = True")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 BFS 之后，打印出所有结点(的关键字)及其距离")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@staticmethod")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print_vertex_distance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("distance"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在计算出 BFS 之后，获取从源结点 s 到目标结点 v 的一条最短路径上的所有结点(的关键字)")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 此处图结构为邻接矩阵 adj_m")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return 'get_path: 输入的参数类型不合法'")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先把 key 关键字 转为顶点点结构体")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" start_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            start_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" start_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            start_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("start_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return '输入的 start_key 不是任何顶点的关键字'")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" end_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            end_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("end_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" end_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            end_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("end_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# return '输入的 end_key 不是任何顶点的关键字'")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_get_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("_get_path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" end_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# self.path = 'No path from ' + str(start_v.key) + ' to ' + str(end_v.key) + ' exists.'")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_recursion "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先获取其前驱结点/父结点的关键字，再获取本结点的关键字")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_get_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("start_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("p"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Ford-Fulkerson 最大流算法 - O(VE^2)")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FordFulkerson")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 所有结点的 distance 初始化为 inf")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算流网络(邻接矩阵) adj_m 的最大流")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("do_edmonds_karp")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 首先确认输入的合法性，并将输入的源结点和汇点关键字 转为结点结构体")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" AdjacencyMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" source_v_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" terminal_v_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n        source_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("source_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        terminal_v "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("terminal_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("source_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("terminal_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 将各边的流量 flow 初始化为 0")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" edge "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 在 while 循环中，重复在残存网络 Gf 中寻找一条增广路径 p")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 然后使用残存容量 cf(p) 来对路径 p 上的流 flow 进行增加")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 路径 p 上的一条边要么是原来网络中的一条边，要么是原来网络中的边 的反向边")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在本实现中，adj_m 中的矩阵保存的就是残存网络 Gf 中各个边的流量")]),t._v("\n        bfs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BFS"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        is_exist_aug_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 循环标志 True 表示当前残存网络 Gf 中存在一条增广路径")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" is_exist_aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.1. 找出增广路径 aug_path")]),t._v("\n            bfs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("do_bfs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            aug_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" bfs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果不存在增广路径，BFS 会返回空列表。如果返回仅含 1 个元素关键字的列表，是异常情况")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.2. 找出路径 p 中的最小残存容量 cf(u, v)")]),t._v("\n            min_cf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据 key 获取 index")]),t._v("\n                from_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n                from_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更新路径上的最小残存容量 min_cf")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" min_cf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    min_cf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" min_cf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("inf  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 断言增广路径上的流量值不为 inf")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.3. 在 for 循环中，对路径上 p 的每条边 (u, v) 的流量进行更新")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据 key 获取 index")]),t._v("\n                from_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aug_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" from_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" to_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index\n                from_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2v_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果残存边 (u, v) 是原来流网络中的一条边，则增加其流量 (u, v).f")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果残存边 (u, v) 不是原来流网络中的一条边，则减少其反向边 (v, u) 流量 (v, u).f")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在本实现中，增加边 (u, v) 的流量，等于缩减 (u, v) 的剩余容量、增加反平行边 (v, u) 的剩余容量")]),t._v("\n                adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-=")]),t._v(" min_cf\n                adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" min_cf\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 最后，当 while 循环结束时，不再有增广路径。根据最大流最小切割定理，此时流 flow 就是最大流")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在本实现中，adj_m 中的矩阵保存的就是残存网络 Gf 中各个边的流量，最终将实际的流量赋予各个结点的 flow 属性")]),t._v("\n        adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" from_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" to_index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                from_node "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                to_node "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_node"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("isinstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("to_node"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果此边是原图中的边，则赋予该边 flow 属性，表示最大流的流量")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_node"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_node"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    edge_index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key2e_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_node"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_node"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    edge "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("edge_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n                    edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 赋予此边流量属性 flow")]),t._v("\n                    adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("from_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("to_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 增长图的最大流量值")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构造图同《CLRS》图 26-6 的(含边容量的)有向图用于计算最大流")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于构造邻接矩阵的顶点的 key/val 信息列表")]),t._v("\n    matrix_vertices_info "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("400")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 有向边的 from/to/c/is_directed 信息列表")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# is_directed 为 True 表示此边为有向边，否则为无向边")]),t._v("\n    di_edges_info "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("14")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v4'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据前述列表信息构造结点列表")]),t._v("\n    inf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0x3f3f3f3f")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 需保证与程序中其它 inf 是相同的值")]),t._v("\n    matrix_vertices "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    di_edges "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" v "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" matrix_vertices_info"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        matrix_vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VertexMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" val"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("v"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" distance"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("inf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" e "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" di_edges_info"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        di_edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Edge"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_v"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" to_v"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" capacity"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_directed"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建邻接矩阵 (用邻接矩阵+有向图 执行最大流算法)")]),t._v("\n    adj_m "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AdjacencyMatrix"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("matrix_vertices"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" di_edges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 执行 O(VE^2) Ford-Fulkerson 最大流算法")]),t._v("\n    source_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),t._v("\n    ford_fulkerson "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" FordFulkerson"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    start "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ford_fulkerson"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("do_edmonds_karp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" source_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" terminal_v_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    end "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("process_time"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输出结果 & 运行时间")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# max_flow: 23")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 4, 2, 0, 0, 0]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [12, 0, 0, 0, 0, 0]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [11, 4, 0, 0, 3, 0]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 12, 9, 0, 7, 1]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 0, 11, 0, 0, 0]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [0, 0, 0, 19, 4, 0]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\nmax_flow:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_flow"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    adj_m"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_matrix_info"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Running Time: %.5f ms'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("end "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" start"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    sys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("main"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),n("h2",{attrs:{id:"参考资料"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),n("ul",[n("li",[t._v("Introduction to Algorithm (aka CLRS) Third Edition - Chapter 26")])])])}),[],!1,null,null,null);s.default=e.exports}}]);